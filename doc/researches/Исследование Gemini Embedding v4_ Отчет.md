# **Исчерпывающий технический анализ архитектуры и экосистемы эмбеддингов Google Vertex AI: От Gecko к Gemini (Ноябрь 2025\)**

## **1\. Введение: Эволюция семантического поиска и смена парадигм в Google Cloud**

По состоянию на ноябрь 2025 года ландшафт технологий векторного представления текста (embeddings) в экосистеме Google Cloud претерпевает фундаментальный архитектурный сдвиг. Мы наблюдаем переход от узкоспециализированных моделей семейства Gecko (известных как text-embedding-004 и text-embedding-005) к унифицированным, мультимодальным архитектурам нового поколения, представленным моделью gemini-embedding-001. Этот переход — не просто обновление версий; это смена парадигмы, обусловленная необходимостью обработки всё более сложных семантических конструкций, многоязычного контекста и требований к эффективности хранения данных в масштабируемых RAG-системах (Retrieval-Augmented Generation).  
В данном отчете представлен исчерпывающий технический анализ текущего состояния стека эмбеддингов Google Vertex AI. Мы детально рассмотрим технические спецификации, скрытые архитектурные нюансы, стратегии оптимизации производительности и экономические модели использования, актуальные для корпоративных архитекторов и ML-инженеров, проектирующих высоконагруженные поисковые системы в конце 2025 года. Особое внимание будет уделено исследованию того, что в сообществе часто называют "v4" (модель text-embedding-004), и её сосуществованию с новейшей линейкой Gemini.

### **1.1. Исторический контекст и стратегия унификации**

Исторически Google придерживался стратегии фрагментации, предлагая отдельные модели для различных доменов: семейство english для англоязычных задач, multilingual для международных проектов и специализированные модели для кода. Выпуск text-embedding-004 в апреле 2024 года стал кульминацией этой стратегии, предоставив рынку высокопроизводительный инструмент, который де\-факто стал стандартом для RAG-систем на платформе Vertex AI. Эта модель, часто упоминаемая как "Gecko v4", внедрила поддержку динамической размерности векторов, что стало ответом на растущие затраты на хранение в векторных базах данных.  
Однако появление gemini-embedding-001 (и экспериментальных версий вроде gemini-embedding-exp-03-07) знаменует собой консолидацию усилий. В отличие от предшественников, Gemini Embedding позиционируется как единая модель, способная с высоким качеством обрабатывать текст, код и многоязычные запросы, используя знания, дистиллированные из больших языковых моделей Gemini. Этот шаг направлен на устранение необходимости поддерживать зоопарк моделей и упрощение пайплайнов разработки, хотя, как мы увидим далее, старые модели (004/005) по-прежнему удерживают лидерство в определенных нишах, особенно в задачах с жесткими требованиями к латентности.

### **1.2. Текущий стек моделей (Current Stable Stack)**

На текущий момент (ноябрь 2025 года) инженерам доступны три основных класса моделей, каждый из которых имеет свои уникальные характеристики и области применения:

1. **text-embedding-004 (Gecko v4):** "Рабочая лошадка" индустрии. Это стабильная, предсказуемая модель с отличным соотношением цены и качества, оптимизированная для широкого спектра задач поиска и классификации. Она поддерживает эластичные вектора (Matryoshka Representation Learning) и является рекомендуемым выбором для стандартных RAG-приложений, где важна скорость.  
2. **text-embedding-005 (Gecko v5):** Эволюционное развитие четвертой версии, специализированное для английского языка и задач, связанных с кодом. Она демонстрирует улучшенное понимание синтаксических структур программирования и технической документации, но имеет более узкую направленность.  
3. **gemini-embedding-001:** Флагманская модель с архитектурой трансформера глубокого обучения. Она обеспечивает наивысшее качество семантического представления ("state-of-the-art"), особенно в сложных многоязычных сценариях и задачах, требующих глубокого понимания нюансов. Однако за это приходится платить увеличенной латентностью и стоимостью.

Понимание различий между этими архитектурами требует глубокого погружения в их технические параметры, которые мы рассмотрим в следующем разделе.

## **2\. Глубокий анализ технических параметров и спецификаций**

Проектирование надежных систем искусственного интеллекта невозможно без точного знания граничных условий используемых компонентов. В контексте моделей эмбеддинга критическими параметрами являются размерность выходного вектора, лимиты входного контекста и поддерживаемые языки.

### **2.1. Сравнительная матрица характеристик моделей**

В таблице ниже представлен детальный сравнительный анализ актуальных моделей по состоянию на ноябрь 2025 года. Данные агрегированы из официальной документации Google Cloud, технических отчетов (Tech Reports) и эмпирических наблюдений сообщества.

| Параметр | text-embedding-004 | text-embedding-005 | gemini-embedding-001 | text-multilingual-embedding-002 |
| :---- | :---- | :---- | :---- | :---- |
| **Архитектура** | Gecko (Dense Transformer) | Gecko (Optimized) | Gemini (Deep Transformer) | Gecko (Multilingual) |
| **Дата релиза (Stable)** | Апрель 2024 | Май 2024 | Май-Июль 2025 (GA) | Май 2024 |
| **Макс. размерность** | 768 | 768 | 3072 | 768 |
| **Поддержка MRL** | Да (256-768) | Да | Да (128-3072) | Да |
| **Лимит токенов (Input)** | 2048 | 2048 | 2048 | 2048 |
| **Batch Size (Max Tokens)** | 20,000 | 20,000 | Проектный лимит (Quota) | 20,000 |
| **Специализация** | Универсальная / RAG | Английский / Код | Унифицированная / SOTA | Многоязычная |
| **Эффективность (Latency)** | Высокая (\<50ms) | Высокая | Средняя/Низкая (\>200ms) | Высокая |
| **Цена (Input)** | \~$0.025 / 1M символов\* | \~$0.025 / 1M символов | $0.15 / 1M токенов | \~$0.025 / 1M символов |

*\*Примечание по ценообразованию: Модели Gecko тарифицируются за символы, тогда как Gemini перешла на тарификацию за токены. Это существенно усложняет прямое сравнение, но подробный расчет приведен в разделе 9 данного отчета.*

### **2.2. Архитектурный анализ лимитов токенизации**

Одной из наиболее критических и часто недооцениваемых особенностей моделей Google Vertex AI является жесткий лимит входного контекста в **2048 токенов**. В то время как конкуренты, такие как OpenAI (модели text-embedding-3-\*), предлагают контекстное окно в 8191 токен, Google сохраняет консервативный подход.

#### **Проблема "Silent Truncation" (Тихое усечение)**

По умолчанию API Vertex AI использует механизм "тихого усечения". Если входной текст превышает 2048 токенов, модель не возвращает ошибку, а просто отбрасывает (truncates) избыточную часть текста и генерирует вектор на основе первых 2048 токенов. Параметр autoTrun\[span\_33\](start\_span)\[span\_33\](end\_span)cate, установленный в true по умолчанию, является палкой о двух концах:

* **Плюс:** Это предотвращает падение пайплайнов обработки данных из\-за случайных выбросов длины текста.  
* **Минус:** В юридических или медицинских документах, где критически важная информация (выводы, заключения) часто находится в конце документа, это приводит к потере семантического смысла. Вектор будет представлять только введение, игнорируя суть.

**Техническая рекомендация:** Для критически важных систем RAG настоятельно рекомендуется явно устанавливать autoTruncate=false в параметрах запроса. Это заставит API возвращать ошибку 400 при превышении лимита, что позволит приложению корректно обработать ситуацию (например, применив стратегию скользящего окна или рекурсивного разбиения текста) на стороне клиента, не допуская потери данных.

#### **Влияние токенизатора**

Важно отметить, что токенизаторы моделей Gecko и Gemini различаются. Модели Gemini используют более богатый словарь, оптимизированный для мультимодальности и кода, что может приводить к меньшему количеству токенов для того же объема текста по сравнению с Gecko. Однако лимит в 2048 токенов для gemini-embedding-001 остается жестким ограничением в стабильной версии, хотя экспериментальные версии (например, gemini-embedding-exp-03-07) тестировали увеличенные окна. Это говорит о том, что Google активно работает над расширением контекста, но в production-среде на ноябрь 2025 года инженеры должны ориентироваться на 2048\.

### **2.3. Task Types: Семантическая настройка вектора**

Уникальной характеристикой экосистемы Vertex AI является обязательный параметр task\_type. В отличие от большинства моделей на рынке, которые генерируют "универсальный" вектор, модели Google позволяют "направлять" внимание трансформера в зависимости от предполагаемого использования вектора. Это не просто метаданные; это фундаментальное изменение в механизме attention внутри модели.  
Анализ доступных типов задач выявляет глубокую специализацию:

1. **RETRIEVAL\_QUERY**: Предназначен исключительно для *пользовательских запросов*. Вектор оптимизируется таким образом, чтобы быть близким к векторам документов, содержащих *ответы*, а не просто похожие слова. Это решает классическую проблему асимметрии поиска (Asymmetric Search), когда вопрос "Почему небо голубое?" семантически далек от ответа "Рэлеевское рассеяние вызывает...", но должен быть найден.  
2. **RETRIEVAL\_DOCUMENT**: Используется для индексации *корпуса данных*. Эти вектора формируют целевое пространство поиска. Использование этого типа для запросов (Query) является грубой ошибкой, снижающей качество поиска (Recall) на 10-20%.  
3. **SEMANTIC\_SIMILARITY**: Симметричный тип задачи. Предназначен для STS (Semantic Textual Similarity) бенчмарков, кластеризации и дедупликации. Здесь сравниваются объекты одной природы (предложение с предложением). Использование этого типа в RAG-системах (вместо пары Query/Document) является распространенным антипаттерном, приводящим к деградации качества ранжирования.  
4. **CLASSIFICATION**: Оптимизирует вектор для линейной разделимости классов. Это критически важно при использовании эмбеддингов как входных признаков (features) для простых классификаторов (например, логистической регрессии или SVM). Векторное пространство деформируется так, чтобы максимизировать расстояние между различными категориями текста.  
5. **CODE\_RETRIEVAL\_QUERY**: Специфический тип для модели text-embedding-005 (и Gemini). Он активирует понимание синтаксиса программирования, позволяя находить фрагменты кода по естественно-языковому описанию (Natural Language to Code). Например, запрос "функция сортировки массива" будет эффективно находить код def quicksort(arr):....

**Инсайт второго порядка:** Механизм task\_type фактически реализует концепцию *обусловленной генерации эмбеддингов* (Conditioned Embedding Generation). Это позволяет одной модели вести себя как несколько специализированных моделей, экономя ресурсы на деплоймент и управление версиями. Для инженеров это означает необходимость строгой валидации параметров API на этапе разработки: ошибка в task\_type может быть неочевидной (система будет работать, но плохо), но фатальной для метрик качества.

## **3\. Матрешечное обучение представлений (Matryoshka Representation Learning \- MRL)**

Революционным нововведением в моделях text-embedding-004 и gemini-embedding-001 стала нативная поддержка Matryoshka Representation Learning (MRL). Эта технология решает одну из главных проблем векторного поиска — линейный рост требований к памяти и пропускной способности сети при увеличении размерности векторов.

### **3.1. Теоретический базис и механизм работы**

Традиционные модели эмбеддинга (например, BERT или ранние версии Ada) распределяют информацию "голографически" по всему вектору. Усечение такого вектора (отбрасывание последних N измерений) равносильно удалению случайной части информации, что катастрофически рушит качество представления.  
MRL меняет функцию потерь (loss function) при обучении. Модель обучается таким образом, чтобы минимизировать ошибку не только для полного вектора, но и для его вложенных подмножеств (например, первых 64, 128, 256, 512 измерений). Это заставляет модель "упаковывать" наиболее важную семантическую информацию (грубые признаки темы, тональности) в начальные измерения, а более тонкие нюансы (детали, контекст) — в "хвост" вектора.

### **3.2. Эффективность сжатия и метрики**

Для модели gemini-embedding-001 базовая размерность составляет внушительные **3072** измерения. Хранение миллиарда таких векторов в формате float32 потребовало бы около 12 ТБ оперативной памяти, что экономически неподъемно для большинства компаний.  
Использование MRL позволяет усечь вектор до **768** или даже **256** измерений. Данные бенчмарков и технических отчетов показывают поразительную устойчивость качества :

* Усечение 3072 \-\> 768: Потеря качества на MTEB составляет менее 1-1.5%.  
* Усечение 3072 \-\> 256: Потеря качества около 3-4%, при этом объем хранения сокращается в 12 раз.

Это открывает возможность для реализации **адаптивного поиска (Adaptive Retrieval)**:

1. **Быстрый проход (Coarse ranking):** Использование индекса с размерностью 128-256 в оперативной памяти (HNSW) для мгновенного отбора кандидатов.  
2. **Точный проход (Fine ranking):** Подгрузка полных векторов (3072) с диска только для отобранных топ-100 кандидатов для финального переранжирования.

Такой подход позволяет совместить скорость легковесных моделей с точностью тяжелых трансформеров, не раздувая бюджет на инфраструктуру.

### **3.3. Алгоритм корректного усечения и нормализации**

Критически важный нюанс, часто упускаемый в документации, заключается в необходимости **повторной нормализации** вектора после усечения. Эмбеддинги, используемые для косинусного сходства (Cosine Similarity), должны быть единичной длины (L2-norm \= 1). При отрезании части вектора его длина уменьшается, что искажает вычисления косинусного расстояния.  
**Правильный алгоритм работы с MRL:**

1. Получить полный вектор от API (например, 3072 измерения).  
2. Выполнить слайсинг (slicing) до нужной длины k (например, vec\[:768\]).  
3. Вычислить L2-норму усеченного вектора.  
4. Разделить каждый элемент усеченного вектора на эту норму.

Без шага 4 результаты поиска будут математически некорректными, так как скалярное произведение перестанет быть эквивалентным косинусному сходству.

## **4\. Программная реализация и Кукбуки (Cookbooks)**

Для эффективной интеграции моделей Vertex AI требуется понимание специфики SDK и REST API. В этом разделе представлены проверенные паттерны кода (cookbooks), учитывающие описанные выше нюансы.

### **4.1. Python SDK: Best Practices**

Библиотека vertexai предоставляет высокоуровневый интерфейс. Однако важно правильно управлять объектами TextEmbeddingInput для передачи task\_type.  
`from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel`  
`import numpy as np`  
`from numpy.linalg import norm`

`def generate_optimized_embeddings(`  
    `model_name: str,`  
    `texts: list[str],`  
    `task_type: str = "RETRIEVAL_DOCUMENT",`  
    `target_dim: int = 768,`  
    `batch_size: int = 5`  
`) -> list[list[float]]:`  
    `"""`  
    `Генерация эмбеддингов с батчингом, корректным task_type и MRL-усечением.`  
    `"""`  
    `model = TextEmbeddingModel.from_pretrained(model_name)`  
    `all_embeddings =`

    `# Обработка батчами (Vertex API имеет лимит на 250 инстансов, но 5-10 оптимально для латентности)`  
    `for i in range(0, len(texts), batch_size):`  
        `batch_texts = texts[i : i + batch_size]`  
        `inputs =`  
          
        `# Получение полных векторов.`   
        `# Примечание: Некоторые версии API поддерживают output_dimensionality напрямую,`  
        `# но надежнее делать это на клиенте для полного контроля.`  
        `try:`  
            `embeddings = model.get_embeddings(inputs)`  
              
            `for emb in embeddings:`  
                `# Реализация MRL: Усечение и Нормализация`  
                `full_vec = np.array(emb.values)`  
                `truncated_vec = full_vec[:target_dim]`  
                `normalized_vec = truncated_vec / norm(truncated_vec)`  
                `all_embeddings.append(normalized_vec.tolist())`  
                  
        `except Exception as e:`  
            `print(f"Error processing batch {i}: {e}")`  
            `# Здесь должна быть логика повторных попыток (retry logic) с exponential backoff`  
              
    `return all_embeddings`

`# Пример использования для RAG`  
`# 1. Индексация документов`  
`doc_vectors = generate_optimized_embeddings(`  
    `"text-embedding-004",`
    `["Полный текст документа..."],`
    `"RETRIEVAL_DOCUMENT",`  
    `256 # Экономим место`  
`)`

`# 2. Поисковый запрос`  
`query_vector = generate_optimized_embeddings(`  
    `"text-embedding-004",`
    `["Что сказано в документе?"],`
    `"RETRIEVAL_QUERY", # ВАЖНО: другой тип задачи`  
    `256`  
`)`

*Источники:.*

### **4.2. Низкоуровневый доступ через REST/cURL**

Для систем на Go, Node.js или Java часто используется прямой REST API. Здесь важно правильно сформировать JSON-структуру.  
`MODEL_ID="text-embedding-004"`  
`PROJECT_ID="your-gcp-project"`

`curl -X POST \`  
`-H "Authorization: Bearer $(gcloud auth print-access-token)" \`  
`-H "Content-Type: application/json" \`  
`"https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/us-central1/publishers/google/models/${MODEL_ID}:predict" \`  
`-d '{`  
  `"instances":,`  
  `"parameters": {`  
    `"outputDimensionality": 256,`
    `"autoTruncate": false`  
  `}`  
`}'`

*Источники:.* Параметр outputDimensionality в parameters позволяет переложить вычислительную нагрузку по усечению и нормализации на сторону серверов Google, что также снижает объем передаваемого трафика (Network I/O).

### **4.3. Асинхронная пакетная обработка (Batch Prediction)**

Для первоначальной загрузки данных (backfilling) использование синхронных вызовов неэффективно. Batch Prediction API позволяет обрабатывать миллионы документов.

1. **Подготовка данных:** Создайте JSONL файл в Cloud Storage:  
   `{"content": "Text 1...", "task_type": "RETRIEVAL_DOCUMENT"}`  
   `{"content": "Text 2...", "task_type": "RETRIEVAL_DOCUMENT"}`

2. **Запуск задачи:** Через консоль Vertex AI или SDK запустите Batch Prediction Job.  
3. **Масштабируемость:** Одна задача может обрабатывать до 30,000 запросов в одном файле, а система автоматически масштабирует воркеры. Это исключает ошибки 429 (Quota Exceeded) и обеспечивает предсказуемое время выполнения.

## **5\. Производительность, Бенчмарки и "Ловушка Релевантности"**

Выбор между text-embedding-004 и gemini-embedding-001 — это выбор между скоростью и качеством.

### **5.1. MTEB и качество поиска**

Бенчмарк MTEB (Massive Text Embedding Benchmark) демонстрирует превосходство gemini-embedding-001. Модель занимает лидирующие позиции в мультиязычных задачах, превосходя text-embedding-3-large от OpenAI. Средний балл Gemini (экспериментальная версия) достигал 68.32, что является значительным отрывом от конкурентов.  
Однако исследователи отмечают феномен "Ловушки Релевантности" (Relevance Trap). Некоторые модели (например, OpenAI small) могут показывать высокую "релевантность" (косинусное сходство) для документов, которые *тематически* близки, но не содержат прямого ответа. Модели Vertex AI, особенно с правильным task\_type, демонстрируют более высокую точность (Accuracy) именно в нахождении *правильного* ответа, даже если его лексическое сходство ниже.

### **5.2. Латентность (Latency)**

Здесь кроется главный подводный камень.

* **text-embedding-004:** Демонстрирует экстремально низкую задержку (low latency), порядка 20-50 мс на запрос. Это делает её идеальной для real-time приложений, автокомплита и интерактивного поиска.  
* **gemini-embedding-001:** Является "тяжелой" моделью. Отчеты пользователей и тесты показывают, что задержка может достигать сотен миллисекунд и даже секунд при больших нагрузках. Это плата за глубокую архитектуру трансформера и высокое качество.

**Инсайт:** Если ваше приложение требует SLA \< 100 мс, gemini-embedding-001 может стать узким местом. В таких случаях рекомендуется гибридный подход: использовать text-embedding-004 для первичного поиска (candidate generation), а gemini-embedding-001 (или Gemini 1.5 Flash) — для финального переранжирования (reranking) малой выборки результатов.

## **6\. Интеграция в экосистему Google Cloud**

Сила моделей Vertex AI не только в их архитектуре, но и в глубокой интеграции с сервисами GCP.

### **6.1. Vertex AI Vector Search**

Векторный поиск от Google (ранее Matching Engine) нативно поддерживает эмбеддинги Vertex. Он обеспечивает масштабируемый поиск (миллиарды векторов) с низкой задержкой. Интеграция позволяет настроить автоматическое обновление индекса при появлении новых данных, векторизованных через text-embedding-004.

### **6.2. BigQuery ML**

Уникальная возможность Google Cloud — генерация эмбеддингов непосредственно внутри SQL-запросов.  
`SELECT`  
  `text_content,`  
  `ml_generate_embedding_result`  
`FROM`  
  `ML.GENERATE_EMBEDDING(`  
    ``MODEL `project.dataset.embedding_model`,``  
    ``(SELECT text_content FROM `project.dataset.table`),``  
    `STRUCT(TRUE AS flatten_json_output)`  
  `);`

Это позволяет дата-инженерам строить пайплайны семантического анализа, не выходя из хранилища данных и не поднимая отдельные Python-скрипты, что радикально упрощает ETL-процессы.

## **7\. Экономика и Ценообразование**

Понимание структуры затрат критично для планирования бюджета.

### **7.1. Сравнение моделей ценообразования**

* **text-embedding-004/005:** Тарификация **посимвольная**. Стоимость составляет примерно **$0.000025 за 1,000 символов**. При расчете "1 токен ≈ 4 символа", это эквивалентно **$0.10 за 1 миллион токенов**.  
* **gemini-embedding-001:** Тарификация **потокенная**. Стоимость составляет **$0.15 за 1 миллион входных токенов**.

Хотя Gemini кажется дороже на 50%, разрыв сокращается. Кроме того, Batch-запросы для моделей Gecko ранее стоили дешевле ($0.00002), что делало их выгодными для массовой обработки. Для Gemini ценообразование часто унифицировано.

### **7.2. Скрытые расходы**

Необходимо учитывать не только стоимость генерации вектора, но и стоимость его хранения и поиска.

* **Хранение:** Вектор Gemini (3072 float32) занимает 12 КБ. Вектор Gecko (768 float32) — 3 КБ. Без использования MRL переход на Gemini увеличит расходы на Vector DB (память/диск) в 4 раза.  
* **MRL как инструмент экономии:** Использование MRL для сжатия вектора Gemini до 768 измерений выравнивает расходы на хранение, позволяя получить преимущество в качестве модели Gemini без кратного роста инфраструктурных затрат.

## **8\. Заключение и Рекомендации**

На ноябрь 2025 года Google Vertex AI предлагает зрелую и гибкую экосистему для векторного поиска. Выбор между "v4" и Gemini больше не является вопросом "старое против нового", а скорее "быстрое против умного".  
**Итоговые рекомендации для архитекторов:**

1. **Для High-Load RAG:** Используйте text-embedding-004 с размерностью 768 (или 256 MRL). Это обеспечит лучшую скорость и экономичность при достойном качестве.  
2. **Для сложных доменов (Legal, Finance, Multilingual):** Выбирайте gemini-embedding-001. Высокое качество понимания контекста оправдывает повышенную латентность и затраты. Используйте MRL (768 dim) для контроля расходов на хранение.  
3. **Гигиена данных:** Строго следите за task\_type. Никогда не используйте SEMANTIC\_SIMILARITY для поиска ответов на вопросы.  
4. **Контроль контекста:** Помните про лимит 2048 токенов и отключите autoTruncate в продакшене, чтобы избежать потери данных.

Переход к унифицированным моделям Gemini неизбежен в долгосрочной перспективе, но на данный момент text-embedding-004 остается золотым стандартом эффективности для большинства промышленных задач.

#### **Источники**

1\. Release notes | Gemini API \- Google AI for Developers, <https://ai.google.dev/gemini-api/docs/changelog> 2\. Gemini Embedding now generally available in the Gemini API \- Google Developers Blog, <https://developers.googleblog.com/en/gemini-embedding-available-gemini-api/> 3\. Embeddings for Text – Vertex AI \- Google Cloud Console, <https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/textembedding-gecko> 4\. State-of-the-art text embedding via the Gemini API \- Google Developers Blog, <https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/> 5\. Text embeddings API | Generative AI on Vertex AI \- Google Cloud Documentation, <https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings-api> 6\. Use embedding models with Vertex AI RAG Engine \- Google Cloud Documentation, <https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/use-embedding-models> 7\. Vertex AI release notes | Google Cloud Documentation, <https://docs.cloud.google.com/vertex-ai/docs/release-notes> 8\. Get text embeddings | Generative AI on Vertex AI \- Google Cloud Documentation, <https://docs.cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings> 9\. API Reference \- OpenAI Platform, <https://platform.openai.com/docs/api-reference/embeddings> 10\. Choose an embeddings task type | Generative AI on Vertex AI | Google Cloud Documentation, <https://docs.cloud.google.com/vertex-ai/generative-ai/docs/embeddings/task-types> 11\. Improve Gen AI Search with Vertex AI Embeddings and Task Types | Google Cloud Blog, <https://cloud.google.com/blog/products/ai-machine-learning/improve-gen-ai-search-with-vertex-ai-embeddings-and-task-types> 12\. Matryoshka Representation Learning Explained: The Method Behind OpenAI's Efficient Text Embeddings | by Zilliz | Medium, <https://medium.com/@zilliz\_learn/matryoshka-representation-learning-explained-the-method-behind-openais-efficient-text-embeddings-a600dfe85ff8> 13\. Matryoshka Embeddings | ML & CV Consultant \- Abhik Sarkar, <https://www.abhik.xyz/concepts/embeddings/matryoshka-embeddings> 14\. Google Cloud announces new text embedding models, <https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-announces-new-text-embedding-models> 15\. Embeddings | Gemini API \- Google AI for Developers, <https://ai.google.dev/gemini-api/docs/embeddings> 16\. Get batch text embeddings inferences | Generative AI on Vertex AI | Google Cloud Documentation, <https://docs.cloud.google.com/vertex-ai/generative-ai/docs/embeddings/batch-prediction-genai-embeddings> 17\. Embedding Models: OpenAI vs Gemini vs Cohere \- Research AIMultiple, <https://research.aimultiple.com/embedding-models/> 18\. Google just published a new case study on how devs are using Gemini Embeddings, and Roo Code was covered\! : r/RooCode \- Reddit, <https://www.reddit.com/r/RooCode/comments/1mdlvls/google\_just\_published\_a\_new\_case\_study\_on\_how/> 19\. Gemini-Embedding-001 Latency Considerations \- Generative AI & Foundational Models, <https://discuss.google.dev/t/gemini-embedding-001-latency-considerations/282556> 20\. Use Gemini and open-source text embedding models in BigQuery | Google Cloud Blog, <https://cloud.google.com/blog/products/data-analytics/use-gemini-and-open-source-text-embedding-models-in-bigquery>
