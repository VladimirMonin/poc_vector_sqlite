# ‚ö†Ô∏è Phase 13: –†–∏—Å–∫–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è

> –ß—Ç–æ –º–æ–∂–µ—Ç —Å–ª–æ–º–∞—Ç—å—Å—è –≤ production

---

## üö® –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —Ä–∏—Å–∫–∏

### 1. Long Video Processing ‚Äî Exponential Time Growth

**–¢–µ–∫—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ:**

| –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å | –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ | –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ | Frames |
|-------------|--------------|-----------------|--------|
| 30 —Å–µ–∫ | 5.3MB | **13 —Å–µ–∫** | 5 frames |
| 30 —Å–µ–∫ | 890KB | **4.3 —Å–µ–∫** | 5 frames |

**–≠–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è –Ω–∞ –¥–ª–∏–Ω–Ω—ã–µ –≤–∏–¥–µ–æ:**

```
–§–æ—Ä–º—É–ª–∞: T = frame_extraction + (frames √ó gemini_request) + audio_analysis
```

| –í–∏–¥–µ–æ | Frames (1fps) | Time | –†–∏—Å–∫ |
|-------|---------------|------|------|
| 5 –º–∏–Ω—É—Ç | 30 frames | **~90 —Å–µ–∫** | ‚ö†Ô∏è Slow |
| 30 –º–∏–Ω—É—Ç | 180 frames | **~9 –º–∏–Ω—É—Ç** | üî• Timeout |
| 1 —á–∞—Å | 360 frames | **~18 –º–∏–Ω—É—Ç** | üíÄ FAIL |

**–ü–æ—á–µ–º—É –æ–ø–∞—Å–Ω–æ:**

1. **Request Timeout:** HTTP timeout –æ–±—ã—á–Ω–æ 30-60 —Å–µ–∫
2. **Memory Growth:** 360 frames √ó 1920px = **~2GB RAM**
3. **API Costs:** 360 Gemini Vision requests = **$$$**
4. **User Experience:** 18 –º–∏–Ω—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è = abandoned operation

**–†–µ—à–µ–Ω–∏—è:**

```python
# Option 1: Adaptive sampling
if duration > 5_minutes:
    frame_rate = 0.1  # 1 frame per 10 sec
elif duration > 30_minutes:
    frame_rate = 0.033  # 1 frame per 30 sec

# Option 2: Background processing
await queue_manager.add_task(video_path, priority="low")

# Option 3: Streaming analysis
async for frame_batch in extract_frames(video, batch_size=10):
    await analyze_batch(frame_batch)
```

---

### 2. Document-Level Search –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç

**–ü—Ä–æ–±–ª–µ–º–∞:**

```python
# ‚ùå –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –≤–µ—Å—å –¥–æ–∫—É–º–µ–Ω—Ç
results = semantic_core.search("find article about Python", mode="hybrid")
# –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: chunks, –∞ –Ω–µ documents

# ‚úÖ –û–∂–∏–¥–∞–µ–º–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ
results = semantic_core.search_documents("Python guide")
# –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: –ø–æ–ª–Ω—ã–π nested_headers_example.md
```

**–ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è:**

1. **RAG Context Loss:** LLM –ø–æ–ª—É—á–∞–µ—Ç —Ä–∞–∑—Ä–æ–∑–Ω–µ–Ω–Ω—ã–µ chunks –±–µ–∑ document structure
2. **User Confusion:** "–Ø –∏—Å–∫–∞–ª —Å—Ç–∞—Ç—å—é, –∞ –ø–æ–ª—É—á–∏–ª 10 —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤"
3. **Duplicate Content:** –û–¥–∏–Ω document ‚Üí 5 chunks ‚Üí 5 results –≤ —Ç–æ–ø–µ

**–¢–µ–∫—É—â–∏–π workaround:**

```python
# –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –¥–æ–ª–∂–µ–Ω –≤—Ä—É—á–Ω—É—é –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å
results = semantic_core.search("query", limit=50)
docs = group_by_document_id(results)  # ‚ùå Not implemented
```

**–ù—É–∂–Ω–æ:**

```python
class SearchResult:
    document_id: int
    document_title: str  # NEW
    relevant_chunks: List[Chunk]  # Grouped
    best_score: float
```

---

### 3. FTS5 Granularity Mismatch

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Vector Search‚îÇ‚îÄ‚îÄ> Returns CHUNK IDs
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ
        ‚îú‚îÄ RRF Merge ‚îÄ‚îê
        ‚îÇ              ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚ñº
‚îÇ FTS5 Search  ‚îÇ‚îÄ‚îÄ> Returns DOCUMENT IDs ‚ùå
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**–ü—Ä–∏–º–µ—Ä –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ merge:**

```python
vector_results = [
    (chunk_id=18, score=0.75),
    (chunk_id=19, score=0.72),
]

fts_results = [
    (doc_id=2, score=1.0),  # ‚ùå –†–∞–∑–Ω—ã–µ entities!
]

# RRF –ø—ã—Ç–∞–µ—Ç—Å—è merge:
rrf_score(chunk_18) = ???  # chunk –Ω–µ –≤ fts_results
rrf_score(doc_2) = ???     # doc –Ω–µ –≤ vector_results
```

**–ü–æ—á–µ–º—É –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç:**

```python
# semantic_core/integrations/peewee/search_proxy.py

# FTS –∏—â–µ—Ç –ø–æ documents
fts_query = """
SELECT doc_id FROM documents_fts
WHERE documents_fts MATCH ?
"""

# Vector –∏—â–µ—Ç –ø–æ chunks
vector_query = """
SELECT chunk_id, vec_distance(embedding, ?)
FROM vec_chunks
"""

# Merge –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ ‚ùå
```

**–†–µ—à–µ–Ω–∏—è:**

```python
# Option 1: Chunk-level FTS (–Ω—É–∂–µ–Ω chunks_fts –≤–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞)
CREATE VIRTUAL TABLE chunks_fts USING fts5(content, chunk_id);

# Option 2: Document-level Vector (–∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å chunk embeddings)
doc_embedding = mean([chunk1.vec, chunk2.vec, ...])

# Option 3: Two-stage search
stage1 = fts_search(query) ‚Üí doc_ids
stage2 = vector_search(query, filter=doc_ids) ‚Üí chunks
```

---

### 4. Duplicate Chunks ‚Äî Storage Waste

**–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –≤ –∞—É–¥–∏—Ç–µ:**

```sql
-- Same content, different IDs
doc_id=1, content="–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫..."  -- metadata: {"type": "plain_text"}
doc_id=7, content="–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫..."  -- metadata: {"category": "text"}
```

**–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:**

```
127 chunks created
~15 duplicates detected (12% waste)
15 √ó 768 floats = 46KB wasted embeddings
15 √ó Gemini API calls = $0.0015 wasted
```

**–ú–∞—Å—à—Ç–∞–± –ø—Ä–æ–±–ª–µ–º—ã:**

| Corpus Size | Duplicate Rate | Wasted Storage | Wasted API $ |
|-------------|----------------|----------------|--------------|
| 10K chunks | 12% | 3.6MB | $0.12 |
| 100K chunks | 12% | 36MB | $1.20 |
| 1M chunks | 12% | 360MB | $12.00 |

**–ü—Ä–∏—á–∏–Ω—ã:**

1. **Test Data Artifacts:** –¢–µ—Å—Ç—ã —Å–æ–∑–¥–∞—é—Ç multiple documents —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º content
2. **No Deduplication Strategy:** –°–∏—Å—Ç–µ–º–∞ –Ω–µ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç duplicates –ø—Ä–∏ ingestion
3. **Metadata Variations:** `{"type": "text"}` vs `{"category": "text"}` ‚Üí —Ä–∞–∑–Ω—ã–µ records

**–†–µ—à–µ–Ω–∏—è:**

```python
# Option 1: Content hash
content_hash = hashlib.sha256(chunk.content.encode()).hexdigest()
# –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ–¥ –≤—Å—Ç–∞–≤–∫–æ–π:
if Chunk.select().where(Chunk.content_hash == content_hash).exists():
    skip_or_update()

# Option 2: Unique constraint
class Chunk(Model):
    content = TextField()
    content_hash = CharField(unique=True, index=True)

# Option 3: Merge metadata
# –ï—Å–ª–∏ content –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π, –Ω–æ metadata —Ä–∞–∑–Ω–∞—è ‚Üí merge –≤ –æ–¥–∏–Ω record
```

---

## üîß –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è

### 1. Token Limits ‚Äî Context Window

**Gemini limits:**

| Model | Max Input | Max Output | Total |
|-------|-----------|------------|-------|
| `gemini-2.5-flash-lite` | 1M tokens | 8K tokens | 1M |
| `gemini-2.5-flash` | 1M tokens | 8K tokens | 1M |
| `gemini-2.5-pro` | 2M tokens | 8K tokens | 2M |

**–ü—Ä–æ–±–ª–µ–º–∞ –¥–ª—è RAG:**

```python
# RAG context construction
context = "\n\n".join([chunk.content for chunk in top_10_results])
# –ï—Å–ª–∏ –∫–∞–∂–¥—ã–π chunk ~1000 tokens ‚Üí 10K total ‚úÖ

# –ù–æ –µ—Å–ª–∏ top_50_results:
context = "\n\n".join([chunk.content for chunk in top_50_results])
# 50 √ó 1000 = 50K tokens ‚úÖ Still OK

# Danger zone:
context = full_document  # 200K tokens ‚Üí OK
context = all_related_docs  # 1.5M tokens ‚Üí ‚ùå FAIL for flash-lite
```

**Mitigation:**

```python
# semantic_core/core/rag.py
def _build_context(chunks: List[Chunk], max_tokens: int = 100_000):
    total = 0
    selected = []
    for chunk in chunks:
        tokens = estimate_tokens(chunk.content)
        if total + tokens > max_tokens:
            break
        selected.append(chunk)
        total += tokens
    return selected
```

---

### 2. Rate Limiting ‚Äî RPM/TPM

**–¢–µ–∫—É—â–∏–µ limits (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º—ã–µ):**

```python
# infrastructure/gemini/rate_limiter.py
TokenBucket(
    rpm=15,      # Requests Per Minute
    tpm=1_000_000  # Tokens Per Minute (Flash Lite)
)
```

**Bottleneck scenarios:**

```python
# Scenario 1: Bulk ingestion
documents = load_corpus(1000_files)  # 1000 files
chunks = chunker.split_all(documents)  # 50K chunks
embeddings = [embedder.embed(c) for c in chunks]
# 50K requests √∑ 15 RPM = 3333 minutes = 55 HOURS ‚ùå
```

**Real limits –º–æ–≥—É—Ç –±—ã—Ç—å:**

| Tier | RPM | TPM | Daily Quota |
|------|-----|-----|-------------|
| Free | 15 | 32K | 1500 req |
| Paid | 1000 | 4M | Unlimited |

**–†–µ—à–µ–Ω–∏–µ:**

```python
# Use Batch API for bulk
batch_manager.submit_batch(chunks, priority="low")
# Process in background, receive results in 1-24 hours
# Cost: 50% cheaper ‚úÖ
```

---

### 3. SQLite Limitations

**Known issues:**

```python
# 1. No concurrent writes
# ‚ùå Two processes trying to INSERT simultaneously ‚Üí SQLITE_LOCKED

# 2. Vec0 index size
# Vector index = chunks √ó 768 floats √ó 4 bytes
# 100K chunks = 307MB index (in-memory during queries)

# 3. FTS5 memory
# Full-text index –º–æ–∂–µ—Ç –±—ã—Ç—å 50-100% –æ—Ç corpus size
```

**Production recommendations:**

```python
# Option 1: WAL mode (better concurrency)
db.execute_sql("PRAGMA journal_mode=WAL")

# Option 2: Batch writes
with db.atomic():
    Chunk.bulk_create(chunks, batch_size=100)

# Option 3: Read replicas
# Master: writes
# Replicas: reads (search queries)
```

---

## üìâ Performance Degradation Points

### 1. Chunking Slowdown –Ω–∞ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–∞—Ö

**–¢–µ–∫—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ:**

```
Small files (2KB):  ~2 —Å–µ–∫ per file
Medium files (5KB): ~4 —Å–µ–∫ per file
Large files (50KB): ~40 —Å–µ–∫ per file (—ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è)
```

**–ü—Ä–∏—á–∏–Ω–∞:**

```python
# processing/parsers/markdown_node_parser.py
def parse(content: str) -> List[Node]:
    ast = markdown_it.parse(content)  # O(n)
    nodes = traverse_ast(ast)  # O(n)
    enriched = enrich_nodes(nodes)  # O(n¬≤) ‚ùå
    return nodes
```

**Hotspot:** `enrich_nodes()` –º–æ–∂–µ—Ç –±—ã—Ç—å O(n¬≤) –ø—Ä–∏ –±–æ–ª—å—à–æ–π –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏.

---

### 2. Search Latency Growth

**–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç corpus size:**

| Chunks | Vector Search | FTS5 Search | Hybrid | RRF Overhead |
|--------|---------------|-------------|--------|--------------|
| 100 | 50ms | 10ms | 60ms | +10ms |
| 1K | 100ms | 20ms | 120ms | +20ms |
| 10K | 500ms | 50ms | 600ms | +100ms |
| 100K | 2000ms | 200ms | 2500ms | +300ms |

**RRF overhead —Ä–∞—Å—Ç—ë—Ç** –ø—Ä–∏ merge –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ results.

**Optimization:**

```python
# Limit intermediate results
vector_results = vector_search(query, limit=100)  # Not 10K
fts_results = fts_search(query, limit=100)
rrf_results = rrf_merge(vector_results, fts_results, top_k=10)
```

---

## üõ°Ô∏è Mitigation Strategies

### Priority Matrix

| –†–∏—Å–∫ | Severity | Probability | Priority |
|------|----------|-------------|----------|
| Long video timeout | üî• High | Medium | **P0** |
| Hybrid search scores | üî• High | High | **P0** |
| Document-level search | ‚ö†Ô∏è Medium | High | **P1** |
| FTS granularity | ‚ö†Ô∏è Medium | Medium | **P1** |
| Duplicate chunks | üí∞ Low | High | **P2** |
| Rate limiting | ‚ö†Ô∏è Medium | Low | **P2** |

### Recommended Fixes

**P0 (Immediate):**
1. Fix RRF score normalization
2. Add adaptive video frame sampling

**P1 (Before Production):**
3. Implement document-level search API
4. Align FTS5 to chunk-level OR switch vector to doc-level

**P2 (Nice to Have):**
5. Add content hash deduplication
6. Implement batch ingestion via Batch API

---

## üìö –ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã

### ‚úÖ –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ production:

- ‚úÖ Chunking –¥–ª—è **–º–∞–ª—ã—Ö-—Å—Ä–µ–¥–Ω–∏—Ö —Ñ–∞–π–ª–æ–≤** (<50KB)
- ‚úÖ Media analysis –¥–ª—è **–∫–æ—Ä–æ—Ç–∫–∏—Ö –≤–∏–¥–µ–æ** (<5 –º–∏–Ω)
- ‚úÖ Vector search (–∏–≥–Ω–æ—Ä–∏—Ä—É—è hybrid)
- ‚úÖ Rate limiting –¥–ª—è **–æ–¥–∏–Ω–æ—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤**

### ‚ùå –ù–µ–ª—å–∑—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–µ–∑ —Ñ–∏–∫—Å–æ–≤:

- ‚ùå **Hybrid search** (–Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —Å–∫–æ—Ä—ã)
- ‚ùå **–î–ª–∏–Ω–Ω—ã–µ –≤–∏–¥–µ–æ** (timeout risk)
- ‚ùå **Bulk ingestion** (RPM bottleneck)
- ‚ùå **Document-level retrieval** (API –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç)

### üîÑ –¢—Ä–µ–±—É–µ—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:

- üìä RRF score distribution (–¥–æ–ª–∂–Ω—ã –±—ã—Ç—å 0.6-0.9)
- üìä Video processing time (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å <30 —Å–µ–∫ –¥–ª—è 5 –º–∏–Ω)
- üìä Duplicate rate (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å <5%)
- üìä Search latency (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å <500ms –¥–ª—è 10K chunks)

---

**Next Step:** [–û–±–Ω–æ–≤–∏—Ç—å 00_overview.md](#update-overview) —Å —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –≥–ª–∞–≤—ã 62-64.
