# 83. MediaService.reprocess_document() ‚Äî –ü–æ–≤—Ç–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–µ–¥–∏–∞

**–§–∞–∑–∞:** Phase 14.3.3 (Configuration & Flexibility)  
**–î–∞—Ç–∞:** 2025-12-06  
**–ö–æ–º–º–∏—Ç:** `65f060b`  
**–ü—Ä–µ–¥—ã–¥—É—â–∞—è —Å—Ç–∞—Ç—å—è:** [82. Configuration & Template Injection](82_configuration_template_injection.md)

---

## üéØ –ü—Ä–æ–±–ª–µ–º–∞

–ü–æ—Å–ª–µ Phase 14.3.1-14.3.2 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –º–æ–∂–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å –ø—Ä–æ–º–ø—Ç—ã –∏ chunk sizes –≤ `semantic.toml`, –Ω–æ **–∫–∞–∫ –ø—Ä–∏–º–µ–Ω–∏—Ç—å –∏—Ö –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º?**

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–∞—è –æ—à–∏–±–∫–∞

**–ü–µ—Ä–≤–∞—è –ø–æ–ø—ã—Ç–∫–∞ (–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ):**

```python
# ‚ùå –ü–õ–û–•–û: –õ–æ–≥–∏–∫–∞ reprocess –≤ SemanticCore
class SemanticCore:
    def reanalyze(self, document_id: str):
        # 1. –î–æ—Å—Ç–∞—Ç—å media_path –∏–∑ MediaTaskModel.file_path
        task = MediaTaskModel.get(result_document_id=document_id)
        media_path = Path(task.file_path)  # ‚Üê –ü–†–û–ë–õ–ï–ú–ê!
        
        # 2. –£–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏
        ChunkModel.delete().where(ChunkModel.document == document_id).execute()
        
        # 3. –ü–æ–≤—Ç–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑...
```

**–î–≤–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏:**

| –ü—Ä–æ–±–ª–µ–º–∞ | –ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è |
|----------|-------------|
| **MediaTaskModel –∫–∞–∫ –∏—Å—Ç–æ—á–Ω–∏–∫ –ø—Ä–∞–≤–¥—ã** | –ï—Å–ª–∏ —á–∏—Å—Ç–∏—Ç—å —Å—Ç–∞—Ä—ã–µ tasks ‚Üí –ø–æ—Ç–µ—Ä—è –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª—É |
| **–õ–æ–≥–∏–∫–∞ –≤ SemanticCore** | –ù–∞—Ä—É—à–µ–Ω–∏–µ SRP (Single Responsibility Principle) |

**–°—Ü–µ–Ω–∞—Ä–∏–π –ø—Ä–æ–≤–∞–ª–∞:**

```sql
-- –ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä —á–∏—Å—Ç–∏—Ç —Å—Ç–∞—Ä—ã–µ –∑–∞–¥–∞—á–∏
DELETE FROM media_tasks WHERE processed_at < NOW() - INTERVAL '30 days';

-- –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—ã—Ç–∞–µ—Ç—Å—è reanalyze
>>> core.reanalyze("doc-123")
DoesNotExist: MediaTask matching query does not exist.
```

---

## ‚úÖ –†–µ—à–µ–Ω–∏–µ

### 1. Single Source of Truth

**–ò—Å–ø–æ–ª—å–∑—É–µ–º `Document.metadata["source"]`** ‚Äî –ø—É—Ç—å –∫ –º–µ–¥–∏–∞ —É–∂–µ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏–Ω–∂–µ—Å—Ç–µ:

```python
# semantic_core/pipeline.py (—Å—Ç—Ä–æ–∫–∏ 680, 709, 838)
metadata = {"source": str(path)}  # ‚Üê –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –≤ metadata!
```

**–ü–æ—á–µ–º—É —ç—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ:**

- ‚úÖ Document –∂–∏–≤—ë—Ç –≤–µ—á–Ω–æ (–ø–æ–∫–∞ –Ω–µ —É–¥–∞–ª—ë–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º)
- ‚úÖ MediaTask ‚Äî –≤—Ä–µ–º–µ–Ω–Ω–∞—è —Å—É—â–Ω–æ—Å—Ç—å (–æ—á–µ—Ä–µ–¥—å –æ–±—Ä–∞–±–æ—Ç–∫–∏)
- ‚úÖ –ú–æ–∂–Ω–æ —á–∏—Å—Ç–∏—Ç—å tasks –±–µ–∑ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏–π

### 2. SRP Compliance

**–õ–æ–≥–∏–∫–∞ –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∏ ‚Üí `MediaService`** (–ù–ï SemanticCore):

```mermaid
graph TB
    A[SemanticCore.reanalyze] -->|–¢–æ–Ω–∫–∞—è –ø—Ä–æ–∫—Å–∏| B[MediaService.reprocess_document]
    B --> C[–ó–∞–≥—Ä—É–∑–∫–∞ Document –∏–∑ –ë–î]
    C --> D[–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ media_path –∏–∑ metadata]
    D --> E[–£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —á–∞–Ω–∫–æ–≤]
    E --> F[–ü–æ–≤—Ç–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ Gemini]
    F --> G[–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤]
    G --> H[–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î]
```

**–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏:**

| –ö–ª–∞—Å—Å | –†–æ–ª—å |
|-------|------|
| **SemanticCore** | –§–∞—Å–∞–¥ (–¥–µ–ª–µ–≥–∏—Ä—É–µ—Ç MediaService) |
| **MediaService** | –ê–≥—Ä–µ–≥–∞—Ü–∏—è + –ü–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∞ –º–µ–¥–∏–∞ |
| **MediaPipeline** | –°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ |

---

## üèó –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

### MediaService: –î–æ –∏ –ü–æ—Å–ª–µ

**–î–æ Phase 14.3.3:**

```python
class MediaService:
    """–¢–æ–ª—å–∫–æ –∞–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö."""
    
    def get_media_details(self, document_id: str) -> MediaDetails:
        # –°–æ–±–∏—Ä–∞–µ—Ç summary + transcript + OCR –≤ DTO
        pass
```

**–ü–æ—Å–ª–µ Phase 14.3.3:**

```python
class MediaService:
    """–ê–≥—Ä–µ–≥–∞—Ü–∏—è + –ü–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∞."""
    
    def __init__(
        self,
        image_analyzer: Optional[GeminiImageAnalyzer] = None,
        audio_analyzer: Optional[GeminiAudioAnalyzer] = None,
        video_analyzer: Optional[GeminiVideoAnalyzer] = None,
        splitter: Optional[BaseSplitter] = None,
        store: Optional[BaseVectorStore] = None,
        config: Optional[SemanticConfig] = None,
    ):
        # –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è reprocess_document()
        pass
    
    def reprocess_document(
        self,
        document_id: str,
        custom_instructions: Optional[str] = None,
    ) -> Document:
        # –õ–æ–≥–∏–∫–∞ –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∏
        pass
```

**–ó–∞—á–µ–º –Ω—É–∂–Ω—ã –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏?**

| –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å | –î–ª—è —á–µ–≥–æ |
|-------------|----------|
| `image_analyzer` / `audio_analyzer` / `video_analyzer` | –ü–æ–≤—Ç–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ Gemini |
| `splitter` | –ù–∞—Ä–µ–∑–∫–∞ transcript/OCR –Ω–∞ —á–∞–Ω–∫–∏ |
| `store` | –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤ –≤ –ë–î |
| `config` | chunk_sizes, enable_timecodes, ocr_parser_mode |

---

## üîÑ –ê–ª–≥–æ—Ä–∏—Ç–º reprocess_document()

**8 —à–∞–≥–æ–≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∏:**

```mermaid
sequenceDiagram
    participant User
    participant MediaService
    participant DocumentModel
    participant Analyzer
    participant MediaPipeline
    participant VectorStore
    
    User->>MediaService: reprocess_document("doc-123", "Extract medical terms")
    MediaService->>DocumentModel: get_by_id("doc-123")
    DocumentModel-->>MediaService: doc_model (media_type="audio", metadata={"source": "/audio.mp3"})
    
    MediaService->>MediaService: –ü—Ä–æ–≤–µ—Ä–∫–∞ media_type ‚àà {image, audio, video}
    MediaService->>MediaService: –ò–∑–≤–ª–µ—á—å media_path –∏–∑ metadata["source"]
    MediaService->>MediaService: Path.exists() ‚Äî –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–∞
    
    MediaService->>MediaService: _delete_media_chunks("doc-123")
    Note right of MediaService: –£–¥–∞–ª—è–µ—Ç —á–∞–Ω–∫–∏ —Å role ‚àà {summary, transcript, ocr}
    
    MediaService->>Analyzer: analyze(media_path, custom_instructions)
    Analyzer-->>MediaService: new_analysis = {"description": "...", "transcription": "..."}
    
    MediaService->>MediaPipeline: build_chunks(context)
    MediaPipeline-->>MediaService: new_chunks (summary + transcript chunks)
    
    MediaService->>VectorStore: save(document, new_chunks)
    VectorStore-->>User: ‚úÖ Document reprocessed
```

**–î–µ—Ç–∞–ª–∏ —à–∞–≥–æ–≤:**

| –®–∞–≥ | –î–µ–π—Å—Ç–≤–∏–µ | –í–∞–ª–∏–¥–∞—Ü–∏—è |
|-----|----------|-----------|
| 1Ô∏è‚É£ | –ó–∞–≥—Ä—É–∑–∫–∞ `DocumentModel.get_by_id()` | `DoesNotExist` ‚Üí ValueError |
| 2Ô∏è‚É£ | –ü—Ä–æ–≤–µ—Ä–∫–∞ `media_type` | –ï—Å–ª–∏ TEXT ‚Üí ValueError |
| 3Ô∏è‚É£ | –ò–∑–≤–ª–µ—á—å `metadata["source"]` | –ï—Å–ª–∏ –Ω–µ—Ç ‚Äî ValueError |
| 4Ô∏è‚É£ | –ü—Ä–æ–≤–µ—Ä–∫–∞ `Path.exists()` | –§–∞–π–ª —É–¥–∞–ª—ë–Ω ‚Üí FileNotFoundError |
| 5Ô∏è‚É£ | –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —á–∞–Ω–∫–æ–≤ | WHERE role IN ('summary', 'transcript', 'ocr') |
| 6Ô∏è‚É£ | –ü–æ–≤—Ç–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ | analyzer.analyze(media_path, custom_instructions) |
| 7Ô∏è‚É£ | –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤ | MediaPipeline —Å config.media.chunk_sizes |
| 8Ô∏è‚É£ | –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î | store.save(document) |

---

## üìã –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –ü—Ä–∏–º–µ—Ä 1: –£–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞

**–°—Ü–µ–Ω–∞—Ä–∏–π:** –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –æ–±–Ω–æ–≤–∏–ª –ø—Ä–æ–º–ø—Ç –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –ª–µ–∫—Ü–∏–π.

```toml
# semantic.toml
[media.prompts]
audio_summary = """
Extract: diagnoses, medications, dosages, contraindications.
Focus on medical terminology.
"""
```

**–ü–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∞:**

```python
from semantic_core import SemanticCore

core = SemanticCore()

# –ü–æ–≤—Ç–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –Ω–æ–≤—ã–º –ø—Ä–æ–º–ø—Ç–æ–º
core.reanalyze(
    document_id="doc-123",
    custom_instructions="Extract medical terminology",
)

# –†–µ–∑—É–ª—å—Ç–∞—Ç:
# - –°—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏ —É–¥–∞–ª–µ–Ω—ã
# - –ù–æ–≤—ã–π analysis —Å –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏
# - –ù–æ–≤—ã–µ —á–∞–Ω–∫–∏ —Å–æ–∑–¥–∞–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã
```

### –ü—Ä–∏–º–µ—Ä 2: –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç —Å chunk sizes

**–°—Ü–µ–Ω–∞—Ä–∏–π:** –í–∏–¥–µ–æ-—Ç—É—Ç–æ—Ä–∏–∞–ª —Å –∫–æ–¥–æ–º ‚Äî —Ö–æ—Ç–∏–º –±–æ–ª—å—à–∏–µ —á–∞–Ω–∫–∏ –¥–ª—è OCR.

```toml
[media.chunk_sizes]
ocr_text_chunk_size = 3000  # –ë—ã–ª–æ 1800
```

**–ü–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∞:**

```python
# –ü–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –í–°–ï –≤–∏–¥–µ–æ —Å –Ω–æ–≤—ã–º chunk_size
video_docs = Document.select().where(Document.media_type == "video")

for doc in video_docs:
    core.reanalyze(document_id=doc.id)
    print(f"‚úÖ {doc.id} reprocessed with new chunk sizes")
```

### –ü—Ä–∏–º–µ—Ä 3: –ú–∏–≥—Ä–∞—Ü–∏—è –Ω–∞ –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å Gemini

**–°—Ü–µ–Ω–∞—Ä–∏–π:** –í—ã—à–µ–ª `gemini-3.0-pro` —Å –ª—É—á—à–µ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–µ–π.

```python
# –û–±–Ω–æ–≤–ª—è–µ–º model –≤ –∫–æ–Ω—Ñ–∏–≥–µ
config.gemini.model_name = "gemini-3.0-pro"

# –ü–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
important_docs = ["lecture-01", "interview-02", "podcast-03"]

for doc_id in important_docs:
    core.reanalyze(document_id=doc_id)
```

---

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

**9 unit-—Ç–µ—Å—Ç–æ–≤ —Å –º–æ–∫–∞–º–∏ (100% coverage):**

| –¢–µ—Å—Ç | –ü—Ä–æ–≤–µ—Ä—è–µ—Ç |
|------|-----------|
| `test_reprocess_document_requires_dependencies` | ValueError –µ—Å–ª–∏ –Ω–µ—Ç splitter/store/config |
| `test_reprocess_document_not_found` | ValueError –µ—Å–ª–∏ document_id –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç |
| `test_reprocess_document_not_media_file` | ValueError –µ—Å–ª–∏ media_type = "text" |
| `test_reprocess_document_missing_source_metadata` | ValueError –µ—Å–ª–∏ –Ω–µ—Ç metadata["source"] |
| `test_reprocess_document_file_not_found` | FileNotFoundError –µ—Å–ª–∏ —Ñ–∞–π–ª —É–¥–∞–ª—ë–Ω |
| `test_reprocess_document_audio_success` | –£—Å–ø–µ—à–Ω–∞—è –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ |
| `test_reprocess_document_deletes_old_chunks` | –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –º–µ–¥–∏–∞-—á–∞–Ω–∫–æ–≤ |
| `test_reprocess_document_calls_correct_analyzer` | –í—ã–±–æ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ analyzer (IMAGE/AUDIO/VIDEO) |
| `test_reprocess_document_no_analyzer_raises_error` | ValueError –µ—Å–ª–∏ –Ω–µ—Ç –Ω—É–∂–Ω–æ–≥–æ analyzer |

**–†–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ—Å—Ç–æ–≤:**

```bash
tests/unit/services/test_media_service_reprocess.py::test_* PASSED [100%]
========================================== 9 passed in 0.07s ===========================================
```

---

## üéØ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –≥–∞—Ä–∞–Ω—Ç–∏–∏

### 1. Single Responsibility Principle (SRP)

```mermaid
graph LR
    A[SemanticCore] -->|4 —Å—Ç—Ä–æ–∫–∏ proxy| B[MediaService]
    B --> C[–í—Å—è –ª–æ–≥–∏–∫–∞ reprocess]
```

**SemanticCore.reanalyze() ‚Äî –≤—Å–µ–≥–æ 4 —Å—Ç—Ä–æ–∫–∏:**

```python
def reanalyze(self, document_id: str, custom_instructions: Optional[str] = None):
    """–¢–æ–Ω–∫–∞—è –ø—Ä–æ–∫—Å–∏ –¥–ª—è MediaService.reprocess_document()."""
    media_service = MediaService(
        image_analyzer=self.image_analyzer,
        audio_analyzer=self.audio_analyzer,
        video_analyzer=self.video_analyzer,
        splitter=self.splitter,
        store=self.store,
        config=self.config,
    )
    return media_service.reprocess_document(document_id, custom_instructions)
```

### 2. Single Source of Truth

| –ò—Å—Ç–æ—á–Ω–∏–∫ –ø—Ä–∞–≤–¥—ã | –î–ª—è —á–µ–≥–æ | –í—Ä–µ–º—è –∂–∏–∑–Ω–∏ |
|-----------------|----------|-------------|
| `Document.metadata["source"]` | –ü—É—Ç—å –∫ –º–µ–¥–∏–∞-—Ñ–∞–π–ª—É | –ü–æ–∫–∞ Document –Ω–µ —É–¥–∞–ª—ë–Ω |
| ~~`MediaTaskModel.file_path`~~ | ‚ùå –ù–ï –ò–°–ü–û–õ–¨–ó–û–í–ê–¢–¨ | –í—Ä–µ–º–µ–Ω–Ω–∞—è —Å—É—â–Ω–æ—Å—Ç—å (–æ—á–µ—Ä–µ–¥—å) |

### 3. –ê—Ç–æ–º–∞—Ä–Ω–æ—Å—Ç—å —É–¥–∞–ª–µ–Ω–∏—è

**–ß–∞–Ω–∫–∏ —É–¥–∞–ª—è—é—Ç—Å—è –ü–ï–†–ï–î —Å–æ–∑–¥–∞–Ω–∏–µ–º –Ω–æ–≤—ã—Ö:**

```python
# 1. –£–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—ã–µ
deleted_count = self._delete_media_chunks(document_id)

# 2. –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–µ
new_chunks = self._build_chunks_via_pipeline(...)

# 3. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å
self.store.save(document)
```

**–ü–æ—á–µ–º—É –≤–∞–∂–Ω–æ:**

- ‚úÖ –ù–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —á–∞–Ω–∫–æ–≤ (—Å—Ç–∞—Ä—ã–µ + –Ω–æ–≤—ã–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ)
- ‚úÖ –ï—Å–ª–∏ –Ω–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —É–ø–∞–¥—ë—Ç ‚Üí —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏ —É–∂–µ —É–¥–∞–ª–µ–Ω—ã (—á–∏—Å—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è retry)
- ‚ö†Ô∏è –í—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ—Ç —á–∞–Ω–∫–æ–≤ (–º–µ–∂–¥—É —É–¥–∞–ª–µ–Ω–∏–µ–º –∏ —Å–æ–∑–¥–∞–Ω–∏–µ–º) ‚Äî –Ω–æ —ç—Ç–æ –ª—É—á—à–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤

### 4. –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ

```mermaid
graph TD
    A[document_id] --> B{Document exists?}
    B -->|No| C[ValueError: not found]
    B -->|Yes| D{media_type ‚àà image/audio/video?}
    D -->|No| E[ValueError: not media]
    D -->|Yes| F{metadata has 'source'?}
    F -->|No| G[ValueError: no source]
    F -->|Yes| H{File exists?}
    H -->|No| I[FileNotFoundError]
    H -->|Yes| J{Has analyzer?}
    J -->|No| K[ValueError: no analyzer]
    J -->|Yes| L[‚úÖ Proceed with reprocess]
```

---

## üîó –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥

**Phase 14.3.4: CLI Integration** ‚Äî –∫–æ–º–∞–Ω–¥–∞ `semantic reanalyze <document_id>`

**Phase 14.3.5: Documentation** ‚Äî –ø—Ä–∏–º–µ—Ä—ã –∫–æ–Ω—Ñ–∏–≥–æ–≤ –≤ docs/

---

## üìå –ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã

1. **Document.metadata["source"]** ‚Äî –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –ø—Ä–∞–≤–¥—ã –¥–ª—è media_path
2. **MediaService** –≤–ª–∞–¥–µ–µ—Ç –ª–æ–≥–∏–∫–æ–π –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∏ (SRP)
3. **SemanticCore** –æ—Å—Ç–∞—ë—Ç—Å—è —Ç–æ–Ω–∫–∏–º —Ñ–∞—Å–∞–¥–æ–º (4 —Å—Ç—Ä–æ–∫–∏)
4. **–ê—Ç–æ–º–∞—Ä–Ω–æ—Å—Ç—å**: —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —á–∞–Ω–∫–æ–≤ –ü–ï–†–ï–î —Å–æ–∑–¥–∞–Ω–∏–µ–º –Ω–æ–≤—ã—Ö
5. **9 unit-—Ç–µ—Å—Ç–æ–≤** –ø–æ–∫—Ä—ã–≤–∞—é—Ç –≤—Å–µ edge cases

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –º–æ–∂–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –ª—é–±–æ–π –º–µ–¥–∏–∞-—Ñ–∞–π–ª —Å –Ω–æ–≤—ã–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏/chunk sizes –±–µ–∑ —Ä–∏—Å–∫–∞ –ø–æ—Ç–µ—Ä–∏ –¥–∞–Ω–Ω—ã—Ö.
