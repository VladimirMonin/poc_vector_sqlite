# üî• Phase 14.0: The Critical Fix ‚Äî –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ—Ç–µ—Ä–∏ –¥–∞–Ω–Ω—ã—Ö

**–î–∞—Ç–∞:** 2025-12-06  
**–°—Ç–∞—Ç—É—Å:** ‚úÖ **–ß–ê–°–¢–ò–ß–ù–û –†–ï–ê–õ–ò–ó–û–í–ê–ù–û** | ‚ùå **OCR Markdown parsing –ù–ï –ó–ê–í–ï–†–®–Å–ù**  
**–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:** Phase 12 (max_output_tokens fix), Phase 2-3 (SmartSplitter + Config)  
**–¶–µ–ª—å:** –ü–µ—Ä–µ–π—Ç–∏ –æ—Ç "1 —Ñ–∞–π–ª = 1 —á–∞–Ω–∫" –∫ "1 —Ñ–∞–π–ª = –î–µ—Ä–µ–≤–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤"

---

## üìã –û–≥–ª–∞–≤–ª–µ–Ω–∏–µ

1. [–ü—Ä–æ–±–ª–µ–º—ã —Ç–µ–∫—É—â–µ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã](#1-–ø—Ä–æ–±–ª–µ–º—ã-—Ç–µ–∫—É—â–µ–π-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã)
2. [–ß—Ç–æ —É–∂–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ](#2-—á—Ç–æ-—É–∂–µ-–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ)
3. [–ß—Ç–æ –ù–ï –∑–∞–≤–µ—Ä—à–µ–Ω–æ](#3-—á—Ç–æ-–Ω–µ-–∑–∞–≤–µ—Ä—à–µ–Ω–æ)
4. [–ü–ª–∞–Ω –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è Phase 14.0](#4-–ø–ª–∞–Ω-–∑–∞–≤–µ—Ä—à–µ–Ω–∏—è-phase-140)
5. [–†–µ–∞–ª–∏–∑–∞—Ü–∏—è ‚Äî Step by Step](#5-—Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è--step-by-step)
6. [Validation & Testing](#6-validation--testing)
7. [–†–∏—Å–∫–∏ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥](#7-—Ä–∏—Å–∫–∏-–∏-–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥)

---

## 1. –ü—Ä–æ–±–ª–µ–º—ã —Ç–µ–∫—É—â–µ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

### 1.1 –°–∏–º–ø—Ç–æ–º—ã

–ü—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ **3-–º–∏–Ω—É—Ç–Ω–æ–≥–æ –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞**:

- ‚ùå –í –ë–î —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è **1 —á–∞–Ω–∫** –≤–º–µ—Å—Ç–æ –æ–∂–∏–¥–∞–µ–º—ã—Ö 6-8
- ‚ùå –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ **~50 —Å–µ–∫—É–Ω–¥** –∏–∑ 180 —Å–µ–∫—É–Ω–¥
- ‚ùå Semantic search –Ω–∞—Ö–æ–¥–∏—Ç —Ç–æ–ª—å–∫–æ –Ω–∞—á–∞–ª–æ –∞—É–¥–∏–æ
- ‚ùå Code –≤ –≤–∏–¥–µ–æ-—Å–∫—Ä–∏–Ω–∫–∞—Å—Ç–∞—Ö –Ω–µ –¥–µ—Ç–µ–∫—Ç–∏—Ç—Å—è (plain text –≤–º–µ—Å—Ç–æ CODE chunks)

### 1.2 –ö–æ—Ä–Ω–µ–≤—ã–µ –ø—Ä–∏—á–∏–Ω—ã

| –ü—Ä–æ–±–ª–µ–º–∞ | –§–∞–π–ª | –°—Ç–∞—Ç—É—Å |
|----------|------|--------|
| **max_output_tokens=8192** (–ª–∏–º–∏—Ç 8x –º–µ–Ω—å—à–µ –º–æ–¥–µ–ª–∏) | `*_analyzer.py` | ‚úÖ **–ò–°–ü–†–ê–í–õ–ï–ù–û** (‚Üí 65,536) |
| **1 —á–∞–Ω–∫ –Ω–∞ –º–µ–¥–∏–∞** (–±–µ–∑ SmartSplitter) | `pipeline.py` | ‚úÖ **–ò–°–ü–†–ê–í–õ–ï–ù–û** (`_build_media_chunks()`) |
| **OCR uses MediaType.TEXT** (code –Ω–µ –¥–µ—Ç–µ–∫—Ç–∏—Ç—Å—è) | `pipeline.py:1501` | ‚ùå **–ù–ï –ò–°–ü–†–ê–í–õ–ï–ù–û** |
| **–ü—Ä–æ–º–ø—Ç—ã –Ω–µ —Ç—Ä–µ–±—É—é—Ç Markdown** | `*_analyzer.py` | ‚ùå **–ù–ï –ò–°–ü–†–ê–í–õ–ï–ù–û** |

---

## 2. –ß—Ç–æ —É–∂–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ ‚úÖ

### 2.1 –°–Ω—è—Ç–∏–µ –ª–∏–º–∏—Ç–æ–≤ max_output_tokens

**–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ –≤:** Phase 12

**–§–∞–π–ª—ã:**
- `semantic_core/infrastructure/gemini/audio_analyzer.py:85`
- `semantic_core/infrastructure/gemini/video_analyzer.py:109`

**–ò–∑–º–µ–Ω–µ–Ω–∏–µ:**
```python
# –ë–´–õ–û (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≤ 8x):
max_output_tokens: int = 8_192

# –°–¢–ê–õ–û (–ø–æ–ª–Ω—ã–π –ª–∏–º–∏—Ç –º–æ–¥–µ–ª–∏):
max_output_tokens: int = 65_536
```

**–≠—Ñ—Ñ–µ–∫—Ç:**
- Gemini –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å ~50,000 —Å–ª–æ–≤ (~130 –º–∏–Ω—É—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏)
- –ú–æ–¥–µ–ª—å –±–æ–ª—å—à–µ –Ω–µ –æ–±—Ä–µ–∑–∞–µ—Ç –¥–ª–∏–Ω–Ω—ã–µ –∞—É–¥–∏–æ

---

### 2.2 –í–Ω–µ–¥—Ä–µ–Ω–∏–µ SmartSplitter –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π –∏ OCR

**–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ –≤:** –¢–µ–∫—É—â–∞—è —Å–µ—Å—Å–∏—è (bugfix video reindex)

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**

#### –ú–µ—Ç–æ–¥ `_build_media_chunks()` (pipeline.py:1394-1454)

```python
def _build_media_chunks(
    self,
    document: Document,
    media_path: Path,
    chunk_type: ChunkType,
    analysis: Optional[dict],
    fallback_metadata: Optional[dict] = None,
) -> list[Chunk]:
    """–§–æ—Ä–º–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –¥–ª—è –º–µ–¥–∏–∞: summary + transcript + OCR."""
    
    # 1. –°–æ–∑–¥–∞—ë–º summary chunk (role='summary')
    summary_metadata = self._build_metadata_from_analysis(analysis, media_path)
    summary_metadata["role"] = "summary"
    
    chunks = [
        Chunk(
            content=self._build_content_from_analysis(analysis),
            chunk_index=0,
            chunk_type=chunk_type,
            metadata=summary_metadata,
        )
    ]
    
    # 2. –†–∞–∑–±–∏–≤–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é —á–µ—Ä–µ–∑ SmartSplitter
    transcription = analysis.get("transcription")
    if transcription:
        transcript_chunks = self._split_transcription_into_chunks(
            transcription=transcription,
            base_index=len(chunks),
            media_path=media_path,
        )
        chunks.extend(transcript_chunks)
    
    # 3. –†–∞–∑–±–∏–≤–∞–µ–º OCR —á–µ—Ä–µ–∑ SmartSplitter
    ocr_text = analysis.get("ocr_text")
    if ocr_text:
        ocr_chunks = self._split_ocr_into_chunks(
            ocr_text=ocr_text,
            base_index=len(chunks),
            media_path=media_path,
        )
        chunks.extend(ocr_chunks)
    
    return chunks
```

#### –ú–µ—Ç–æ–¥ `_split_transcription_into_chunks()` (pipeline.py:1456-1482)

```python
def _split_transcription_into_chunks(
    self,
    transcription: str,
    base_index: int,
    media_path: Path,
) -> list[Chunk]:
    """–†–µ–∂–µ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –Ω–∞ —á–∞–Ω–∫–∏ —á–µ—Ä–µ–∑ SmartSplitter."""
    
    # –°–æ–∑–¥–∞—ë–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π Document
    temp_doc = Document(
        content=transcription,
        metadata={"source": str(media_path)},
        media_type=MediaType.TEXT,
    )
    
    # –†–µ–∂–µ–º —á–µ—Ä–µ–∑ splitter (chunk_size=1800 –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞)
    split_chunks = self.splitter.split(temp_doc)
    
    # –û–±–æ–≥–∞—â–∞–µ–º metadata
    for idx, chunk in enumerate(split_chunks):
        meta = dict(chunk.metadata or {})
        meta.setdefault("_original_path", str(media_path))
        meta["role"] = "transcript"
        meta["parent_media_path"] = str(media_path)
        
        chunk.chunk_index = base_index + idx
        chunk.metadata = meta
    
    return split_chunks
```

#### –ú–µ—Ç–æ–¥ `_split_ocr_into_chunks()` (pipeline.py:1484-1518)

```python
def _split_ocr_into_chunks(
    self,
    ocr_text: str,
    base_index: int,
    media_path: Path,
) -> list[Chunk]:
    """–†–µ–∂–µ—Ç OCR —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —á–µ—Ä–µ–∑ SmartSplitter."""
    
    temp_doc = Document(
        content=ocr_text,
        metadata={"source": str(media_path)},
        media_type=MediaType.TEXT,  # ‚ùå –ü–†–û–ë–õ–ï–ú–ê: –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å MARKDOWN!
    )
    
    split_chunks = self.splitter.split(temp_doc)
    
    for idx, chunk in enumerate(split_chunks):
        meta = dict(chunk.metadata or {})
        meta.setdefault("_original_path", str(media_path))
        meta["role"] = "ocr"
        meta["parent_media_path"] = str(media_path)
        
        chunk.chunk_index = base_index + idx
        chunk.metadata = meta
    
    return split_chunks
```

**–ß—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç:**

‚úÖ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ N —á–∞–Ω–∫–æ–≤ (–Ω–µ 1 –≥–∏–≥–∞–Ω—Ç—Å–∫–∏–π)  
‚úÖ OCR —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ N —á–∞–Ω–∫–æ–≤  
‚úÖ `role="summary"/"transcript"/"ocr"` metadata –¥–æ–±–∞–≤–ª–µ–Ω–∞  
‚úÖ Chunk indexes –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã (0, 1, 2, 3...)  
‚úÖ `parent_media_path` —Å–≤—è–∑—ã–≤–∞–µ—Ç —á–∞–Ω–∫–∏ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º —Ñ–∞–π–ª–æ–º

---

### 2.3 –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è chunk_size

**–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ –≤:** Phase 2-3 (SOLID —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥)

**–§–∞–π–ª—ã:**
- `semantic_core/config.py:138-158`
- `semantic_core/cli/context.py:159-163`

**–î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ:**

```python
# config.py
class SemanticConfig(BaseSettings):
    chunk_size: int = Field(default=1800, ge=500, le=8000)
    code_chunk_size: int = Field(default=2000, ge=500, le=10000)

# cli/context.py
splitter = SmartSplitter(
    parser=parser,
    chunk_size=config.chunk_size,        # ‚Üê –ß–∏—Ç–∞–µ—Ç –∏–∑ semantic.toml
    code_chunk_size=config.code_chunk_size,
)
```

**–≠—Ñ—Ñ–µ–∫—Ç:**
- ‚úÖ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –º–æ–∂–µ—Ç –º–µ–Ω—è—Ç—å `chunk_size` —á–µ—Ä–µ–∑ `semantic.toml`
- ‚úÖ SmartSplitter –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è

---

## 3. –ß—Ç–æ –ù–ï –∑–∞–≤–µ—Ä—à–µ–Ω–æ ‚ùå

### 3.1 OCR Markdown Parsing

**–ü—Ä–æ–±–ª–µ–º–∞:** –í–∏–¥–µ–æ —Å –∫–æ–¥–æ–º (—Å–∫—Ä–∏–Ω–∫–∞—Å—Ç—ã, —Ç—É—Ç–æ—Ä–∏–∞–ª—ã) ‚Üí –∫–æ–¥ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –∫–∞–∫ plain text.

**–¢–µ–∫—É—â–µ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ:**

```python
# Gemini OCR output:
ocr_text = """
Function Example
def calculate_total(items):
    return sum(item.price for item in items)
This function iterates...
"""

# SmartSplitter –ø–∞—Ä—Å–∏—Ç –∫–∞–∫ plain TEXT (media_type=MediaType.TEXT)
# –†–µ–∑—É–ª—å—Ç–∞—Ç: 1 –±–æ–ª—å—à–æ–π TEXT chunk, CODE BLOCKS –ù–ï –î–ï–¢–ï–ö–¢–Ø–¢–°–Ø
```

**–¶–µ–ª–µ–≤–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ:**

```python
# Gemini OCR output (Markdown):
ocr_text = """
## Function Example

```python
def calculate_total(items):
    return sum(item.price for item in items)
```

This function iterates...
"""

# SmartSplitter –ø–∞—Ä—Å–∏—Ç –∫–∞–∫ MARKDOWN (media_type=MediaType.MARKDOWN)
# –†–µ–∑—É–ª—å—Ç–∞—Ç:
# - Chunk 1: ChunkType.TEXT, content="## Function Example"
# - Chunk 2: ChunkType.CODE, language="python", content="def calculate..."
# - Chunk 3: ChunkType.TEXT, content="This function iterates..."
```

**–†–µ—à–µ–Ω–∏–µ:** –ò–∑–º–µ–Ω–∏—Ç—å 1 —Å—Ç—Ä–æ–∫—É –≤ `pipeline.py:1501`

```python
# –ë–´–õ–û:
media_type=MediaType.TEXT,

# –î–û–õ–ñ–ù–û –ë–´–¢–¨:
media_type=MediaType.MARKDOWN,
```

**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** üî¥ **P0** (–∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –≤–∏–¥–µ–æ)

---

### 3.2 –ü—Ä–æ–º–ø—Ç—ã –Ω–µ —Ç—Ä–µ–±—É—é—Ç Markdown

**–ü—Ä–æ–±–ª–µ–º–∞:** –¢–µ–∫—É—â–∏–µ –ø—Ä–æ–º–ø—Ç—ã –Ω–µ –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏—Ä—É—é—Ç Gemini —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã –≤ Markdown.

**–¢–µ–∫—É—â–∏–π –ø—Ä–æ–º–ø—Ç (audio_analyzer.py:37):**

```python
SYSTEM_PROMPT_TEMPLATE = """You are an audio analyst creating descriptions for semantic search indexing.

Analyze the audio and provide:
1. transcription: Full transcript of the spoken content
2. description: Summary of the audio content (2-4 sentences)
...

Output valid JSON matching the schema.
Answer in {language} language."""
```

**–ü—Ä–æ–±–ª–µ–º—ã:**
- ‚ùå –ù–µ—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –ø–æ Markdown —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—é
- ‚ùå –î–ª–∏–Ω–Ω—ã–µ –ª–µ–∫—Ü–∏–∏ ‚Üí "–ø—Ä–æ—Å—Ç—ã–Ω—è —Ç–µ–∫—Å—Ç–∞" –±–µ–∑ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
- ‚ùå Code snippets –≤ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è—Ö –Ω–µ –≤—ã–¥–µ–ª—è—é—Ç—Å—è

**–¶–µ–ª–µ–≤–æ–π –ø—Ä–æ–º–ø—Ç:**

```python
SYSTEM_PROMPT_TEMPLATE = """You are an audio analyst creating descriptions for semantic search indexing.
Response language: {language}

Return a JSON with:

{{
  "description": "Brief 2-3 sentence summary",
  "keywords": ["keyword1", ...],
  "participants": ["Speaker1", ...],
  "action_items": ["Task 1", ...],
  "transcription": "MARKDOWN_FORMATTED_TRANSCRIPT"
}}

CRITICAL INSTRUCTIONS FOR TRANSCRIPTION FIELD:
- Use Markdown formatting (paragraphs, headers, lists)
- Split long monologues into logical paragraphs (every 3-5 sentences)
- Use `## Speaker Name` headers for speaker changes
- Use `**bold**` for emphasis or key terms
- For code snippets, wrap in triple backticks with language:
  ```python
  def example():
      pass
  ```
- DO NOT escape newlines as \\n ‚Äî use actual line breaks

Example:

## Introduction

The speaker introduces semantic search.

Key points:
- Embeddings capture meaning
- Vector databases enable similarity

## Technical Deep Dive

Here's the similarity formula:

```python
def cosine_similarity(a, b):
    return dot(a, b) / (norm(a) * norm(b))
```

This is fundamental to vector search.
"""
```

**–ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥–ª—è video_analyzer.py ‚Äî OCR —Å–µ–∫—Ü–∏—è:**

```python
CRITICAL INSTRUCTIONS FOR OCR_TEXT FIELD:
- Detect and preserve code blocks from screenshots
- Wrap code in triple backticks with language
- Use `## Slide Title` headers for new slides
- Use bullet points for slide lists
- For UI text (buttons), use plain text

Example OCR:

## Introduction to SOLID

### Single Responsibility Principle

A class should have only one reason to change.

**Example:**

```python
class UserService:
    def validate(self, user): ...
    def save(self, user): ...
```

**Problem:** Mixes validation and persistence.
"""
```

**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** üü° **P1** (—É–ª—É—á—à–∞–µ—Ç quality, –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è —Ä–∞–±–æ—Ç—ã)

---

## 4. –ü–ª–∞–Ω –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è Phase 14.0

### 4.1 Immediate Actions (—ç—Ç–æ—Ç —Å–ø—Ä–∏–Ω—Ç)

| ID | –ó–∞–¥–∞—á–∞ | –°–ª–æ–∂–Ω–æ—Å—Ç—å | –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç |
|----|--------|-----------|-----------|
| **A1** | –ò–∑–º–µ–Ω–∏—Ç—å `MediaType.TEXT` ‚Üí `MARKDOWN` –≤ `pipeline.py:1501` | 5 –º–∏–Ω | üî¥ **P0** |
| **A2** | –û–±–Ω–æ–≤–∏—Ç—å `SYSTEM_PROMPT_TEMPLATE` –≤ `audio_analyzer.py` | 30 –º–∏–Ω | üü° **P1** |
| **A3** | –û–±–Ω–æ–≤–∏—Ç—å `SYSTEM_PROMPT_TEMPLATE` –≤ `video_analyzer.py` | 30 –º–∏–Ω | üü° **P1** |
| **A4** | –°–æ–∑–¥–∞—Ç—å —Ç–µ—Å—Ç `test_media_code_detection.py` | 2 —á–∞—Å–∞ | üü° **P1** |
| **A5** | –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º –≤–∏–¥–µ–æ —Å –∫–æ–¥–æ–º | 1 —á–∞—Å | üü¢ **P2** |

**Total time:** ~4 —á–∞—Å–∞

---

### 4.2 Validation Criteria (–∫—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏)

- [ ] **V1:** –í–∏–¥–µ–æ —Å Python –∫–æ–¥–æ–º ‚Üí —Ö–æ—Ç—è –±—ã 1 CODE chunk —Å `role="ocr"`, `language="python"`
- [ ] **V2:** 5-–º–∏–Ω—É—Ç–Ω–æ–µ –∞—É–¥–∏–æ ‚Üí –º–∏–Ω–∏–º—É–º 5 —á–∞–Ω–∫–æ–≤ —Å `role="transcript"`
- [ ] **V3:** Chunk indexes –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã (0, 1, 2, 3...) –±–µ–∑ –ø—Ä–æ–ø—É—Å–∫–æ–≤
- [ ] **V4:** Semantic search –Ω–∞—Ö–æ–¥–∏—Ç —Ñ—Ä–∞–∑—É –∏–∑ **—Å–µ—Ä–µ–¥–∏–Ω—ã** 10-–º–∏–Ω—É—Ç–Ω–æ–π –ª–µ–∫—Ü–∏–∏
- [ ] **V5:** –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å–æ–¥–µ—Ä–∂–∏—Ç Markdown –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã (–Ω–µ –ø—Ä–æ—Å—Ç—ã–Ω—è —Ç–µ–∫—Å—Ç–∞)

---

### 4.3 Post-Implementation Monitoring

- [ ] **M1:** –°–æ–±—Ä–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É `code_ratio` –ø–æ 100 —Ä–µ–∞–ª—å–Ω—ã–º –≤–∏–¥–µ–æ
- [ ] **M2:** –ü—Ä–æ–≤–µ—Ä–∏—Ç—å false positives (UI text –¥–µ—Ç–µ–∫—Ç–∏—Ç—Å—è –∫–∞–∫ CODE)
- [ ] **M3:** –ï—Å–ª–∏ `code_ratio > 50%`, –¥–æ–±–∞–≤–∏—Ç—å config toggle `ocr_parser_mode` (Phase 14.3)

**–ú–µ—Ç—Ä–∏–∫–∞ code_ratio:**

```python
code_chunks = len([c for c in ocr_chunks if c.chunk_type == ChunkType.CODE])
total_ocr_chunks = len(ocr_chunks)
code_ratio = code_chunks / total_ocr_chunks

if code_ratio > 0.5:
    logger.warning(
        "High code ratio ‚Äî possible UI text false positives",
        code_ratio=f"{code_ratio:.2%}",
        path=str(media_path),
    )
```

---

## 5. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è ‚Äî Step by Step

### 5.1 Action A1: OCR Markdown Parsing

**–§–∞–π–ª:** `semantic_core/pipeline.py`  
**–°—Ç—Ä–æ–∫–∞:** 1501

**–ò–∑–º–µ–Ω–µ–Ω–∏–µ:**

```python
# BEFORE:
def _split_ocr_into_chunks(...):
    temp_doc = Document(
        content=ocr_text,
        metadata={"source": str(media_path)},
        media_type=MediaType.TEXT,  # ‚ùå Plain text
    )

# AFTER:
def _split_ocr_into_chunks(...):
    temp_doc = Document(
        content=ocr_text,
        metadata={"source": str(media_path)},
        media_type=MediaType.MARKDOWN,  # ‚úÖ Activates MarkdownNodeParser
    )
```

**–≠—Ñ—Ñ–µ–∫—Ç:**
- ‚úÖ Code blocks –∏–∑ Gemini OCR –∏–∑–æ–ª–∏—Ä—É—é—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —á–∞–Ω–∫–∏
- ‚úÖ `ChunkType.CODE` —Å `language="python"` –¥–ª—è code chunks
- ‚úÖ –ó–∞–≥–æ–ª–æ–≤–∫–∏ —Å–ª–∞–π–¥–æ–≤ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ `metadata["headers"]`

**–†–∏—Å–∫:** False positives (UI text –∫–∞–∫ Markdown syntax).  
**Mitigation:** Monitoring `code_ratio` (—Å–º. —Ä–∞–∑–¥–µ–ª 7).

---

### 5.2 Action A2: –û–±–Ω–æ–≤–∏—Ç—å audio_analyzer –ø—Ä–æ–º–ø—Ç

**–§–∞–π–ª:** `semantic_core/infrastructure/gemini/audio_analyzer.py`  
**–°—Ç—Ä–æ–∫–∞:** 37

**–ü–æ–ª–Ω—ã–π –Ω–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç:**

```python
SYSTEM_PROMPT_TEMPLATE = """You are an audio analyst creating descriptions for semantic search indexing.
Response language: {language}

Return a JSON with the following structure:

{{
  "description": "Brief 2-3 sentence summary of the audio content",
  "keywords": ["keyword1", "keyword2", ...],
  "participants": ["Speaker1", "Speaker2", ...],
  "action_items": ["Task 1", "Task 2", ...],
  "duration_seconds": <number>,
  "transcription": "MARKDOWN_FORMATTED_TRANSCRIPT_HERE"
}}

CRITICAL INSTRUCTIONS FOR TRANSCRIPTION FIELD:
- Use Markdown formatting (paragraphs, headers, lists)
- Split long monologues into logical paragraphs (every 3-5 sentences)
- Use `## Speaker Name` headers for speaker changes
- Use `**bold**` for emphasis or key terms
- Use `> quote` for direct quotations
- For technical content, wrap code snippets in triple backticks with language:
  ```python
  def example():
      pass
  ```
- DO NOT escape newlines as \\n ‚Äî use actual line breaks inside the JSON string

Example transcription format:

## Introduction

The speaker introduces the topic of semantic search and explains how embeddings work in modern NLP systems.

Key points:
- Embeddings capture semantic meaning
- Vector databases enable similarity search
- Context matters more than keywords

## Technical Deep Dive

Here's how we calculate cosine similarity:

```python
def cosine_similarity(a, b):
    return dot(a, b) / (norm(a) * norm(b))
```

This formula is fundamental to understanding vector search.

## Conclusion

The session concludes with practical examples of implementing semantic search in production systems.
"""
```

---

### 5.3 Action A3: –û–±–Ω–æ–≤–∏—Ç—å video_analyzer –ø—Ä–æ–º–ø—Ç

**–§–∞–π–ª:** `semantic_core/infrastructure/gemini/video_analyzer.py`  
**–°—Ç—Ä–æ–∫–∞:** 52

**–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å–µ–∫—Ü–∏—è –¥–ª—è OCR:**

```python
SYSTEM_PROMPT_TEMPLATE = """You are a video analyst for semantic search indexing.
Response language: {language}

Return a JSON with:

{{
  "description": "What happens in the video (3-5 sentences)",
  "keywords": ["keyword1", ...],
  "transcription": "MARKDOWN_FORMATTED_SPEECH_TRANSCRIPT",
  "ocr_text": "MARKDOWN_FORMATTED_VISUAL_TEXT",
  "duration_seconds": <number>
}}

CRITICAL INSTRUCTIONS FOR OCR_TEXT FIELD:
- Detect and preserve code blocks from screenshots/screencasts
- Wrap code in triple backticks with language:
  ```python
  class Example:
      pass
  ```
- Use `## Slide Title` headers for new slides
- Use bullet points for slide bullet lists:
  - Point 1
  - Point 2
- For UI text (buttons, labels), use plain text
- For diagrams/charts, describe structure in Markdown tables if possible

Example OCR output:

## Introduction to SOLID Principles

### Single Responsibility Principle

A class should have only one reason to change.

**Example:**

```python
class UserService:
    def validate(self, user): ...
    def save(self, user): ...
```

**Problem:** Mixes validation and persistence.

## Better Design

Split into two classes:

```python
class UserValidator:
    def validate(self, user): ...

class UserRepository:
    def save(self, user): ...
```
"""
```

---

### 5.4 Action A4: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç

**–§–∞–π–ª:** `tests/integration/test_media_code_detection.py`

```python
import pytest
from pathlib import Path
from semantic_core.domain import ChunkType
from semantic_core.infrastructure.storage.peewee.models import ChunkModel


@pytest.fixture
def sample_ocr_with_code():
    """OCR text with Python code block."""
    return """
## Function Example

```python
def calculate_total(items):
    return sum(item.price for item in items)
```

This function iterates over items and sums their prices.
"""


def test_ocr_detects_code_blocks(core, tmp_path):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ OCR —Å –∫–æ–¥–æ–º —Å–æ–∑–¥–∞—ë—Ç CODE chunks."""
    from semantic_core.domain import Document, MediaType, Chunk
    
    # –°–∏–º—É–ª–∏—Ä—É–µ–º OCR chunking –Ω–∞–ø—Ä—è–º—É—é
    ocr_text = """
## Example

```python
def hello():
    print("world")
```
"""
    
    temp_doc = Document(
        content=ocr_text,
        metadata={"source": "test.mp4"},
        media_type=MediaType.MARKDOWN,  # ‚Üê –ö–ª—é—á–µ–≤–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ
    )
    
    chunks = core.splitter.split(temp_doc)
    
    # Assertions
    code_chunks = [c for c in chunks if c.chunk_type == ChunkType.CODE]
    assert len(code_chunks) == 1, "Expected exactly 1 CODE chunk"
    assert code_chunks[0].language == "python"
    assert "def hello():" in code_chunks[0].content


def test_video_with_code_creates_ocr_code_chunks_e2e(core, tmp_path):
    """E2E —Ç–µ—Å—Ç: –≤–∏–¥–µ–æ —Å –∫–æ–¥–æ–º ‚Üí –ø—Ä–æ–≤–µ—Ä–∫–∞ –ë–î."""
    # NOTE: –¢—Ä–µ–±—É–µ—Ç —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤–∏–¥–µ–æ –∏–ª–∏ mock GeminiVideoAnalyzer
    # –î–ª—è –ø–æ–ª–Ω–æ—Ç—ã –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å mock, –∫–æ—Ç–æ—Ä—ã–π –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç OCR —Å –∫–æ–¥–æ–º
    pass  # TODO: Implement with mocked analyzer
```

---

## 6. Validation & Testing

### 6.1 Manual Testing Workflow

**Step 1:** –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –≤–∏–¥–µ–æ —Å –∫–æ–¥–æ–º

```bash
# –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª—å–Ω–æ–µ –≤–∏–¥–µ–æ (Python tutorial screencast)
semantic ingest examples/test_assets/python_tutorial.mp4
```

**Step 2:** –ü—Ä–æ–≤–µ—Ä—è–µ–º –ë–î

```python
from semantic_core.infrastructure.storage.peewee.models import ChunkModel

chunks = list(ChunkModel.select().order_by(ChunkModel.chunk_index))

for chunk in chunks:
    print(f"Index: {chunk.chunk_index}, Type: {chunk.chunk_type}, Role: {chunk.metadata.get('role')}")
    if chunk.chunk_type == "code":
        print(f"  Language: {chunk.language}")
        print(f"  Content: {chunk.content[:100]}...")
```

**–û–∂–∏–¥–∞–µ–º—ã–π output:**

```
Index: 0, Type: video_ref, Role: summary
Index: 1, Type: text, Role: transcript
Index: 2, Type: text, Role: transcript
Index: 3, Type: text, Role: ocr
Index: 4, Type: code, Role: ocr
  Language: python
  Content: def calculate_total(items):
    return sum(item.price for item in items)
Index: 5, Type: text, Role: ocr
```

---

### 6.2 Automated Testing

**Unit tests:**

```bash
pytest tests/unit/core/test_pipeline.py::test_split_ocr_into_chunks -v
```

**Integration tests:**

```bash
pytest tests/integration/test_media_code_detection.py -v
```

**E2E tests (—Å —Ä–µ–∞–ª—å–Ω—ã–º Gemini API):**

```bash
pytest tests/e2e/test_video_analysis.py::test_video_with_code_creates_code_chunks -v
```

---

## 7. –†–∏—Å–∫–∏ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

### 7.1 –†–∏—Å–∫: False Positives –≤ OCR

**–°—Ü–µ–Ω–∞—Ä–∏–π:** UI text –¥–µ—Ç–µ–∫—Ç–∏—Ç—Å—è –∫–∞–∫ Markdown code blocks.

**–ü—Ä–∏–º–µ—Ä:**

```
# OCR from mobile app screenshot:
Settings
  > Dark Mode
  > Font Size: Large
  > Language: English

# MarkdownNodeParser –º–æ–∂–µ—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å ">" –∫–∞–∫ quote block
# –∏ —Å–æ–∑–¥–∞—Ç—å –ª–∏—à–Ω–∏–π chunk
```

**Detection:**

–î–æ–±–∞–≤–ª—è–µ–º warning –≤ `_split_ocr_into_chunks()`:

```python
# –ü–æ—Å–ª–µ split_chunks:
code_chunks = [c for c in split_chunks if c.chunk_type == ChunkType.CODE]
code_ratio = len(code_chunks) / len(split_chunks) if split_chunks else 0

if code_ratio > 0.5:
    logger.warning(
        "High code ratio in OCR ‚Äî possible UI text false positives",
        code_ratio=f"{code_ratio:.2%}",
        media_path=str(media_path),
    )
```

**Mitigation (Phase 14.3):**

–î–æ–±–∞–≤–∏—Ç—å config field:

```toml
# semantic.toml
[processing.media]
ocr_parser_mode = "markdown"  # or "plain"
```

---

### 7.2 –†–∏—Å–∫: Gemini –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç Markdown –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏

**–°—Ü–µ–Ω–∞—Ä–∏–π:** –ú–æ–¥–µ–ª—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç plain text –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –ø—Ä–æ–º–ø—Ç.

**Detection:**

–ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ Markdown –≤ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è—Ö:

```python
# –í audio_analyzer.py –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞:
if "```" not in result["transcription"] and len(result["transcription"]) > 5000:
    logger.warning(
        "Long transcription without code blocks ‚Äî model might ignore Markdown instructions",
        length=len(result["transcription"]),
    )
```

**Mitigation:**

- –î–æ–±–∞–≤–∏—Ç—å few-shot examples –≤ –ø—Ä–æ–º–ø—Ç
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `gemini-2.5-flash` –≤–º–µ—Å—Ç–æ `flash-lite` (–ª—É—á—à–µ instruction following)

---

### 7.3 Success Metrics

**–ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è Phase 14.0:**

| –ú–µ—Ç—Ä–∏–∫–∞ | –¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ | –¶–µ–ª–µ–≤–æ–µ | –°—Ç–∞—Ç—É—Å |
|---------|------------------|---------|--------|
| Avg chunks per 5-min audio | 1 | 5-7 | ‚ùå |
| Code detection rate (tech videos) | 0% | >80% | ‚ùå |
| Search recall @10 (middle of audio) | 15% | >90% | ‚ùå |
| Chunk index errors | 0% | 0% | ‚úÖ |

**–¶–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ—Å–ª–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏:**

| –ú–µ—Ç—Ä–∏–∫–∞ | –¶–µ–ª–µ–≤–æ–µ |
|---------|---------|
| Avg chunks per 5-min audio | ‚úÖ 5-7 |
| Code detection rate (tech videos) | ‚úÖ >80% |
| Search recall @10 (middle of audio) | ‚úÖ >90% |

---

## 8. Roadmap ‚Äî Next Steps

–ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è Phase 14.0:

**Phase 14.1: ProcessingStep Abstraction** (3-4 –Ω–µ–¥–µ–ª–∏)
- –†–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ `_build_media_chunks()` ‚Üí step-based system
- `SummaryStep`, `TranscriptionStep`, `OCRStep`
- `register_step()` –¥–ª—è –∫–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏–∏
- `rerun_step()` –¥–ª—è –∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏

**Phase 14.2: Aggregation & Service Layer** (2 –Ω–µ–¥–µ–ª–∏)
- `MediaService.get_media_details(doc_id)` ‚Äî —Å–±–æ—Ä–∫–∞ —á–∞–Ω–∫–æ–≤
- Flask UI `/media/<id>` —Å timeline
- Search filters –ø–æ `role`

**Phase 14.3: User Flexibility** (2 –Ω–µ–¥–µ–ª–∏)
- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä—É–µ–º—ã–µ –ø—Ä–æ–º–ø—Ç—ã —á–µ—Ä–µ–∑ `semantic.toml`
- `ocr_parser_mode` config toggle
- Per-role chunk sizing

---

**End of Phase 14.0 Plan**  
**Status:** Ready for immediate implementation  
**Estimated time:** 4 hours  
**Risk level:** LOW (minimal code changes, backward compatible)
