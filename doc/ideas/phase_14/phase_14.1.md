# üèóÔ∏è Phase 14.1: Pipeline Abstraction & ProcessingStep Architecture

**–î–∞—Ç–∞:** 2025-12-06  
**–°—Ç–∞—Ç—É—Å:** Planning  
**–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:** Phase 14.0 (Smart-Splitter –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è)  
**–¶–µ–ª—å:** –†–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ –º–æ–Ω–æ–ª–∏—Ç–Ω–æ–≥–æ `pipeline.py` ‚Üí –º–æ–¥—É–ª—å–Ω–∞—è step-based —Å–∏—Å—Ç–µ–º–∞

---

## üìã –û–≥–ª–∞–≤–ª–µ–Ω–∏–µ

1. [–ú–æ—Ç–∏–≤–∞—Ü–∏—è –∏ –ø—Ä–æ–±–ª–µ–º—ã —Ç–µ–∫—É—â–µ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã](#1-–º–æ—Ç–∏–≤–∞—Ü–∏—è-–∏-–ø—Ä–æ–±–ª–µ–º—ã-—Ç–µ–∫—É—â–µ–π-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã)
2. [–¶–µ–ª–µ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ ProcessingStep](#2-—Ü–µ–ª–µ–≤–∞—è-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞-processingstep)
3. [–ü–ª–∞–Ω —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏](#3-–ø–ª–∞–Ω-—Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏)
4. [–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è Markdown-–æ—Ç–≤–µ—Ç–æ–≤](#4-–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ-–ø—Ä–æ–º–ø—Ç–æ–≤-–¥–ª—è-markdown-–æ—Ç–≤–µ—Ç–æ–≤)
5. [E2E Testing Strategy](#5-e2e-testing-strategy)
6. [–†–∏—Å–∫–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è](#6-—Ä–∏—Å–∫–∏-–∏-–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è)

---

## 1. –ú–æ—Ç–∏–≤–∞—Ü–∏—è –∏ –ø—Ä–æ–±–ª–µ–º—ã —Ç–µ–∫—É—â–µ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

### 1.1 –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ `pipeline.py`

**–ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –º–µ—Ç–æ–¥—ã:**

| –ú–µ—Ç–æ–¥ | –°—Ç—Ä–æ–∫–∏ | –ü—Ä–æ–±–ª–µ–º–∞ |
|-------|--------|----------|
| `_build_media_chunks()` | 1394-1454 | –ú–æ–Ω–æ–ª–∏—Ç–Ω–∞—è –ª–æ–≥–∏–∫–∞: summary + transcript + OCR –≤ –æ–¥–Ω–æ–º –º–µ—Ç–æ–¥–µ |
| `_split_transcription_into_chunks()` | 1456-1482 | –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ª–æ–≥–∏–∫–∏ —Å `_split_ocr_into_chunks()` |
| `_split_ocr_into_chunks()` | 1484-1518 | –ñ—ë—Å—Ç–∫–∞—è —Å–≤—è–∑–∞–Ω–Ω–æ—Å—Ç—å —Å `self.splitter` |
| `ingest_image()` / `ingest_audio()` / `ingest_video()` | 703-1029 | –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –≤—ã–∑–æ–≤–∞ `_build_media_chunks()` |

**–ß—Ç–æ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –±–µ–∑ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞:**

‚ùå –î–æ–±–∞–≤–∏—Ç—å `SentimentStep` –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è `_build_media_chunks()`  
‚ùå –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø—Ä–æ–º–ø—Ç –¥–ª—è OCR –±–µ–∑ —Ñ–æ—Ä–∫–∞ –∫–æ–¥–∞  
‚ùå –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å —Ç–æ–ª—å–∫–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –±–µ–∑ re-analyze –≤—Å–µ–≥–æ –≤–∏–¥–µ–æ  
‚ùå A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —á–∞–Ω–∫–∏–Ω–≥–∞  
‚ùå –î–æ–±–∞–≤–∏—Ç—å –∫–∞—Å—Ç–æ–º–Ω—ã–π —à–∞–≥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–∞–π–º–∫–æ–¥–æ–≤

### 1.2 –¶–µ–ª–µ–≤–∞—è –º–æ–¥–µ–ª—å —Ä–∞—Å—à–∏—Ä—è–µ–º–æ—Å—Ç–∏

**–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (–ø–æ—Å–ª–µ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞):**

```python
from semantic_core.processing.steps import SummaryStep, TranscriptionStep, OCRStep
from my_custom_steps import AdSpotDetectionStep, SentimentAnalysisStep

# –ë–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
pipeline = SemanticCore.create_with_steps([
    SummaryStep(),
    TranscriptionStep(chunk_size=1500),
    OCRStep(parser_mode="markdown"),
])

# –ö–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—è –¥–ª—è –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–∞
marketing_pipeline = SemanticCore.create_with_steps([
    SummaryStep(prompt_template="Summarize in pirate speak style"),
    TranscriptionStep(),
    AdSpotDetectionStep(),  # –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–∞–π–º–∫–æ–¥—ã —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–π
])

# –ü–æ–≤—Ç–æ—Ä–Ω—ã–π –∑–∞–ø—É—Å–∫ –æ–¥–Ω–æ–≥–æ —à–∞–≥–∞
pipeline.rerun_step("summary", document_id="abc-123")
```

---

## 2. –¶–µ–ª–µ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ ProcessingStep

### 2.1 –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å `BaseProcessingStep`

**–§–∞–π–ª:** `semantic_core/processing/steps/base.py`

```python
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import List, Optional, Any
from pathlib import Path

from semantic_core.domain import Chunk, Document


@dataclass
class MediaContext:
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –º–µ–¥–∏–∞-—Ñ–∞–π–ª–∞.
    
    Immutable –æ–±—ä–µ–∫—Ç, –ø–µ—Ä–µ–¥–∞–≤–∞–µ–º—ã–π –º–µ–∂–¥—É —à–∞–≥–∞–º–∏.
    –®–∞–≥–∏ –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ copy.
    """
    
    # –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    media_path: Path
    document: Document
    analysis: dict[str, Any]  # –†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç Gemini API
    
    # –ù–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏
    chunks: List[Chunk]
    
    # –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è
    base_index: int
    
    def with_chunks(self, new_chunks: List[Chunk], increment_index: bool = True) -> "MediaContext":
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ —á–∞–Ω–∫–∞–º–∏."""
        from copy import copy
        ctx = copy(self)
        ctx.chunks = self.chunks + new_chunks
        if increment_index:
            ctx.base_index = self.base_index + len(new_chunks)
        return ctx


class BaseProcessingStep(ABC):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è —à–∞–≥–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –º–µ–¥–∏–∞."""
    
    @property
    @abstractmethod
    def step_name(self) -> str:
        """–£–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è —à–∞–≥–∞ (–¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ re-run)."""
        pass
    
    @abstractmethod
    def process(self, context: MediaContext) -> MediaContext:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π.
        
        Args:
            context: –¢–µ–∫—É—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏.
            
        Returns:
            –ù–æ–≤—ã–π MediaContext —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ —á–∞–Ω–∫–∞–º–∏.
            
        Raises:
            ProcessingStepError: –ü—Ä–∏ –æ—à–∏–±–∫–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –ø–∞–π–ø–ª–∞–π–Ω–∞).
        """
        pass
    
    @property
    def is_optional(self) -> bool:
        """–ï—Å–ª–∏ True, –æ—à–∏–±–∫–∞ —à–∞–≥–∞ –Ω–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ø–∞–π–ø–ª–∞–π–Ω."""
        return False
    
    def should_run(self, context: MediaContext) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –∑–∞–ø—É—Å–∫–∞—Ç—å —à–∞–≥ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
        
        –ü—Ä–∏–º–µ—Ä: TranscriptionStep –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç—Å—è, –µ—Å–ª–∏ analysis["transcription"] –ø—É—Å—Ç–æ–π.
        """
        return True


class ProcessingStepError(Exception):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ —à–∞–≥–µ."""
    
    def __init__(self, step_name: str, message: str, context: Optional[MediaContext] = None):
        self.step_name = step_name
        self.context = context
        super().__init__(f"[{step_name}] {message}")
```

### 2.2 –†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —à–∞–≥–æ–≤

#### 2.2.1 SummaryStep

**–§–∞–π–ª:** `semantic_core/processing/steps/summary_step.py`

```python
from pathlib import Path
from typing import Optional

from semantic_core.domain import Chunk, ChunkType
from semantic_core.processing.steps.base import BaseProcessingStep, MediaContext
from semantic_core.utils.logger import get_logger

logger = get_logger(__name__)


class SummaryStep(BaseProcessingStep):
    """–°–æ–∑–¥–∞—ë—Ç summary chunk –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞.
    
    Attributes:
        chunk_type_map: –ú–∞–ø–ø–∏–Ω–≥ media type ‚Üí ChunkType –¥–ª—è summary.
    """
    
    CHUNK_TYPE_MAP = {
        "image": ChunkType.IMAGE_REF,
        "audio": ChunkType.AUDIO_REF,
        "video": ChunkType.VIDEO_REF,
    }
    
    def __init__(self, include_keywords: bool = True):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è.
        
        Args:
            include_keywords: –í–∫–ª—é—á–∞—Ç—å –ª–∏ keywords –≤ metadata summary —á–∞–Ω–∫–∞.
        """
        self.include_keywords = include_keywords
    
    @property
    def step_name(self) -> str:
        return "summary"
    
    def process(self, context: MediaContext) -> MediaContext:
        """–°–æ–∑–¥–∞—ë—Ç summary chunk."""
        logger.info(f"[{self.step_name}] Creating summary chunk", path=str(context.media_path))
        
        analysis = context.analysis
        media_type = analysis.get("type", "unknown")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º content (—Ç–æ–ª—å–∫–æ description, –±–µ–∑ transcript/OCR)
        summary_content = self._build_summary_content(analysis)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º metadata
        summary_metadata = self._build_summary_metadata(analysis, context.media_path)
        summary_metadata["role"] = "summary"
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º chunk_type
        chunk_type = self.CHUNK_TYPE_MAP.get(media_type, ChunkType.TEXT)
        
        # –°–æ–∑–¥–∞—ë–º chunk
        summary_chunk = Chunk(
            content=summary_content,
            chunk_index=context.base_index,
            chunk_type=chunk_type,
            metadata=summary_metadata,
        )
        
        logger.debug(
            f"[{self.step_name}] Summary created",
            chunk_type=chunk_type.value,
            content_length=len(summary_content),
        )
        
        return context.with_chunks([summary_chunk])
    
    def _build_summary_content(self, analysis: dict) -> str:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è summary chunk (–±–µ–∑ transcript/OCR)."""
        media_type = analysis.get("type", "unknown")
        
        if media_type == "image":
            return analysis.get("description", "")
        elif media_type in ("audio", "video"):
            # –¢–æ–ª—å–∫–æ description, transcript –±—É–¥–µ—Ç –≤ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —á–∞–Ω–∫–∞—Ö
            return analysis.get("description", "")
        
        return ""
    
    def _build_summary_metadata(self, analysis: dict, media_path: Path) -> dict:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç metadata –¥–ª—è summary chunk."""
        metadata = {"_original_path": str(media_path)}
        media_type = analysis.get("type", "unknown")
        
        if media_type == "image":
            metadata["_vision_alt"] = analysis.get("alt_text", "")
            if self.include_keywords:
                metadata["_vision_keywords"] = analysis.get("keywords", [])
            if analysis.get("ocr_text"):
                metadata["_vision_ocr"] = analysis["ocr_text"]
        
        elif media_type == "audio":
            metadata["_audio_description"] = analysis.get("description", "")
            if self.include_keywords:
                metadata["_audio_keywords"] = analysis.get("keywords", [])
            metadata["_audio_participants"] = analysis.get("participants", [])
            metadata["_audio_action_items"] = analysis.get("action_items", [])
            if analysis.get("duration_seconds"):
                metadata["_audio_duration"] = analysis["duration_seconds"]
        
        elif media_type == "video":
            if self.include_keywords:
                metadata["_video_keywords"] = analysis.get("keywords", [])
            if analysis.get("duration_seconds"):
                metadata["_video_duration"] = analysis["duration_seconds"]
        
        return metadata
```

#### 2.2.2 TranscriptionStep

**–§–∞–π–ª:** `semantic_core/processing/steps/transcription_step.py`

```python
from pathlib import Path
from typing import Optional

from semantic_core.domain import Chunk, Document, MediaType
from semantic_core.interfaces.splitter import BaseSplitter
from semantic_core.processing.steps.base import BaseProcessingStep, MediaContext
from semantic_core.utils.logger import get_logger

logger = get_logger(__name__)


class TranscriptionStep(BaseProcessingStep):
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –Ω–∞ —á–∞–Ω–∫–∏ —á–µ—Ä–µ–∑ SmartSplitter.
    
    Attributes:
        splitter: –≠–∫–∑–µ–º–ø–ª—è—Ä BaseSplitter –¥–ª—è —á–∞–Ω–∫–∏–Ω–≥–∞.
        chunk_size_override: –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ chunk_size (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–∑ splitter).
    """
    
    def __init__(
        self,
        splitter: BaseSplitter,
        chunk_size_override: Optional[int] = None,
    ):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è.
        
        Args:
            splitter: –°–ø–ª–∏—Ç—Ç–µ—Ä –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏.
            chunk_size_override: –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π.
        """
        self.splitter = splitter
        self.chunk_size_override = chunk_size_override
    
    @property
    def step_name(self) -> str:
        return "transcription"
    
    def should_run(self, context: MediaContext) -> bool:
        """–ó–∞–ø—É—Å–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å transcription –≤ analysis."""
        return bool(context.analysis.get("transcription"))
    
    def process(self, context: MediaContext) -> MediaContext:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –Ω–∞ —á–∞–Ω–∫–∏."""
        transcription = context.analysis["transcription"]
        
        logger.info(
            f"[{self.step_name}] Splitting transcription",
            path=str(context.media_path),
            length=len(transcription),
        )
        
        # –°–æ–∑–¥–∞—ë–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π Document
        temp_doc = Document(
            content=transcription,
            metadata={"source": str(context.media_path)},
            media_type=MediaType.TEXT,
        )
        
        # –†–µ–∂–µ–º —á–µ—Ä–µ–∑ splitter
        # TODO: –ï—Å–ª–∏ chunk_size_override –∑–∞–¥–∞–Ω, –Ω—É–∂–Ω–æ –≤—Ä–µ–º–µ–Ω–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å splitter.chunk_size
        split_chunks = self.splitter.split(temp_doc)
        
        # –û–±–æ–≥–∞—â–∞–µ–º metadata
        transcript_chunks = []
        for idx, chunk in enumerate(split_chunks):
            meta = dict(chunk.metadata or {})
            meta.setdefault("_original_path", str(context.media_path))
            meta["role"] = "transcript"
            meta["parent_media_path"] = str(context.media_path)
            
            chunk.chunk_index = context.base_index + idx
            chunk.metadata = meta
            
            transcript_chunks.append(chunk)
        
        logger.info(
            f"[{self.step_name}] Created chunks",
            count=len(transcript_chunks),
            avg_size=sum(len(c.content) for c in transcript_chunks) // len(transcript_chunks) if transcript_chunks else 0,
        )
        
        return context.with_chunks(transcript_chunks)
```

#### 2.2.3 OCRStep

**–§–∞–π–ª:** `semantic_core/processing/steps/ocr_step.py`

```python
from pathlib import Path
from typing import Literal, Optional

from semantic_core.domain import Chunk, Document, MediaType
from semantic_core.interfaces.splitter import BaseSplitter
from semantic_core.processing.steps.base import BaseProcessingStep, MediaContext
from semantic_core.utils.logger import get_logger

logger = get_logger(__name__)


class OCRStep(BaseProcessingStep):
    """–†–∞–∑–±–∏–≤–∞–µ—Ç OCR —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —á–µ—Ä–µ–∑ SmartSplitter.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Markdown-–ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è code blocks –≤ –≤–∏–¥–µ–æ —Å –∫–æ–¥–æ–º.
    
    Attributes:
        splitter: –≠–∫–∑–µ–º–ø–ª—è—Ä BaseSplitter.
        parser_mode: "markdown" (–¥–µ—Ç–µ–∫—Ç–∏—Ç code blocks) –∏–ª–∏ "plain" (–ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç).
        chunk_size_override: –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ chunk_size –¥–ª—è OCR.
    """
    
    def __init__(
        self,
        splitter: BaseSplitter,
        parser_mode: Literal["markdown", "plain"] = "markdown",
        chunk_size_override: Optional[int] = None,
    ):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è.
        
        Args:
            splitter: –°–ø–ª–∏—Ç—Ç–µ—Ä –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è OCR —Ç–µ–∫—Å—Ç–∞.
            parser_mode: –†–µ–∂–∏–º –ø–∞—Ä—Å–∏–Ω–≥–∞ ("markdown" —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è –≤–∏–¥–µ–æ —Å –∫–æ–¥–æ–º).
            chunk_size_override: –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è OCR.
        """
        self.splitter = splitter
        self.parser_mode = parser_mode
        self.chunk_size_override = chunk_size_override
    
    @property
    def step_name(self) -> str:
        return "ocr"
    
    def should_run(self, context: MediaContext) -> bool:
        """–ó–∞–ø—É—Å–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å ocr_text –≤ analysis."""
        return bool(context.analysis.get("ocr_text"))
    
    def process(self, context: MediaContext) -> MediaContext:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç OCR —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏."""
        ocr_text = context.analysis["ocr_text"]
        
        logger.info(
            f"[{self.step_name}] Splitting OCR text",
            path=str(context.media_path),
            parser_mode=self.parser_mode,
            length=len(ocr_text),
        )
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º media_type –¥–ª—è Document (–≤–ª–∏—è–µ—Ç –Ω–∞ –ø–∞—Ä—Å–∏–Ω–≥)
        media_type = MediaType.MARKDOWN if self.parser_mode == "markdown" else MediaType.TEXT
        
        # –°–æ–∑–¥–∞—ë–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π Document
        temp_doc = Document(
            content=ocr_text,
            metadata={"source": str(context.media_path)},
            media_type=media_type,
        )
        
        # –†–µ–∂–µ–º —á–µ—Ä–µ–∑ splitter
        split_chunks = self.splitter.split(temp_doc)
        
        # –û–±–æ–≥–∞—â–∞–µ–º metadata
        ocr_chunks = []
        for idx, chunk in enumerate(split_chunks):
            meta = dict(chunk.metadata or {})
            meta.setdefault("_original_path", str(context.media_path))
            meta["role"] = "ocr"
            meta["parent_media_path"] = str(context.media_path)
            
            chunk.chunk_index = context.base_index + idx
            chunk.metadata = meta
            
            ocr_chunks.append(chunk)
        
        # –°—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É code chunks (–¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π)
        code_chunks = [c for c in ocr_chunks if c.chunk_type.value == "code"]
        code_ratio = len(code_chunks) / len(ocr_chunks) if ocr_chunks else 0
        
        logger.info(
            f"[{self.step_name}] Created chunks",
            count=len(ocr_chunks),
            code_chunks=len(code_chunks),
            code_ratio=f"{code_ratio:.2%}",
        )
        
        # WARNING: –ï—Å–ª–∏ code_ratio > 50%, –≤–æ–∑–º–æ–∂–Ω—ã –ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è (UI text)
        if code_ratio > 0.5:
            logger.warning(
                f"[{self.step_name}] High code ratio detected (possibly UI text misdetected as code)",
                code_ratio=f"{code_ratio:.2%}",
                path=str(context.media_path),
            )
        
        return context.with_chunks(ocr_chunks)
```

### 2.3 Pipeline Executor –≤ SemanticCore

**–ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è:** `semantic_core/pipeline.py`

```python
from typing import List, Optional
from semantic_core.processing.steps.base import (
    BaseProcessingStep,
    MediaContext,
    ProcessingStepError,
)


class SemanticCore:
    """–î–æ–±–∞–≤–ª—è–µ–º step-based processing."""
    
    def __init__(
        self,
        # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ...
        processing_steps: Optional[List[BaseProcessingStep]] = None,
    ):
        # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ...
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º steps (–µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ)
        self._processing_steps = processing_steps or self._create_default_steps()
    
    def _create_default_steps(self) -> List[BaseProcessingStep]:
        """–°–æ–∑–¥–∞—ë—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –Ω–∞–±–æ—Ä —à–∞–≥–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏."""
        from semantic_core.processing.steps import SummaryStep, TranscriptionStep, OCRStep
        
        return [
            SummaryStep(),
            TranscriptionStep(splitter=self.splitter),
            OCRStep(splitter=self.splitter, parser_mode="markdown"),
        ]
    
    def _build_media_chunks_v2(
        self,
        document: Document,
        media_path: Path,
        chunk_type: ChunkType,
        analysis: Optional[dict],
        fallback_metadata: Optional[dict] = None,
    ) -> list[Chunk]:
        """NEW: Step-based –≤–µ—Ä—Å–∏—è _build_media_chunks().
        
        –ó–∞–º–µ–Ω–∏—Ç —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ –ø–æ—Å–ª–µ –º–∏–≥—Ä–∞—Ü–∏–∏.
        """
        if analysis is None:
            # Fallback: —Å–æ–∑–¥–∞—ë–º –æ–¥–∏–Ω —á–∞–Ω–∫
            return [
                Chunk(
                    content=str(media_path),
                    chunk_index=0,
                    chunk_type=chunk_type,
                    metadata=dict(fallback_metadata or {}),
                )
            ]
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context = MediaContext(
            media_path=media_path,
            document=document,
            analysis=analysis,
            chunks=[],
            base_index=0,
        )
        
        # –ü—Ä–æ–≥–æ–Ω—è–µ–º —á–µ—Ä–µ–∑ –≤—Å–µ —à–∞–≥–∏
        for step in self._processing_steps:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –∑–∞–ø—É—Å–∫–∞—Ç—å —à–∞–≥
                if not step.should_run(context):
                    logger.debug(
                        f"Skipping step (condition not met)",
                        step=step.step_name,
                        path=str(media_path),
                    )
                    continue
                
                # –ó–∞–ø—É—Å–∫–∞–µ–º —à–∞–≥
                context = step.process(context)
                
            except ProcessingStepError as e:
                # –ï—Å–ª–∏ —à–∞–≥ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π, –ª–æ–≥–∏—Ä—É–µ–º –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
                if step.is_optional:
                    logger.warning(
                        f"Optional step failed",
                        step=step.step_name,
                        error=str(e),
                    )
                    continue
                else:
                    # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π —à–∞–≥ ‚Äî –ø—Ä–µ—Ä—ã–≤–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω
                    logger.error(
                        f"Critical step failed",
                        step=step.step_name,
                        error=str(e),
                    )
                    raise
        
        logger.info(
            f"Media processing completed",
            path=str(media_path),
            total_chunks=len(context.chunks),
            steps_executed=len(self._processing_steps),
        )
        
        return context.chunks
    
    def register_step(self, step: BaseProcessingStep, position: Optional[int] = None) -> None:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –∫–∞—Å—Ç–æ–º–Ω—ã–π —à–∞–≥ –≤ –ø–∞–π–ø–ª–∞–π–Ω.
        
        Args:
            step: –≠–∫–∑–µ–º–ø–ª—è—Ä ProcessingStep.
            position: –ü–æ–∑–∏—Ü–∏—è –≤ —Å–ø–∏—Å–∫–µ (None = –¥–æ–±–∞–≤–∏—Ç—å –≤ –∫–æ–Ω–µ—Ü).
        """
        if position is None:
            self._processing_steps.append(step)
        else:
            self._processing_steps.insert(position, step)
        
        logger.info(
            f"Registered processing step",
            step=step.step_name,
            position=position or len(self._processing_steps) - 1,
        )
    
    def rerun_step(
        self,
        step_name: str,
        document_id: str,
        delete_old_chunks: bool = True,
    ) -> int:
        """–ü–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —à–∞–≥ –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
        
        Args:
            step_name: –ò–º—è —à–∞–≥–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "summary", "transcription").
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –ë–î.
            delete_old_chunks: –£–¥–∞–ª—è—Ç—å –ª–∏ —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏ —Å —ç—Ç–æ–π —Ä–æ–ª—å—é –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π.
        
        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤.
        
        Raises:
            ValueError: –ï—Å–ª–∏ —à–∞–≥ —Å —Ç–∞–∫–∏–º –∏–º–µ–Ω–µ–º –Ω–µ –Ω–∞–π–¥–µ–Ω.
        """
        # –ù–∞—Ö–æ–¥–∏–º —à–∞–≥
        step = next((s for s in self._processing_steps if s.step_name == step_name), None)
        if step is None:
            raise ValueError(f"Step '{step_name}' not found in pipeline")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –∏ –µ–≥–æ –∑–∞–¥–∞—á—É
        from semantic_core.infrastructure.storage.peewee.models import MediaTaskModel, ChunkModel
        
        task = MediaTaskModel.get_or_none(MediaTaskModel.result_document_id == document_id)
        if not task:
            raise ValueError(f"Document {document_id} has no associated media task")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º analysis –∏–∑ –∑–∞–¥–∞—á–∏
        import json
        analysis = {
            "type": task.media_type,
            "description": task.result_description,
            "transcription": task.result_transcription,
            "keywords": json.loads(task.result_keywords) if task.result_keywords else None,
            "ocr_text": task.result_ocr_text,
            "duration_seconds": task.result_duration_seconds,
        }
        
        # –°–æ–∑–¥–∞—ë–º –∫–æ–Ω—Ç–µ–∫—Å—Ç (–±–µ–∑ —Å—Ç–∞—Ä—ã—Ö —á–∞–Ω–∫–æ–≤)
        context = MediaContext(
            media_path=Path(task.file_path),
            document=self.store.get_document_by_id(document_id),
            analysis=analysis,
            chunks=[],
            base_index=0,  # TODO: Calculate actual index
        )
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏ —Å —ç—Ç–æ–π —Ä–æ–ª—å—é
        if delete_old_chunks:
            deleted = (
                ChunkModel.delete()
                .where(
                    (ChunkModel.document_id == document_id)
                    & (ChunkModel.metadata["role"].as_json() == step_name)
                )
                .execute()
            )
            logger.info(f"Deleted old chunks", step=step_name, count=deleted)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º —à–∞–≥
        new_context = step.process(context)
        new_chunks = new_context.chunks
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–µ —á–∞–Ω–∫–∏
        # TODO: Implement partial chunk save in PeeweeVectorStore
        
        logger.info(
            f"Re-ran step",
            step=step_name,
            document_id=document_id,
            new_chunks=len(new_chunks),
        )
        
        return len(new_chunks)
```

---

## 3. –ü–ª–∞–Ω —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

### 3.1 –≠—Ç–∞–ø—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏

**–ù–µ–¥–µ–ª—è 1: –ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞**

- [ ] –°–æ–∑–¥–∞—Ç—å `semantic_core/processing/steps/` –ø–∞–∫–µ—Ç
- [ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `BaseProcessingStep` –∏ `MediaContext`
- [ ] –î–æ–±–∞–≤–∏—Ç—å `ProcessingStepError` –≤ exceptions
- [ ] –ù–∞–ø–∏—Å–∞—Ç—å unit-—Ç–µ—Å—Ç—ã –¥–ª—è `MediaContext.with_chunks()`

**–ù–µ–¥–µ–ª—è 2: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —à–∞–≥–∏**

- [ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `SummaryStep`
- [ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `TranscriptionStep`
- [ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `OCRStep` —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π `parser_mode`
- [ ] –ù–∞–ø–∏—Å–∞—Ç—å unit-—Ç–µ—Å—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ (—Å –º–æ–∫–∞–º–∏)

**–ù–µ–¥–µ–ª—è 3: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ pipeline**

- [ ] –î–æ–±–∞–≤–∏—Ç—å `_processing_steps` –≤ `SemanticCore.__init__()`
- [ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `_build_media_chunks_v2()` —á–µ—Ä–µ–∑ step executor
- [ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `register_step()` –¥–ª—è –∫–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏–∏
- [ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `rerun_step()` –¥–ª—è –∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
- [ ] –ù–∞–ø–∏—Å–∞—Ç—å E2E —Ç–µ—Å—Ç—ã (—Å–º. —Ä–∞–∑–¥–µ–ª 5)

**–ù–µ–¥–µ–ª—è 4: –ú–∏–≥—Ä–∞—Ü–∏—è –∏ –æ—á–∏—Å—Ç–∫–∞**

- [ ] –ó–∞–º–µ–Ω–∏—Ç—å –≤—Å–µ –≤—ã–∑–æ–≤—ã `_build_media_chunks()` ‚Üí `_build_media_chunks_v2()`
- [ ] –£–¥–∞–ª–∏—Ç—å legacy –º–µ—Ç–æ–¥—ã `_split_transcription_into_chunks()` –∏ `_split_ocr_into_chunks()`
- [ ] –û–±–Ω–æ–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é
- [ ] –û–±–Ω–æ–≤–∏—Ç—å CLI –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ `rerun_step`

### 3.2 Dependency Injection Strategy

**–ü—Ä–æ–±–ª–µ–º–∞:** Steps –Ω—É–∂–¥–∞—é—Ç—Å—è –≤ `splitter`, –Ω–æ —Ç–∞–∫–∂–µ –º–æ–≥—É—Ç —Ç—Ä–µ–±–æ–≤–∞—Ç—å `image_analyzer`, `rate_limiter` –∏ —Ç.–¥.

**–†–µ—à–µ–Ω–∏–µ: Service Locator –≤ MediaContext**

```python
@dataclass
class MediaContext:
    # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–ª—è ...
    
    services: dict[str, Any] = field(default_factory=dict)
    
    def get_service(self, key: str, default: Any = None) -> Any:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–µ—Ä–≤–∏—Å –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."""
        return self.services.get(key, default)


# –í SemanticCore._build_media_chunks_v2():
context = MediaContext(
    # ... 
    services={
        "splitter": self.splitter,
        "embedder": self.embedder,
        "rate_limiter": self._rate_limiter,
    },
)

# –í TranscriptionStep.process():
splitter = context.get_service("splitter")
if splitter is None:
    raise ProcessingStepError(self.step_name, "Splitter service not available")
```

**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ (–¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã):** –ü–µ—Ä–µ–¥–∞–≤–∞—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä —à–∞–≥–æ–≤ (—Ç–µ–∫—É—â–∏–π –ø–æ–¥—Ö–æ–¥).

---

## 4. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è Markdown-–æ—Ç–≤–µ—Ç–æ–≤

### 4.1 –ü—Ä–æ–±–ª–µ–º–∞ —Å —Ç–µ–∫—É—â–∏–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏

**–¢–µ–∫—É—â–∏–µ –ø—Ä–æ–º–ø—Ç—ã** —Ç—Ä–µ–±—É—é—Ç JSON, –Ω–æ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π/OCR **–ª—É—á—à–µ –ø–æ–¥—Ö–æ–¥–∏—Ç Markdown**:

```python
# audio_analyzer.py:37
SYSTEM_PROMPT_TEMPLATE = """You are an audio analyst creating descriptions for semantic search indexing.
Response language: {language}

Return a JSON with:
- description: Brief summary
- transcription: Full verbatim text
- keywords: List of key terms
- participants: List of speakers
- action_items: List of tasks mentioned
"""
```

**–ü—Ä–æ–±–ª–µ–º—ã:**

1. **–ü–æ—Ç–µ—Ä—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:** –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ª–µ–∫—Ü–∏–π –ø—Ä–µ–≤—Ä–∞—â–∞—é—Ç—Å—è –≤ ¬´–ø—Ä–æ—Å—Ç—ã–Ω—é —Ç–µ–∫—Å—Ç–∞¬ª –±–µ–∑ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
2. **Code blocks –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è:** OCR –∏–∑ –≤–∏–¥–µ–æ —Å –∫–æ–¥–æ–º –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –∫–∞–∫ plain text, —Ç–µ—Ä—è—è —Å–∏–Ω—Ç–∞–∫—Å–∏—Å
3. **–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Å–ø–∏—Å–∫–æ–≤:** Action items –±–µ–∑ –∏–µ—Ä–∞—Ä—Ö–∏–∏

### 4.2 –¶–µ–ª–µ–≤—ã–µ –ø—Ä–æ–º–ø—Ç—ã (Markdown-first)

**–ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥:** –†–∞–∑–¥–µ–ª–∏—Ç—å –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è **–º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö** (JSON) –∏ **–∫–æ–Ω—Ç–µ–Ω—Ç–∞** (Markdown).

#### AudioAnalyzer ‚Äî Hybrid JSON+Markdown

```python
SYSTEM_PROMPT_TEMPLATE = """You are an audio analyst creating descriptions for semantic search indexing.
Response language: {language}

Return a JSON with the following structure:

{{
  "description": "Brief 2-3 sentence summary of the audio content",
  "keywords": ["keyword1", "keyword2", ...],
  "participants": ["Speaker1", "Speaker2", ...],
  "action_items": ["Task 1", "Task 2", ...],
  "duration_seconds": <number>,
  "transcription": "MARKDOWN_FORMATTED_TRANSCRIPT_HERE"
}}

CRITICAL INSTRUCTIONS FOR TRANSCRIPTION FIELD:
- Use Markdown formatting (paragraphs, headers, lists)
- Split long monologues into logical paragraphs (every 3-5 sentences)
- Use `## Speaker Name` headers for speaker changes
- Use `**bold**` for emphasis or key terms
- Use `> quote` for direct quotations
- For technical content, wrap code snippets in triple backticks with language:
  ```python
  def example():
      pass
  ```
- DO NOT escape newlines as \\n ‚Äî use actual line breaks inside the JSON string

Example transcription format:

## Introduction

The speaker introduces the topic of semantic search and explains how embeddings work in modern NLP systems.

Key points:
- Embeddings capture semantic meaning
- Vector databases enable similarity search
- Context matters more than keywords

## Technical Deep Dive

Here's how we calculate cosine similarity:

```python
def cosine_similarity(a, b):
    return dot(a, b) / (norm(a) * norm(b))
```

This formula is fundamental to understanding vector search.
"""
```

#### VideoAnalyzer ‚Äî OCR with Code Detection

```python
SYSTEM_PROMPT_TEMPLATE = """You are a video analyst creating descriptions for semantic search indexing.
Response language: {language}

Return a JSON with:

{{
  "description": "Brief summary",
  "keywords": ["keyword1", ...],
  "transcription": "MARKDOWN_FORMATTED_SPEECH_TRANSCRIPT",
  "ocr_text": "MARKDOWN_FORMATTED_VISUAL_TEXT",
  "duration_seconds": <number>
}}

CRITICAL INSTRUCTIONS FOR OCR_TEXT FIELD:
- Detect and preserve code blocks from screenshots/screencasts
- Wrap code in triple backticks with language:
  ```python
  class Example:
      pass
  ```
- Use `## Slide Title` headers for new slides
- Use bullet points for slide bullet lists:
  - Point 1
  - Point 2
- For UI text (buttons, labels), use plain text
- For diagrams/charts, describe structure in Markdown tables if possible

Example OCR output:

## Introduction to SOLID Principles

### Single Responsibility Principle

A class should have only one reason to change.

**Example:**

```python
class UserService:
    def validate(self, user): ...
    def save(self, user): ...
```

**Problem:** Mixes validation and persistence.

## Better Design

Split into two classes:

```python
class UserValidator:
    def validate(self, user): ...

class UserRepository:
    def save(self, user): ...
```
"""
```

### 4.3 –ü–∞—Ä—Å–∏–Ω–≥ Markdown-–æ—Ç–≤–µ—Ç–æ–≤

**Challenge:** Gemini –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç JSON —Å Markdown **–≤–Ω—É—Ç—Ä–∏** —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –ø–æ–ª–µ–π.

**–†–µ—à–µ–Ω–∏–µ:** –ò—Å–ø–æ–ª—å–∑—É–µ–º `json.loads()` –∫–∞–∫ –æ–±—ã—á–Ω–æ, Markdown –ø–∞—Ä—Å–∏—Ç—Å—è –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ `Document`:

```python
# –í audio_analyzer.py:
response_json = json.loads(response.text)

# transcription —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç Markdown:
# "## Speaker 1\n\nHello world...\n\n```python\ndef foo(): pass\n```"

return {
    "type": "audio",
    "description": response_json["description"],
    "transcription": response_json["transcription"],  # ‚Üê Markdown string
    # ...
}

# –í TranscriptionStep.process():
temp_doc = Document(
    content=analysis["transcription"],  # ‚Üê Markdown content
    media_type=MediaType.MARKDOWN,       # ‚Üê –í–∫–ª—é—á–∞–µ—Ç MarkdownNodeParser
)
chunks = splitter.split(temp_doc)  # ‚Üê –ü–∞—Ä—Å–∏—Ç Markdown ‚Üí –∏–∑–æ–ª–∏—Ä—É–µ—Ç code blocks
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** Code blocks –∏–∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π **–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏** –¥–µ—Ç–µ–∫—Ç—è—Ç—Å—è –∏ –∏–∑–æ–ª–∏—Ä—É—é—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —á–∞–Ω–∫–∏.

### 4.4 Migration Plan –¥–ª—è –ø—Ä–æ–º–ø—Ç–æ–≤

**Phase 14.1.5: –ü—Ä–æ–º–ø—Ç—ã**

- [ ] –û–±–Ω–æ–≤–∏—Ç—å `audio_analyzer.py:SYSTEM_PROMPT_TEMPLATE` (–¥–æ–±–∞–≤–∏—Ç—å Markdown –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏)
- [ ] –û–±–Ω–æ–≤–∏—Ç—å `video_analyzer.py:SYSTEM_PROMPT_TEMPLATE` (OCR code detection)
- [ ] –î–æ–±–∞–≤–∏—Ç—å E2E —Ç–µ—Å—Ç: –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –≤–∏–¥–µ–æ —Å –∫–æ–¥–æ–º ‚Üí –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ CODE chunks
- [ ] –î–æ–±–∞–≤–∏—Ç—å E2E —Ç–µ—Å—Ç: –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –∞—É–¥–∏–æ –ª–µ–∫—Ü–∏—é ‚Üí –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –≤ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
- [ ] –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Ñ–æ—Ä–º–∞—Ç –ø—Ä–æ–º–ø—Ç–æ–≤ –≤ `doc/architecture/`

**–†–∏—Å–∫–∏:**

‚ö†Ô∏è **Gemini –º–æ–∂–µ—Ç –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å Markdown –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏** (–º–æ–¥–µ–ª–∏ –∏–Ω–æ–≥–¥–∞ —É–ø—Ä—è–º—ã)  
**Mitigation:** –î–æ–±–∞–≤–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã –≤ –ø—Ä–æ–º–ø—Ç (few-shot learning)

‚ö†Ô∏è **–ü–∞—Ä—Å–∏–Ω–≥ JSON —Å Markdown –º–æ–∂–µ—Ç —Å–ª–æ–º–∞—Ç—å—Å—è –Ω–∞ –Ω–µ—ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–∞–≤—ã—á–∫–∞—Ö**  
**Mitigation:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `response.text.strip()` + fallback –Ω–∞ regex extraction

---

## 5. E2E Testing Strategy

### 5.1 –ó–∞—á–µ–º E2E —Ç–µ—Å—Ç—ã?

**Unit-—Ç–µ—Å—Ç—ã** (—Ç–µ–∫—É—â–∏–µ) –ø—Ä–æ–≤–µ—Ä—è—é—Ç –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—É—é –ª–æ–≥–∏–∫—É:

```python
def test_summary_step_creates_chunk():
    step = SummaryStep()
    context = MediaContext(analysis={"type": "audio", "description": "Test"}, ...)
    result = step.process(context)
    assert len(result.chunks) == 1
```

‚ùå **–ù–µ –ø—Ä–æ–≤–µ—Ä—è—é—Ç:**

- –†–µ–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ SQLite
- –ü–æ–∑–∏—Ü–∏–∏ —á–∞–Ω–∫–æ–≤ –≤ –ë–î (`chunk_index`)
- –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
- –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —à–∞–≥–æ–≤ (base_index propagation)

**E2E —Ç–µ—Å—Ç—ã** –ø—Ä–æ–≤–µ—Ä—è—é—Ç **–≤–µ—Å—å flow** –æ—Ç —Ñ–∞–π–ª–∞ –¥–æ –ë–î:

```python
def test_video_with_code_creates_ocr_code_chunks(tmp_path, real_db):
    # –°–æ–∑–¥–∞—ë–º —Ç–µ—Å—Ç–æ–≤–æ–µ –≤–∏–¥–µ–æ —Å –∫–æ–¥–æ–º –Ω–∞ —ç–∫—Ä–∞–Ω–µ
    video_path = tmp_path / "python_tutorial.mp4"
    create_test_video_with_code(video_path)  # Helper
    
    # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ SemanticCore
    core = SemanticCore(db_path=real_db)
    doc_id = core.ingest_video(str(video_path), mode="sync")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ë–î –Ω–∞–ø—Ä—è–º—É—é
    chunks = ChunkModel.select().where(ChunkModel.document_id == doc_id)
    
    # Assertions:
    assert chunks.count() >= 3  # summary + transcript + ocr
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º summary chunk
    summary = chunks.where(ChunkModel.metadata["role"].as_json() == "summary").get()
    assert summary.chunk_index == 0
    assert summary.chunk_type == "video_ref"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º OCR code chunks
    code_chunks = chunks.where(
        (ChunkModel.metadata["role"].as_json() == "ocr")
        & (ChunkModel.chunk_type == "code")
    )
    assert code_chunks.count() >= 1  # –•–æ—Ç—è –±—ã –æ–¥–∏–Ω code block –¥–µ—Ç–µ–∫—Ç–Ω—É—Ç
    assert code_chunks[0].language == "python"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º chunk_index –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
    all_indexes = [c.chunk_index for c in chunks.order_by(ChunkModel.chunk_index)]
    assert all_indexes == list(range(len(all_indexes)))  # 0, 1, 2, 3, ...
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    for chunk in chunks:
        assert chunk.embedding is not None
        assert len(chunk.embedding) == 768  # Gemini embedding dimension
```

### 5.2 E2E Test Suite

**–§–∞–π–ª:** `tests/e2e/test_media_pipeline_steps.py`

```python
import pytest
from pathlib import Path
from semantic_core import SemanticCore
from semantic_core.infrastructure.storage.peewee.models import ChunkModel, DocumentModel


@pytest.fixture
def core_with_steps(tmp_path):
    """–°–æ–∑–¥–∞—ë—Ç SemanticCore —Å —Ä–µ–∞–ª—å–Ω–æ–π –ë–î –∏ step-based pipeline."""
    db_path = tmp_path / "test.db"
    core = SemanticCore(db_path=str(db_path))
    yield core
    core.close()


class TestAudioTranscriptionStep:
    """E2E —Ç–µ—Å—Ç—ã –¥–ª—è TranscriptionStep —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –∞—É–¥–∏–æ."""
    
    def test_long_audio_creates_multiple_transcript_chunks(self, core_with_steps, sample_audio_5min):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ 5-–º–∏–Ω—É—Ç–Ω–æ–µ –∞—É–¥–∏–æ —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞–Ω–∫–æ–≤."""
        doc_id = core_with_steps.ingest_audio(str(sample_audio_5min), mode="sync")
        
        chunks = list(ChunkModel.select().where(ChunkModel.document_id == doc_id))
        
        # Summary + N transcript chunks
        assert len(chunks) >= 5, "Expected at least 5 chunks for 5-min audio"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º summary
        summary = next(c for c in chunks if c.metadata.get("role") == "summary")
        assert summary.chunk_index == 0
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º transcript chunks
        transcripts = [c for c in chunks if c.metadata.get("role") == "transcript"]
        assert len(transcripts) >= 4
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–æ–≤
        for i, chunk in enumerate(transcripts):
            assert chunk.chunk_index == i + 1  # –ü–æ—Å–ª–µ summary
    
    def test_transcript_chunks_preserve_parent_path(self, core_with_steps, sample_audio_5min):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –≤—Å–µ transcript —á–∞–Ω–∫–∏ —Å—Å—ã–ª–∞—é—Ç—Å—è –Ω–∞ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π —Ñ–∞–π–ª."""
        doc_id = core_with_steps.ingest_audio(str(sample_audio_5min), mode="sync")
        
        transcripts = list(
            ChunkModel.select().where(
                (ChunkModel.document_id == doc_id)
                & (ChunkModel.metadata["role"].as_json() == "transcript")
            )
        )
        
        for chunk in transcripts:
            assert chunk.metadata["parent_media_path"] == str(sample_audio_5min)


class TestVideoOCRStep:
    """E2E —Ç–µ—Å—Ç—ã –¥–ª—è OCRStep —Å Markdown parsing."""
    
    def test_video_with_code_screenshot_detects_code_blocks(
        self, core_with_steps, sample_video_with_code
    ):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –∫–æ–¥ –Ω–∞ —ç–∫—Ä–∞–Ω–µ –¥–µ—Ç–µ–∫—Ç–∏—Ç—Å—è –∫–∞–∫ CODE chunks."""
        doc_id = core_with_steps.ingest_video(str(sample_video_with_code), mode="sync")
        
        code_chunks = list(
            ChunkModel.select().where(
                (ChunkModel.document_id == doc_id)
                & (ChunkModel.chunk_type == "code")
                & (ChunkModel.metadata["role"].as_json() == "ocr")
            )
        )
        
        assert len(code_chunks) >= 1, "Expected at least one CODE chunk from OCR"
        assert code_chunks[0].language in ("python", "javascript", "java")
    
    def test_video_ocr_markdown_headers_preserved(
        self, core_with_steps, sample_video_slides
    ):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å–ª–∞–π–¥–æ–≤ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ metadata."""
        doc_id = core_with_steps.ingest_video(str(sample_video_slides), mode="sync")
        
        ocr_chunks = list(
            ChunkModel.select().where(
                (ChunkModel.document_id == doc_id)
                & (ChunkModel.metadata["role"].as_json() == "ocr")
            )
        )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —á–∞–Ω–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç header –≤ metadata
        headers_found = any(
            chunk.metadata.get("headers") for chunk in ocr_chunks
        )
        assert headers_found, "Expected headers in OCR chunk metadata"


class TestStepIndexPropagation:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å chunk_index –ø—Ä–∏ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ."""
    
    def test_chunk_indexes_are_sequential(self, core_with_steps, sample_video_full):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –∏–Ω–¥–µ–∫—Å—ã –∏–¥—É—Ç 0, 1, 2, 3... –±–µ–∑ –ø—Ä–æ–ø—É—Å–∫–æ–≤."""
        doc_id = core_with_steps.ingest_video(str(sample_video_full), mode="sync")
        
        chunks = list(
            ChunkModel.select()
            .where(ChunkModel.document_id == doc_id)
            .order_by(ChunkModel.chunk_index)
        )
        
        expected_indexes = list(range(len(chunks)))
        actual_indexes = [c.chunk_index for c in chunks]
        
        assert actual_indexes == expected_indexes, "Chunk indexes must be sequential"
    
    def test_summary_always_first_transcript_second_ocr_last(
        self, core_with_steps, sample_video_full
    ):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ—Ä—è–¥–æ–∫ —à–∞–≥–æ–≤: summary ‚Üí transcript ‚Üí ocr."""
        doc_id = core_with_steps.ingest_video(str(sample_video_full), mode="sync")
        
        chunks = list(
            ChunkModel.select()
            .where(ChunkModel.document_id == doc_id)
            .order_by(ChunkModel.chunk_index)
        )
        
        roles = [c.metadata.get("role") for c in chunks]
        
        # Summary –≤—Å–µ–≥–¥–∞ –ø–µ—Ä–≤—ã–π
        assert roles[0] == "summary"
        
        # Transcript chunks –∏–¥—É—Ç –ø–æ–¥—Ä—è–¥
        first_transcript_idx = roles.index("transcript")
        last_transcript_idx = len(roles) - 1 - roles[::-1].index("transcript")
        
        # OCR chunks –∏–¥—É—Ç –ø–æ—Å–ª–µ –≤—Å–µ—Ö transcript
        if "ocr" in roles:
            first_ocr_idx = roles.index("ocr")
            assert first_ocr_idx > last_transcript_idx


class TestEmbeddings:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤."""
    
    def test_all_chunks_have_embeddings(self, core_with_steps, sample_audio_5min):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –≤—Å–µ —á–∞–Ω–∫–∏ –ø–æ–ª—É—á–∏–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏."""
        doc_id = core_with_steps.ingest_audio(str(sample_audio_5min), mode="sync")
        
        chunks = ChunkModel.select().where(ChunkModel.document_id == doc_id)
        
        for chunk in chunks:
            assert chunk.embedding is not None, f"Chunk {chunk.id} missing embedding"
            assert len(chunk.embedding) == 768, "Gemini embeddings are 768-dim"
    
    def test_code_chunks_embeddings_differ_from_text(
        self, core_with_steps, sample_video_with_code
    ):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ CODE –∏ TEXT —á–∞–Ω–∫–∏ –∏–º–µ—é—Ç —Ä–∞–∑–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏."""
        import numpy as np
        
        doc_id = core_with_steps.ingest_video(str(sample_video_with_code), mode="sync")
        
        code_chunk = ChunkModel.get(
            (ChunkModel.document_id == doc_id) & (ChunkModel.chunk_type == "code")
        )
        text_chunk = ChunkModel.get(
            (ChunkModel.document_id == doc_id) & (ChunkModel.chunk_type == "text")
        )
        
        # –í—ã—á–∏—Å–ª—è–µ–º cosine similarity
        code_vec = np.array(code_chunk.embedding)
        text_vec = np.array(text_chunk.embedding)
        similarity = np.dot(code_vec, text_vec) / (
            np.linalg.norm(code_vec) * np.linalg.norm(text_vec)
        )
        
        # –î–æ–ª–∂–Ω—ã –±—ã—Ç—å –ù–ï –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–º–∏ (similarity < 0.99)
        assert similarity < 0.99, "Code and text embeddings should differ"
```

### 5.3 Test Fixtures

**–§–∞–π–ª:** `tests/fixtures/media_samples.py`

```python
import pytest
from pathlib import Path
import subprocess


@pytest.fixture(scope="session")
def sample_audio_5min(tmp_path_factory):
    """–°–æ–∑–¥–∞—ë—Ç 5-–º–∏–Ω—É—Ç–Ω–æ–µ —Ç–µ—Å—Ç–æ–≤–æ–µ –∞—É–¥–∏–æ —Å —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ä–µ—á—å—é."""
    audio_path = tmp_path_factory.mktemp("media") / "sample_5min.mp3"
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ TTS (–∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º pre-recorded —Ñ–∞–π–ª)
    # –î–ª—è CI: —Å–∫–∞—á–∏–≤–∞–µ–º —Å test assets
    download_test_asset("sample_5min.mp3", audio_path)
    
    return audio_path


@pytest.fixture(scope="session")
def sample_video_with_code(tmp_path_factory):
    """–°–æ–∑–¥–∞—ë—Ç –≤–∏–¥–µ–æ —Å –∫–æ–¥–æ–º –Ω–∞ —ç–∫—Ä–∞–Ω–µ (Python tutorial screencast)."""
    video_path = tmp_path_factory.mktemp("media") / "python_tutorial.mp4"
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º ffmpeg –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–¥–µ–æ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –∫–æ–¥–æ–º
    create_video_from_code_image(video_path)
    
    return video_path


def create_video_from_code_image(output_path: Path):
    """–°–æ–∑–¥–∞—ë—Ç 10-—Å–µ–∫—É–Ω–¥–Ω–æ–µ –≤–∏–¥–µ–æ —Å –∫–æ–¥–æ–º –Ω–∞ —ç–∫—Ä–∞–Ω–µ."""
    # 1. –°–æ–∑–¥–∞—ë–º PNG —Å –∫–æ–¥–æ–º
    code_image = output_path.parent / "code.png"
    generate_code_screenshot(
        code="""
def calculate_similarity(a, b):
    return dot(a, b) / (norm(a) * norm(b))
""",
        output=code_image,
    )
    
    # 2. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –≤–∏–¥–µ–æ
    subprocess.run([
        "ffmpeg", "-loop", "1", "-i", str(code_image),
        "-t", "10", "-pix_fmt", "yuv420p", str(output_path)
    ], check=True)
```

---

## 6. –†–∏—Å–∫–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è

### 6.1 –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ä–∏—Å–∫–∏

| –†–∏—Å–∫ | –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å | –í–ª–∏—è–Ω–∏–µ | Mitigation |
|------|-------------|---------|-----------|
| **Context mutation bugs** | –°—Ä–µ–¥–Ω—è—è | –í—ã—Å–æ–∫–æ–µ | Immutable dataclass + copy() |
| **Step dependency hell** | –ù–∏–∑–∫–∞—è | –°—Ä–µ–¥–Ω–µ–µ | Service locator pattern |
| **False code detection in OCR** | –°—Ä–µ–¥–Ω—è—è | –ù–∏–∑–∫–æ–µ | –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ code_ratio + config toggle |
| **Gemini –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç Markdown** | –°—Ä–µ–¥–Ω—è—è | –°—Ä–µ–¥–Ω–µ–µ | Few-shot examples –≤ –ø—Ä–æ–º–ø—Ç–µ |
| **Rerun step –Ω–µ –æ–±–Ω–æ–≤–ª—è–µ—Ç embeddings** | –í—ã—Å–æ–∫–∞—è | –ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ | TODO: Implement partial embedding update |

### 6.2 –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è Phase 14.1

**–ß—Ç–æ –ù–ï –≤—Ö–æ–¥–∏—Ç –≤ —ç—Ç—É —Ñ–∞–∑—É:**

‚ùå **–ö–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä—É–µ–º—ã–µ –ø—Ä–æ–º–ø—Ç—ã —á–µ—Ä–µ–∑ TOML** ‚Äî —ç—Ç–æ Phase 14.3  
‚ùå **–ü–ª–∞–≥–∏–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω—ã—Ö —à–∞–≥–æ–≤** ‚Äî —á–∞—Å—Ç–∏—á–Ω–æ (—Ç–æ–ª—å–∫–æ `register_step()`)  
‚ùå **Timeline extraction** (timestamps –¥–ª—è –≤–∏–¥–µ–æ) ‚Äî —ç—Ç–æ Phase 14.2  
‚ùå **Re-run —Å –Ω–æ–≤—ã–º –∞–Ω–∞–ª–∏–∑–æ–º** ‚Äî —Ç–µ–∫—É—â–∏–π rerun –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç–∞—Ä—ã–π analysis –∏–∑ –ë–î  
‚ùå **Partial embedding update** ‚Äî rerun –ø–µ—Ä–µ—Å–æ–∑–¥–∞—ë—Ç —á–∞–Ω–∫–∏, –Ω–æ embeddings –Ω—É–∂–Ω–æ –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ

### 6.3 –ó–∞—â–∏—Ç–∞ –æ—Ç –∏–Ω—ä–µ–∫—Ü–∏–π

**–†–µ—à–µ–Ω–∏–µ:** –ù–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ª–∏—á–Ω–æ–≥–æ —Å–æ—Ñ—Ç–∞.

**–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ:**

- SemanticCore –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω–æ, –Ω–µ—Ç –≤–Ω–µ—à–Ω–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
- Flask App (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è) —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ localhost
- –ù–µ—Ç –ø—É–±–ª–∏—á–Ω–æ–≥–æ API endpoint

**–ï—Å–ª–∏ –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –≤ –±—É–¥—É—â–µ–º (Phase 15+):**

- –í–∞–ª–∏–¥–∞—Ü–∏—è `step_name` —á–µ—Ä–µ–∑ whitelist
- Pydantic –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è `MediaContext`
- Rate limiting –¥–ª—è `rerun_step()`

---

## 7. Success Metrics

### 7.1 –ö—Ä–∏—Ç–µ—Ä–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ñ–∞–∑—ã

**Code Metrics:**

- ‚úÖ 100% –ø–æ–∫—Ä—ã—Ç–∏–µ unit-—Ç–µ—Å—Ç–∞–º–∏ –¥–ª—è `BaseProcessingStep`, `MediaContext`
- ‚úÖ E2E —Ç–µ—Å—Ç—ã –ø—Ä–æ—Ö–æ–¥—è—Ç –¥–ª—è audio (5 min), video (—Å –∫–æ–¥–æ–º), video (—Å–ª–∞–π–¥—ã)
- ‚úÖ Chunk indexes –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã (0, 1, 2...) –≤ 100% —Å–ª—É—á–∞–µ–≤
- ‚úÖ Code detection –≤ OCR —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é >80% (–Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–∏–¥–µ–æ)

**Performance:**

- ‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è 5-–º–∏–Ω—É—Ç–Ω–æ–≥–æ –∞—É–¥–∏–æ < 30 —Å–µ–∫—É–Ω–¥ (sync mode)
- ‚úÖ Memory overhead –æ—Ç step executor < 10% (vs legacy)

**Documentation:**

- ‚úÖ Architecture article –Ω–∞–ø–∏—Å–∞–Ω–∞ (`doc/architecture/74_processing_steps.md`)
- ‚úÖ –ü—Ä–∏–º–µ—Ä—ã –∫–∞—Å—Ç–æ–º–Ω—ã—Ö —à–∞–≥–æ–≤ –≤ `examples/custom_steps/`
- ‚úÖ Migration guide –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å Phase 14.0

### 7.2 Rollback Plan

**–ï—Å–ª–∏ Phase 14.1 –ø—Ä–æ–≤–∞–ª–∏—Ç—Å—è:**

1. –û—Ç–∫–∞—Ç—ã–≤–∞–µ–º –∫ Phase 14.0 (legacy `_build_media_chunks()` –æ—Å—Ç–∞—ë—Ç—Å—è —Ä–∞–±–æ—á–∏–º)
2. Step-based –º–µ—Ç–æ–¥—ã (`_build_media_chunks_v2()`) —É–¥–∞–ª—è–µ–º
3. Markdown –ø—Ä–æ–º–ø—Ç—ã –æ—Ç–∫–∞—Ç—ã–≤–∞–µ–º –∫ JSON-only

**–ö—Ä–∏—Ç–µ—Ä–∏–π –ø—Ä–æ–≤–∞–ª–∞:** E2E —Ç–µ—Å—Ç—ã –Ω–µ –ø—Ä–æ—Ö–æ–¥—è—Ç –ø–æ—Å–ª–µ 2 –Ω–µ–¥–µ–ª—å –æ—Ç–ª–∞–¥–∫–∏.

---

## 8. Next Steps

–ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è Phase 14.1:

**Phase 14.2: Aggregation & Service Layer**

- `MediaService.get_media_details(doc_id)` ‚Äî —Å–±–æ—Ä–∫–∞ —á–∞–Ω–∫–æ–≤ –≤ DTO
- Flask UI –¥–ª—è `/media/<id>` —Å timeline
- Search filters –ø–æ `role` (—Ç–æ–ª—å–∫–æ transcript, —Ç–æ–ª—å–∫–æ OCR)

**Phase 14.3: User Flexibility**

- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä—É–µ–º—ã–µ –ø—Ä–æ–º–ø—Ç—ã —á–µ—Ä–µ–∑ `semantic.toml`
- `ocr_parser_mode` config field
- Per-role chunk sizing (`transcript_chunk_size`, `ocr_chunk_size`)
- Full rerun —Å –Ω–æ–≤—ã–º –∞–Ω–∞–ª–∏–∑–æ–º (–Ω–µ –∏–∑ –∫—ç—à–∞ –ë–î)

---

**End of Phase 14.1 Plan**  
**Status:** Ready for implementation  
**Estimated Duration:** 3-4 weeks  
**Team:** 1 senior engineer + 1 QA for E2E tests
