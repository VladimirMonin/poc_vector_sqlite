# üèóÔ∏è Phase 14.1: Pipeline Abstraction & ProcessingStep Architecture

**–î–∞—Ç–∞:** 2025-12-06  
**–°—Ç–∞—Ç—É—Å:** Planning  
**–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:** Phase 14.0 (Smart-Splitter –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è)  
**–¶–µ–ª—å:** –†–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ –º–æ–Ω–æ–ª–∏—Ç–Ω–æ–≥–æ `pipeline.py` ‚Üí –º–æ–¥—É–ª—å–Ω–∞—è step-based —Å–∏—Å—Ç–µ–º–∞

**–ü–æ–¥—Ñ–∞–∑—ã:**

- **Phase 14.1.0** (Core Architecture) ‚Äî MediaContext, MediaPipeline, BaseProcessingStep
- **Phase 14.1.1** (Smart Steps) ‚Äî SummaryStep, TranscriptionStep, OCRStep
- **Phase 14.1.2** (Advanced Features) ‚Äî TimecodeParser, RetryParser, user_instructions
- **Phase 14.1.3** (Integration) ‚Äî –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ SemanticCore, Analyzer –ø—Ä–æ–º–ø—Ç–æ–≤
- **Phase 14.1.4** (Testing & Polish) ‚Äî E2E —Ç–µ—Å—Ç—ã, –º–∏–≥—Ä–∞—Ü–∏—è legacy, –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

---

## üìã –û–≥–ª–∞–≤–ª–µ–Ω–∏–µ

1. [–ú–æ—Ç–∏–≤–∞—Ü–∏—è –∏ –ø—Ä–æ–±–ª–µ–º—ã —Ç–µ–∫—É—â–µ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã](#1-–º–æ—Ç–∏–≤–∞—Ü–∏—è-–∏-–ø—Ä–æ–±–ª–µ–º—ã-—Ç–µ–∫—É—â–µ–π-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã)
2. [–¶–µ–ª–µ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ ProcessingStep](#2-—Ü–µ–ª–µ–≤–∞—è-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞-processingstep)
3. [TimecodeParser ‚Äî –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è —Ç–∞–π–º–∫–æ–¥–æ–≤](#3-timecodeparser--–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ-–∏-–≤–∞–ª–∏–¥–∞—Ü–∏—è-—Ç–∞–π–º–∫–æ–¥–æ–≤)
4. [RetryParser ‚Äî Resilient JSON Parsing](#4-retryparser--resilient-json-parsing)
5. [–ü–ª–∞–Ω —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏](#5-–ø–ª–∞–Ω-—Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏)
6. [–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è Markdown-–æ—Ç–≤–µ—Ç–æ–≤](#6-–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ-–ø—Ä–æ–º–ø—Ç–æ–≤-–¥–ª—è-markdown-–æ—Ç–≤–µ—Ç–æ–≤)
7. [E2E Testing Strategy](#7-e2e-testing-strategy)
8. [–†–∏—Å–∫–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è](#8-—Ä–∏—Å–∫–∏-–∏-–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è)

---

## 1. –ú–æ—Ç–∏–≤–∞—Ü–∏—è –∏ –ø—Ä–æ–±–ª–µ–º—ã —Ç–µ–∫—É—â–µ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

### 1.1 –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ `pipeline.py`

**–ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –º–µ—Ç–æ–¥—ã:**

| –ú–µ—Ç–æ–¥ | –°—Ç—Ä–æ–∫–∏ | –ü—Ä–æ–±–ª–µ–º–∞ |
|-------|--------|----------|
| `_build_media_chunks()` | 1394-1454 | –ú–æ–Ω–æ–ª–∏—Ç–Ω–∞—è –ª–æ–≥–∏–∫–∞: summary + transcript + OCR –≤ –æ–¥–Ω–æ–º –º–µ—Ç–æ–¥–µ |
| `_split_transcription_into_chunks()` | 1456-1482 | –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ª–æ–≥–∏–∫–∏ —Å `_split_ocr_into_chunks()` |
| `_split_ocr_into_chunks()` | 1484-1518 | –ñ—ë—Å—Ç–∫–∞—è —Å–≤—è–∑–∞–Ω–Ω–æ—Å—Ç—å —Å `self.splitter` |
| `ingest_image()` / `ingest_audio()` / `ingest_video()` | 703-1029 | –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –≤—ã–∑–æ–≤–∞ `_build_media_chunks()` |

**–ß—Ç–æ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –±–µ–∑ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞:**

‚ùå –î–æ–±–∞–≤–∏—Ç—å `SentimentStep` –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è `_build_media_chunks()`  
‚ùå –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø—Ä–æ–º–ø—Ç –¥–ª—è OCR –±–µ–∑ —Ñ–æ—Ä–∫–∞ –∫–æ–¥–∞  
‚ùå –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å —Ç–æ–ª—å–∫–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –±–µ–∑ re-analyze –≤—Å–µ–≥–æ –≤–∏–¥–µ–æ  
‚ùå A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —á–∞–Ω–∫–∏–Ω–≥–∞  
‚ùå –î–æ–±–∞–≤–∏—Ç—å –∫–∞—Å—Ç–æ–º–Ω—ã–π —à–∞–≥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–∞–π–º–∫–æ–¥–æ–≤

### 1.2 –¶–µ–ª–µ–≤–∞—è –º–æ–¥–µ–ª—å —Ä–∞—Å—à–∏—Ä—è–µ–º–æ—Å—Ç–∏

**–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (–ø–æ—Å–ª–µ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞):**

```python
from semantic_core.processing.steps import SummaryStep, TranscriptionStep, OCRStep
from my_custom_steps import AdSpotDetectionStep, SentimentAnalysisStep

# –ë–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
pipeline = SemanticCore.create_with_steps([
    SummaryStep(),
    TranscriptionStep(chunk_size=1500),
    OCRStep(parser_mode="markdown"),
])

# –ö–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—è –¥–ª—è –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–∞
marketing_pipeline = SemanticCore.create_with_steps([
    SummaryStep(prompt_template="Summarize in pirate speak style"),
    TranscriptionStep(),
    AdSpotDetectionStep(),  # –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–∞–π–º–∫–æ–¥—ã —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–π
])

# –ü–æ–≤—Ç–æ—Ä–Ω—ã–π –∑–∞–ø—É—Å–∫ –æ–¥–Ω–æ–≥–æ —à–∞–≥–∞
pipeline.rerun_step("summary", document_id="abc-123")
```

---

## 2. –¶–µ–ª–µ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ ProcessingStep

### 2.1 –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å `BaseProcessingStep`

**–§–∞–π–ª:** `semantic_core/processing/steps/base.py`

```python
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import List, Optional, Any
from pathlib import Path

from semantic_core.domain import Chunk, Document


@dataclass
class MediaContext:
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –º–µ–¥–∏–∞-—Ñ–∞–π–ª–∞.
    
    Immutable –æ–±—ä–µ–∫—Ç, –ø–µ—Ä–µ–¥–∞–≤–∞–µ–º—ã–π –º–µ–∂–¥—É —à–∞–≥–∞–º–∏.
    –®–∞–≥–∏ –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ copy.
    """
    
    # –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    media_path: Path
    document: Document
    analysis: dict[str, Any]  # –†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç Gemini API
    
    # –ù–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏
    chunks: List[Chunk]
    
    # –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è
    base_index: int
    
    def with_chunks(self, new_chunks: List[Chunk], increment_index: bool = True) -> "MediaContext":
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ —á–∞–Ω–∫–∞–º–∏."""
        from copy import copy
        ctx = copy(self)
        ctx.chunks = self.chunks + new_chunks
        if increment_index:
            ctx.base_index = self.base_index + len(new_chunks)
        return ctx


class BaseProcessingStep(ABC):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è —à–∞–≥–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –º–µ–¥–∏–∞."""
    
    @property
    @abstractmethod
    def step_name(self) -> str:
        """–£–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è —à–∞–≥–∞ (–¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ re-run)."""
        pass
    
    @abstractmethod
    def process(self, context: MediaContext) -> MediaContext:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π.
        
        Args:
            context: –¢–µ–∫—É—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏.
            
        Returns:
            –ù–æ–≤—ã–π MediaContext —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ —á–∞–Ω–∫–∞–º–∏.
            
        Raises:
            ProcessingStepError: –ü—Ä–∏ –æ—à–∏–±–∫–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –ø–∞–π–ø–ª–∞–π–Ω–∞).
        """
        pass
    
    @property
    def is_optional(self) -> bool:
        """–ï—Å–ª–∏ True, –æ—à–∏–±–∫–∞ —à–∞–≥–∞ –Ω–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ø–∞–π–ø–ª–∞–π–Ω."""
        return False
    
    def should_run(self, context: MediaContext) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –∑–∞–ø—É—Å–∫–∞—Ç—å —à–∞–≥ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
        
        –ü—Ä–∏–º–µ—Ä: TranscriptionStep –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç—Å—è, –µ—Å–ª–∏ analysis["transcription"] –ø—É—Å—Ç–æ–π.
        """
        return True


class ProcessingStepError(Exception):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ —à–∞–≥–µ."""
    
    def __init__(self, step_name: str, message: str, context: Optional[MediaContext] = None):
        self.step_name = step_name
        self.context = context
        super().__init__(f"[{step_name}] {message}")
```

### 2.2 –†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —à–∞–≥–æ–≤

#### 2.2.1 SummaryStep

**–§–∞–π–ª:** `semantic_core/processing/steps/summary_step.py`

```python
from pathlib import Path
from typing import Optional

from semantic_core.domain import Chunk, ChunkType
from semantic_core.processing.steps.base import BaseProcessingStep, MediaContext
from semantic_core.utils.logger import get_logger

logger = get_logger(__name__)


class SummaryStep(BaseProcessingStep):
    """–°–æ–∑–¥–∞—ë—Ç summary chunk –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞.
    
    Attributes:
        chunk_type_map: –ú–∞–ø–ø–∏–Ω–≥ media type ‚Üí ChunkType –¥–ª—è summary.
    """
    
    CHUNK_TYPE_MAP = {
        "image": ChunkType.IMAGE_REF,
        "audio": ChunkType.AUDIO_REF,
        "video": ChunkType.VIDEO_REF,
    }
    
    def __init__(self, include_keywords: bool = True):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è.
        
        Args:
            include_keywords: –í–∫–ª—é—á–∞—Ç—å –ª–∏ keywords –≤ metadata summary —á–∞–Ω–∫–∞.
        """
        self.include_keywords = include_keywords
    
    @property
    def step_name(self) -> str:
        return "summary"
    
    def process(self, context: MediaContext) -> MediaContext:
        """–°–æ–∑–¥–∞—ë—Ç summary chunk."""
        logger.info(f"[{self.step_name}] Creating summary chunk", path=str(context.media_path))
        
        analysis = context.analysis
        media_type = analysis.get("type", "unknown")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º content (—Ç–æ–ª—å–∫–æ description, –±–µ–∑ transcript/OCR)
        summary_content = self._build_summary_content(analysis)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º metadata
        summary_metadata = self._build_summary_metadata(analysis, context.media_path)
        summary_metadata["role"] = "summary"
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º chunk_type
        chunk_type = self.CHUNK_TYPE_MAP.get(media_type, ChunkType.TEXT)
        
        # –°–æ–∑–¥–∞—ë–º chunk
        summary_chunk = Chunk(
            content=summary_content,
            chunk_index=context.base_index,
            chunk_type=chunk_type,
            metadata=summary_metadata,
        )
        
        logger.debug(
            f"[{self.step_name}] Summary created",
            chunk_type=chunk_type.value,
            content_length=len(summary_content),
        )
        
        return context.with_chunks([summary_chunk])
    
    def _build_summary_content(self, analysis: dict) -> str:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è summary chunk (–±–µ–∑ transcript/OCR)."""
        media_type = analysis.get("type", "unknown")
        
        if media_type == "image":
            return analysis.get("description", "")
        elif media_type in ("audio", "video"):
            # –¢–æ–ª—å–∫–æ description, transcript –±—É–¥–µ—Ç –≤ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —á–∞–Ω–∫–∞—Ö
            return analysis.get("description", "")
        
        return ""
    
    def _build_summary_metadata(self, analysis: dict, media_path: Path) -> dict:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç metadata –¥–ª—è summary chunk."""
        metadata = {"_original_path": str(media_path)}
        media_type = analysis.get("type", "unknown")
        
        if media_type == "image":
            metadata["_vision_alt"] = analysis.get("alt_text", "")
            if self.include_keywords:
                metadata["_vision_keywords"] = analysis.get("keywords", [])
            if analysis.get("ocr_text"):
                metadata["_vision_ocr"] = analysis["ocr_text"]
        
        elif media_type == "audio":
            metadata["_audio_description"] = analysis.get("description", "")
            if self.include_keywords:
                metadata["_audio_keywords"] = analysis.get("keywords", [])
            metadata["_audio_participants"] = analysis.get("participants", [])
            metadata["_audio_action_items"] = analysis.get("action_items", [])
            if analysis.get("duration_seconds"):
                metadata["_audio_duration"] = analysis["duration_seconds"]
        
        elif media_type == "video":
            if self.include_keywords:
                metadata["_video_keywords"] = analysis.get("keywords", [])
            if analysis.get("duration_seconds"):
                metadata["_video_duration"] = analysis["duration_seconds"]
        
        return metadata
```

#### 2.2.2 TranscriptionStep

**–§–∞–π–ª:** `semantic_core/processing/steps/transcription_step.py`

```python
from pathlib import Path
from typing import Optional

from semantic_core.domain import Chunk, Document, MediaType
from semantic_core.interfaces.splitter import BaseSplitter
from semantic_core.processing.steps.base import BaseProcessingStep, MediaContext
from semantic_core.utils.logger import get_logger

logger = get_logger(__name__)


class TranscriptionStep(BaseProcessingStep):
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –Ω–∞ —á–∞–Ω–∫–∏ —á–µ—Ä–µ–∑ SmartSplitter.
    
    Attributes:
        splitter: –≠–∫–∑–µ–º–ø–ª—è—Ä BaseSplitter –¥–ª—è —á–∞–Ω–∫–∏–Ω–≥–∞.
        chunk_size_override: –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ chunk_size (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–∑ splitter).
    """
    
    def __init__(
        self,
        splitter: BaseSplitter,
        chunk_size_override: Optional[int] = None,
    ):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è.
        
        Args:
            splitter: –°–ø–ª–∏—Ç—Ç–µ—Ä –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏.
            chunk_size_override: –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π.
        """
        self.splitter = splitter
        self.chunk_size_override = chunk_size_override
    
    @property
    def step_name(self) -> str:
        return "transcription"
    
    def should_run(self, context: MediaContext) -> bool:
        """–ó–∞–ø—É—Å–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å transcription –≤ analysis."""
        return bool(context.analysis.get("transcription"))
    
    def process(self, context: MediaContext) -> MediaContext:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –Ω–∞ —á–∞–Ω–∫–∏."""
        transcription = context.analysis["transcription"]
        
        logger.info(
            f"[{self.step_name}] Splitting transcription",
            path=str(context.media_path),
            length=len(transcription),
        )
        
        # –°–æ–∑–¥–∞—ë–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π Document
        temp_doc = Document(
            content=transcription,
            metadata={"source": str(context.media_path)},
            media_type=MediaType.TEXT,
        )
        
        # –†–µ–∂–µ–º —á–µ—Ä–µ–∑ splitter
        # TODO: –ï—Å–ª–∏ chunk_size_override –∑–∞–¥–∞–Ω, –Ω—É–∂–Ω–æ –≤—Ä–µ–º–µ–Ω–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å splitter.chunk_size
        split_chunks = self.splitter.split(temp_doc)
        
        # –û–±–æ–≥–∞—â–∞–µ–º metadata
        transcript_chunks = []
        for idx, chunk in enumerate(split_chunks):
            meta = dict(chunk.metadata or {})
            meta.setdefault("_original_path", str(context.media_path))
            meta["role"] = "transcript"
            meta["parent_media_path"] = str(context.media_path)
            
            chunk.chunk_index = context.base_index + idx
            chunk.metadata = meta
            
            transcript_chunks.append(chunk)
        
        logger.info(
            f"[{self.step_name}] Created chunks",
            count=len(transcript_chunks),
            avg_size=sum(len(c.content) for c in transcript_chunks) // len(transcript_chunks) if transcript_chunks else 0,
        )
        
        return context.with_chunks(transcript_chunks)
```

#### 2.2.3 OCRStep

**–§–∞–π–ª:** `semantic_core/processing/steps/ocr_step.py`

```python
from pathlib import Path
from typing import Literal, Optional

from semantic_core.domain import Chunk, Document, MediaType
from semantic_core.interfaces.splitter import BaseSplitter
from semantic_core.processing.steps.base import BaseProcessingStep, MediaContext
from semantic_core.utils.logger import get_logger

logger = get_logger(__name__)


class OCRStep(BaseProcessingStep):
    """–†–∞–∑–±–∏–≤–∞–µ—Ç OCR —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —á–µ—Ä–µ–∑ SmartSplitter.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Markdown-–ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è code blocks –≤ –≤–∏–¥–µ–æ —Å –∫–æ–¥–æ–º.
    
    Attributes:
        splitter: –≠–∫–∑–µ–º–ø–ª—è—Ä BaseSplitter.
        parser_mode: "markdown" (–¥–µ—Ç–µ–∫—Ç–∏—Ç code blocks) –∏–ª–∏ "plain" (–ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç).
        chunk_size_override: –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ chunk_size –¥–ª—è OCR.
    """
    
    def __init__(
        self,
        splitter: BaseSplitter,
        parser_mode: Literal["markdown", "plain"] = "markdown",
        chunk_size_override: Optional[int] = None,
    ):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è.
        
        Args:
            splitter: –°–ø–ª–∏—Ç—Ç–µ—Ä –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è OCR —Ç–µ–∫—Å—Ç–∞.
            parser_mode: –†–µ–∂–∏–º –ø–∞—Ä—Å–∏–Ω–≥–∞ ("markdown" —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è –≤–∏–¥–µ–æ —Å –∫–æ–¥–æ–º).
            chunk_size_override: –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è OCR.
        """
        self.splitter = splitter
        self.parser_mode = parser_mode
        self.chunk_size_override = chunk_size_override
    
    @property
    def step_name(self) -> str:
        return "ocr"
    
    def should_run(self, context: MediaContext) -> bool:
        """–ó–∞–ø—É—Å–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å ocr_text –≤ analysis."""
        return bool(context.analysis.get("ocr_text"))
    
    def process(self, context: MediaContext) -> MediaContext:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç OCR —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏."""
        ocr_text = context.analysis["ocr_text"]
        
        logger.info(
            f"[{self.step_name}] Splitting OCR text",
            path=str(context.media_path),
            parser_mode=self.parser_mode,
            length=len(ocr_text),
        )
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º media_type –¥–ª—è Document (–≤–ª–∏—è–µ—Ç –Ω–∞ –ø–∞—Ä—Å–∏–Ω–≥)
        media_type = MediaType.MARKDOWN if self.parser_mode == "markdown" else MediaType.TEXT
        
        # –°–æ–∑–¥–∞—ë–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π Document
        temp_doc = Document(
            content=ocr_text,
            metadata={"source": str(context.media_path)},
            media_type=media_type,
        )
        
        # –†–µ–∂–µ–º —á–µ—Ä–µ–∑ splitter
        split_chunks = self.splitter.split(temp_doc)
        
        # –û–±–æ–≥–∞—â–∞–µ–º metadata
        ocr_chunks = []
        for idx, chunk in enumerate(split_chunks):
            meta = dict(chunk.metadata or {})
            meta.setdefault("_original_path", str(context.media_path))
            meta["role"] = "ocr"
            meta["parent_media_path"] = str(context.media_path)
            
            chunk.chunk_index = context.base_index + idx
            chunk.metadata = meta
            
            ocr_chunks.append(chunk)
        
        # –°—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É code chunks (–¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π)
        code_chunks = [c for c in ocr_chunks if c.chunk_type.value == "code"]
        code_ratio = len(code_chunks) / len(ocr_chunks) if ocr_chunks else 0
        
        logger.info(
            f"[{self.step_name}] Created chunks",
            count=len(ocr_chunks),
            code_chunks=len(code_chunks),
            code_ratio=f"{code_ratio:.2%}",
        )
        
        # WARNING: –ï—Å–ª–∏ code_ratio > 50%, –≤–æ–∑–º–æ–∂–Ω—ã –ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è (UI text)
        if code_ratio > 0.5:
            logger.warning(
                f"[{self.step_name}] High code ratio detected (possibly UI text misdetected as code)",
                code_ratio=f"{code_ratio:.2%}",
                path=str(context.media_path),
            )
        
        return context.with_chunks(ocr_chunks)
```

### 2.3 Pipeline Executor –≤ SemanticCore

**–ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è:** `semantic_core/pipeline.py`

```python
from typing import List, Optional
from semantic_core.processing.steps.base import (
    BaseProcessingStep,
    MediaContext,
    ProcessingStepError,
)


class SemanticCore:
    """–î–æ–±–∞–≤–ª—è–µ–º step-based processing."""
    
    def __init__(
        self,
        # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ...
        processing_steps: Optional[List[BaseProcessingStep]] = None,
    ):
        # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ...
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º steps (–µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ)
        self._processing_steps = processing_steps or self._create_default_steps()
    
    def _create_default_steps(self) -> List[BaseProcessingStep]:
        """–°–æ–∑–¥–∞—ë—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –Ω–∞–±–æ—Ä —à–∞–≥–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏."""
        from semantic_core.processing.steps import SummaryStep, TranscriptionStep, OCRStep
        
        return [
            SummaryStep(),
            TranscriptionStep(splitter=self.splitter),
            OCRStep(splitter=self.splitter, parser_mode="markdown"),
        ]
    
    def _build_media_chunks_v2(
        self,
        document: Document,
        media_path: Path,
        chunk_type: ChunkType,
        analysis: Optional[dict],
        fallback_metadata: Optional[dict] = None,
    ) -> list[Chunk]:
        """NEW: Step-based –≤–µ—Ä—Å–∏—è _build_media_chunks().
        
        –ó–∞–º–µ–Ω–∏—Ç —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ –ø–æ—Å–ª–µ –º–∏–≥—Ä–∞—Ü–∏–∏.
        """
        if analysis is None:
            # Fallback: —Å–æ–∑–¥–∞—ë–º –æ–¥–∏–Ω —á–∞–Ω–∫
            return [
                Chunk(
                    content=str(media_path),
                    chunk_index=0,
                    chunk_type=chunk_type,
                    metadata=dict(fallback_metadata or {}),
                )
            ]
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context = MediaContext(
            media_path=media_path,
            document=document,
            analysis=analysis,
            chunks=[],
            base_index=0,
        )
        
        # –ü—Ä–æ–≥–æ–Ω—è–µ–º —á–µ—Ä–µ–∑ –≤—Å–µ —à–∞–≥–∏
        for step in self._processing_steps:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –∑–∞–ø—É—Å–∫–∞—Ç—å —à–∞–≥
                if not step.should_run(context):
                    logger.debug(
                        f"Skipping step (condition not met)",
                        step=step.step_name,
                        path=str(media_path),
                    )
                    continue
                
                # –ó–∞–ø—É—Å–∫–∞–µ–º —à–∞–≥
                context = step.process(context)
                
            except ProcessingStepError as e:
                # –ï—Å–ª–∏ —à–∞–≥ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π, –ª–æ–≥–∏—Ä—É–µ–º –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
                if step.is_optional:
                    logger.warning(
                        f"Optional step failed",
                        step=step.step_name,
                        error=str(e),
                    )
                    continue
                else:
                    # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π —à–∞–≥ ‚Äî –ø—Ä–µ—Ä—ã–≤–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω
                    logger.error(
                        f"Critical step failed",
                        step=step.step_name,
                        error=str(e),
                    )
                    raise
        
        logger.info(
            f"Media processing completed",
            path=str(media_path),
            total_chunks=len(context.chunks),
            steps_executed=len(self._processing_steps),
        )
        
        return context.chunks
    
    def register_step(self, step: BaseProcessingStep, position: Optional[int] = None) -> None:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –∫–∞—Å—Ç–æ–º–Ω—ã–π —à–∞–≥ –≤ –ø–∞–π–ø–ª–∞–π–Ω.
        
        Args:
            step: –≠–∫–∑–µ–º–ø–ª—è—Ä ProcessingStep.
            position: –ü–æ–∑–∏—Ü–∏—è –≤ —Å–ø–∏—Å–∫–µ (None = –¥–æ–±–∞–≤–∏—Ç—å –≤ –∫–æ–Ω–µ—Ü).
        """
        if position is None:
            self._processing_steps.append(step)
        else:
            self._processing_steps.insert(position, step)
        
        logger.info(
            f"Registered processing step",
            step=step.step_name,
            position=position or len(self._processing_steps) - 1,
        )
    
    def rerun_step(
        self,
        step_name: str,
        document_id: str,
        delete_old_chunks: bool = True,
    ) -> int:
        """–ü–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —à–∞–≥ –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
        
        Args:
            step_name: –ò–º—è —à–∞–≥–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "summary", "transcription").
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –ë–î.
            delete_old_chunks: –£–¥–∞–ª—è—Ç—å –ª–∏ —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏ —Å —ç—Ç–æ–π —Ä–æ–ª—å—é –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π.
        
        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤.
        
        Raises:
            ValueError: –ï—Å–ª–∏ —à–∞–≥ —Å —Ç–∞–∫–∏–º –∏–º–µ–Ω–µ–º –Ω–µ –Ω–∞–π–¥–µ–Ω.
        """
        # –ù–∞—Ö–æ–¥–∏–º —à–∞–≥
        step = next((s for s in self._processing_steps if s.step_name == step_name), None)
        if step is None:
            raise ValueError(f"Step '{step_name}' not found in pipeline")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –∏ –µ–≥–æ –∑–∞–¥–∞—á—É
        from semantic_core.infrastructure.storage.peewee.models import MediaTaskModel, ChunkModel
        
        task = MediaTaskModel.get_or_none(MediaTaskModel.result_document_id == document_id)
        if not task:
            raise ValueError(f"Document {document_id} has no associated media task")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º analysis –∏–∑ –∑–∞–¥–∞—á–∏
        import json
        analysis = {
            "type": task.media_type,
            "description": task.result_description,
            "transcription": task.result_transcription,
            "keywords": json.loads(task.result_keywords) if task.result_keywords else None,
            "ocr_text": task.result_ocr_text,
            "duration_seconds": task.result_duration_seconds,
        }
        
        # –°–æ–∑–¥–∞—ë–º –∫–æ–Ω—Ç–µ–∫—Å—Ç (–±–µ–∑ —Å—Ç–∞—Ä—ã—Ö —á–∞–Ω–∫–æ–≤)
        context = MediaContext(
            media_path=Path(task.file_path),
            document=self.store.get_document_by_id(document_id),
            analysis=analysis,
            chunks=[],
            base_index=0,  # TODO: Calculate actual index
        )
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏ —Å —ç—Ç–æ–π —Ä–æ–ª—å—é
        if delete_old_chunks:
            deleted = (
                ChunkModel.delete()
                .where(
                    (ChunkModel.document_id == document_id)
                    & (ChunkModel.metadata["role"].as_json() == step_name)
                )
                .execute()
            )
            logger.info(f"Deleted old chunks", step=step_name, count=deleted)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º —à–∞–≥
        new_context = step.process(context)
        new_chunks = new_context.chunks
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–µ —á–∞–Ω–∫–∏
        # TODO: Implement partial chunk save in PeeweeVectorStore
        
        logger.info(
            f"Re-ran step",
            step=step_name,
            document_id=document_id,
            new_chunks=len(new_chunks),
        )
        
        return len(new_chunks)
```

---

## 5. –ü–ª–∞–Ω —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

### 5.1 –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–æ–¥—Ñ–∞–∑

**Phase 14.1.0: Core Architecture (Week 1)**

–¶–µ–ª—å: –§—É–Ω–¥–∞–º–µ–Ω—Ç –¥–ª—è step-based pipeline.

- [ ] –°–æ–∑–¥–∞—Ç—å `semantic_core/core/media_context.py` —Å `MediaPipelineContext` (frozen dataclass)
- [ ] –°–æ–∑–¥–∞—Ç—å `semantic_core/core/media_pipeline.py` —Å `MediaPipeline` executor
- [ ] –°–æ–∑–¥–∞—Ç—å `semantic_core/processing/steps/base.py` —Å `BaseProcessingStep`
- [ ] –î–æ–±–∞–≤–∏—Ç—å `ProcessingStepError` –≤ exceptions
- [ ] Unit-—Ç–µ—Å—Ç—ã: `MediaPipelineContext.with_chunks()`, `MediaPipeline.build_chunks()`

**Phase 14.1.1: Smart Steps (Week 2)**

–¶–µ–ª—å: –ú–∏–≥—Ä–∞—Ü–∏—è –ª–æ–≥–∏–∫–∏ –∏–∑ `_build_media_chunks()` –≤ —à–∞–≥–∏.

- [ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `SummaryStep` (–∏–∑–≤–ª–µ—á—å –ª–æ–≥–∏–∫—É –∏–∑ `_build_content_from_analysis()`)
- [ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `TranscriptionStep` (Constructor Injection: `splitter`)
- [ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `OCRStep` —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π `parser_mode="markdown"`
- [ ] Unit-—Ç–µ—Å—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ (—Å –º–æ–∫–∞–º–∏ analyzers/splitters)

**Phase 14.1.2: Advanced Features (Week 2-3)**

–¶–µ–ª—å: –¢–∞–π–º–∫–æ–¥—ã, user prompts, resilient parsing.

- [ ] –°–æ–∑–¥–∞—Ç—å `semantic_core/utils/timecode_parser.py` —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –ø–æ `max_duration_seconds`
- [ ] –î–æ–±–∞–≤–∏—Ç—å `user_instructions` –ø–æ–ª–µ –≤ `MediaPipelineContext`
- [ ] –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å `TimecodeParser` –≤ `TranscriptionStep`
- [ ] **–û–ü–¶–ò–û–ù–ê–õ–¨–ù–û:** –°–æ–∑–¥–∞—Ç—å `RetryParser` –¥–ª—è legacy analyzers (–µ—Å–ª–∏ –Ω–µ –º–∏–≥—Ä–∏—Ä—É–µ–º –Ω–∞ `response_schema`)
- [ ] Unit-—Ç–µ—Å—Ç—ã: `TimecodeParser.parse()`, `inherit_timecode()`, –≤–∞–ª–∏–¥–∞—Ü–∏—è

**Phase 14.1.3: Integration & Analyzer Migration (Week 3)**

–¶–µ–ª—å: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ SemanticCore + –º–∏–≥—Ä–∞—Ü–∏—è –Ω–∞ `response_schema`.

- [ ] –î–æ–±–∞–≤–∏—Ç—å `MediaPipeline` –≤ `SemanticCore.__init__()` (Constructor Injection)
- [ ] –û–±–Ω–æ–≤–∏—Ç—å `ingest_audio/video/image()` —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º `user_prompt`
- [ ] **CRITICAL:** –ú–∏–≥—Ä–∏—Ä–æ–≤–∞—Ç—å analyzers –Ω–∞ Pydantic `response_schema`:
  - [ ] `audio_analyzer.py` ‚Üí `AudioAnalysisResult` Pydantic model
  - [ ] `video_analyzer.py` ‚Üí `VideoAnalysisResult` Pydantic model
  - [ ] `image_analyzer.py` ‚Üí `ImageAnalysisResult` Pydantic model
- [ ] –û–±–Ω–æ–≤–∏—Ç—å –ø—Ä–æ–º–ø—Ç—ã —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –¥–ª—è —Ç–∞–π–º–∫–æ–¥–æ–≤ `[MM:SS]`
- [ ] –û–±–Ω–æ–≤–∏—Ç—å –ø—Ä–æ–º–ø—Ç—ã —Å —Å–µ–∫—Ü–∏–µ–π **User Context** –¥–ª—è `user_instructions`
- [ ] –£–¥–∞–ª–∏—Ç—å `json.loads()` –∏–∑ analyzers (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `response.parsed`)

**Phase 14.1.4: Testing & Polish (Week 4)**

–¶–µ–ª—å: E2E —Ç–µ—Å—Ç—ã, –º–∏–≥—Ä–∞—Ü–∏—è legacy, –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è.

- [ ] E2E —Ç–µ—Å—Ç: `test_audio_with_timecodes()` ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å `metadata['start_seconds']`
- [ ] E2E —Ç–µ—Å—Ç: `test_timecode_inheritance()` ‚Äî —á–∞–Ω–∫ –±–µ–∑ —Ç–∞–π–º–∫–æ–¥–∞ –Ω–∞—Å–ª–µ–¥—É–µ—Ç –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ
- [ ] E2E —Ç–µ—Å—Ç: `test_user_prompt_injection()` ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å `metadata['_user_context']`
- [ ] E2E —Ç–µ—Å—Ç: `test_video_code_detection()` ‚Äî –∏–∑ Phase 14.0 (—É–∂–µ 7/7 passing)
- [ ] –ó–∞–º–µ–Ω–∏—Ç—å –≤—Å–µ –≤—ã–∑–æ–≤—ã `_build_media_chunks()` ‚Üí `MediaPipeline.build_chunks()`
- [ ] –£–¥–∞–ª–∏—Ç—å legacy: `_split_transcription_into_chunks()`, `_split_ocr_into_chunks()`
- [ ] –ù–∞–ø–∏—Å–∞—Ç—å —Å—Ç–∞—Ç—å—é 75: "MediaPipeline Architecture Overview"
- [ ] –û–±–Ω–æ–≤–∏—Ç—å CLI: –¥–æ–±–∞–≤–∏—Ç—å `--user-prompt` flag –¥–ª—è `semantic ingest`

### 5.2 –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ—à–µ–Ω–∏—è

**–ü—Ä–∏–Ω—è—Ç–æ:**

1. ‚úÖ Constructor Injection –≤ steps (–ù–ï Service Locator)
2. ‚úÖ `MediaPipelineContext` frozen dataclass (immutability —á–µ—Ä–µ–∑ `replace()`)
3. ‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è –Ω–∞ `response_schema` –≤–º–µ—Å—Ç–æ RetryParser (Gemini API –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å)
4. ‚úÖ `TimecodeParser` —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π `max_duration_seconds`
5. ‚úÖ `user_instructions` –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ –≤ `MediaPipelineContext` (Optional[str])

**–û—Ç–ª–æ–∂–µ–Ω–æ –Ω–∞ Phase 14.2+:**

- ‚è∏ Fallback modes –¥–ª—è Gemini failures
- ‚è∏ Batch embedding –¥–ª—è `len(chunks) > 10`
- ‚è∏ Timeline UI –¥–ª—è Flask app
- ‚è∏ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä—É–µ–º—ã–µ –ø—Ä–æ–º–ø—Ç—ã —á–µ—Ä–µ–∑ TOML

### 3.2 Dependency Injection Strategy

**–ü—Ä–æ–±–ª–µ–º–∞:** Steps –Ω—É–∂–¥–∞—é—Ç—Å—è –≤ `splitter`, –Ω–æ —Ç–∞–∫–∂–µ –º–æ–≥—É—Ç —Ç—Ä–µ–±–æ–≤–∞—Ç—å `image_analyzer`, `rate_limiter` –∏ —Ç.–¥.

**–†–µ—à–µ–Ω–∏–µ: Service Locator –≤ MediaContext**

```python
@dataclass
class MediaContext:
    # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–ª—è ...
    
    services: dict[str, Any] = field(default_factory=dict)
    
    def get_service(self, key: str, default: Any = None) -> Any:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–µ—Ä–≤–∏—Å –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."""
        return self.services.get(key, default)


# –í SemanticCore._build_media_chunks_v2():
context = MediaContext(
    # ... 
    services={
        "splitter": self.splitter,
        "embedder": self.embedder,
        "rate_limiter": self._rate_limiter,
    },
)

# –í TranscriptionStep.process():
splitter = context.get_service("splitter")
if splitter is None:
    raise ProcessingStepError(self.step_name, "Splitter service not available")
```

**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ (–¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã):** –ü–µ—Ä–µ–¥–∞–≤–∞—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä —à–∞–≥–æ–≤ (—Ç–µ–∫—É—â–∏–π –ø–æ–¥—Ö–æ–¥).

---

## 3. TimecodeParser ‚Äî –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è —Ç–∞–π–º–∫–æ–¥–æ–≤

### 3.1 –ú–æ—Ç–∏–≤–∞—Ü–∏—è

**–ü—Ä–æ–±–ª–µ–º–∞:** –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ —Ö–æ—Ç—è—Ç –Ω–∞–≤–∏–≥–∞—Ü–∏—é –ø–æ –º–µ–¥–∏–∞ –∫–∞–∫ –≤ YouTube ‚Äî –∫–ª–∏–∫ –Ω–∞ —á–∞–Ω–∫ ‚Üí –ø–ª–µ–µ—Ä –ø–µ—Ä–µ–º–∞—Ç—ã–≤–∞–µ—Ç –Ω–∞ –Ω—É–∂–Ω–æ–µ –º–µ—Å—Ç–æ.

**–†–µ—à–µ–Ω–∏–µ:** –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–∞–π–º–∫–æ–¥—ã `[MM:SS]` –∏–∑ –æ—Ç–≤–µ—Ç–æ–≤ Gemini –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ `metadata['start_seconds']`.

### 3.2 –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

**–§–∞–π–ª:** `semantic_core/utils/timecode_parser.py`

```python
import re
from typing import Optional, Tuple
from dataclasses import dataclass
from semantic_core.utils.logger import get_logger

logger = get_logger(__name__)


@dataclass
class TimecodeInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–∞–π–º–∫–æ–¥–µ."""
    original: str  # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ "[02:15]"
    seconds: int   # –°–µ–∫—É–Ω–¥—ã –æ—Ç –Ω–∞—á–∞–ª–∞ (135)
    minutes: int   # –ú–∏–Ω—É—Ç—ã (2)
    secs: int      # –°–µ–∫—É–Ω–¥—ã –≤ –º–∏–Ω—É—Ç–µ (15)


class TimecodeParser:
    """–ü–∞—Ä—Å–µ—Ä —Ç–∞–π–º–∫–æ–¥–æ–≤ –∏–∑ Markdown-—Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ñ–æ—Ä–º–∞—Ç—ã:
    - [MM:SS] ‚Äî –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è Gemini)
    - [HH:MM:SS] ‚Äî —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç (–¥–ª—è –≤–∏–¥–µ–æ >1 —á–∞—Å)
    
    –í–∞–ª–∏–¥–∞—Ü–∏—è:
    - –¢–∞–π–º–∫–æ–¥ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –±–æ–ª—å—à–µ —Ä–µ–∞–ª—å–Ω–æ–π –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ñ–∞–π–ª–∞
    - –¢–∞–π–º–∫–æ–¥—ã –¥–æ–ª–∂–Ω—ã –∏–¥—Ç–∏ –≤ –≤–æ–∑—Ä–∞—Å—Ç–∞—é—â–µ–º –ø–æ—Ä—è–¥–∫–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    """
    
    # Regex patterns
    TIMECODE_PATTERN_MMSS = re.compile(r"\[(\d{1,2}):(\d{2})\]")
    TIMECODE_PATTERN_HHMMSS = re.compile(r"\[(\d{1,2}):(\d{2}):(\d{2})\]")
    
    def __init__(
        self,
        max_duration_seconds: Optional[int] = None,
        strict_ordering: bool = False,
    ):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è.
        
        Args:
            max_duration_seconds: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ñ–∞–π–ª–∞ (–¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏).
                                  –ï—Å–ª–∏ —Ç–∞–π–º–∫–æ–¥ –±–æ–ª—å—à–µ, –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ—Ç—Å—è Warning.
            strict_ordering: –ï—Å–ª–∏ True, —Ç–∞–π–º–∫–æ–¥—ã –¥–æ–ª–∂–Ω—ã –∏–¥—Ç–∏ –≤ –≤–æ–∑—Ä–∞—Å—Ç–∞—é—â–µ–º –ø–æ—Ä—è–¥–∫–µ.
        """
        self.max_duration_seconds = max_duration_seconds
        self.strict_ordering = strict_ordering
        self._last_timecode_seconds: Optional[int] = None
    
    def parse(self, text: str) -> Optional[TimecodeInfo]:
        """–ü–∞—Ä—Å–∏—Ç –ø–µ—Ä–≤—ã–π —Ç–∞–π–º–∫–æ–¥ –∏–∑ —Ç–µ–∫—Å—Ç–∞.
        
        Args:
            text: –¢–µ–∫—Å—Ç —Å —Ç–∞–π–º–∫–æ–¥–æ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, "[02:15] Speaker introduces topic").
        
        Returns:
            TimecodeInfo –∏–ª–∏ None, –µ—Å–ª–∏ —Ç–∞–π–º–∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –Ω–µ–≤–∞–ª–∏–¥–µ–Ω.
        """
        # –ü—Ä–æ–±—É–µ–º HH:MM:SS —Ñ–æ—Ä–º–∞—Ç
        match = self.TIMECODE_PATTERN_HHMMSS.search(text)
        if match:
            hours, minutes, secs = map(int, match.groups())
            total_seconds = hours * 3600 + minutes * 60 + secs
            original = match.group(0)
        else:
            # –ü—Ä–æ–±—É–µ–º MM:SS —Ñ–æ—Ä–º–∞—Ç
            match = self.TIMECODE_PATTERN_MMSS.search(text)
            if not match:
                return None
            
            minutes, secs = map(int, match.groups())
            total_seconds = minutes * 60 + secs
            original = match.group(0)
            hours = 0
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è: —Ç–∞–π–º–∫–æ–¥ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –±–æ–ª—å—à–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ñ–∞–π–ª–∞
        if self.max_duration_seconds is not None:
            if total_seconds > self.max_duration_seconds:
                logger.warning(
                    "Timecode exceeds file duration ‚Äî ignoring",
                    timecode=original,
                    seconds=total_seconds,
                    max_duration=self.max_duration_seconds,
                )
                return None
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è: —Å—Ç—Ä–æ–≥–∏–π –ø–æ—Ä—è–¥–æ–∫ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if self.strict_ordering and self._last_timecode_seconds is not None:
            if total_seconds <= self._last_timecode_seconds:
                logger.warning(
                    "Timecode order violation ‚Äî non-ascending",
                    timecode=original,
                    seconds=total_seconds,
                    last_seconds=self._last_timecode_seconds,
                )
                return None
        
        self._last_timecode_seconds = total_seconds
        
        return TimecodeInfo(
            original=original,
            seconds=total_seconds,
            minutes=minutes,
            secs=secs,
        )
    
    def parse_all(self, text: str) -> list[TimecodeInfo]:
        """–ü–∞—Ä—Å–∏—Ç –≤—Å–µ —Ç–∞–π–º–∫–æ–¥—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞.
        
        Args:
            text: –¢–µ–∫—Å—Ç —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Ç–∞–π–º–∫–æ–¥–∞–º–∏.
        
        Returns:
            –°–ø–∏—Å–æ–∫ TimecodeInfo (–º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º).
        """
        timecodes = []
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        for match in self.TIMECODE_PATTERN_HHMMSS.finditer(text):
            hours, minutes, secs = map(int, match.groups())
            total_seconds = hours * 3600 + minutes * 60 + secs
            original = match.group(0)
            
            if self._is_valid_timecode(original, total_seconds):
                timecodes.append(TimecodeInfo(
                    original=original,
                    seconds=total_seconds,
                    minutes=minutes,
                    secs=secs,
                ))
        
        for match in self.TIMECODE_PATTERN_MMSS.finditer(text):
            minutes, secs = map(int, match.groups())
            total_seconds = minutes * 60 + secs
            original = match.group(0)
            
            if self._is_valid_timecode(original, total_seconds):
                timecodes.append(TimecodeInfo(
                    original=original,
                    seconds=total_seconds,
                    minutes=minutes,
                    secs=secs,
                ))
        
        return timecodes
    
    def _is_valid_timecode(self, original: str, seconds: int) -> bool:
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç —Ç–∞–π–º–∫–æ–¥."""
        if self.max_duration_seconds is not None and seconds > self.max_duration_seconds:
            logger.warning(
                "Timecode exceeds file duration",
                timecode=original,
                seconds=seconds,
                max_duration=self.max_duration_seconds,
            )
            return False
        return True
    
    def inherit_timecode(
        self,
        last_timecode_seconds: Optional[int],
        chunk_position: int,
        total_chunks: int,
        total_duration_seconds: int,
    ) -> int:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ç–∞–π–º–∫–æ–¥ –¥–ª—è —á–∞–Ω–∫–∞ –±–µ–∑ —è–≤–Ω–æ–≥–æ —Ç–∞–π–º–∫–æ–¥–∞ (–Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ).
        
        –õ–æ–≥–∏–∫–∞:
        - –ï—Å–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤—ã–π —á–∞–Ω–∫ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º 0
        - –ï—Å–ª–∏ –µ—Å—Ç—å —Ç–∞–π–º–∫–æ–¥ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —á–∞–Ω–∫–∞ ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º –¥–µ–ª—å—Ç—É
        - –î–µ–ª—å—Ç–∞ = (total_duration / total_chunks)
        
        Args:
            last_timecode_seconds: –ü–æ—Å–ª–µ–¥–Ω–∏–π –∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∞–π–º–∫–æ–¥ (–æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —á–∞–Ω–∫–∞).
            chunk_position: –ü–æ–∑–∏—Ü–∏—è —Ç–µ–∫—É—â–µ–≥–æ —á–∞–Ω–∫–∞ (0-based).
            total_chunks: –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤.
            total_duration_seconds: –û–±—â–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ñ–∞–π–ª–∞.
        
        Returns:
            –°–µ–∫—É–Ω–¥—ã –æ—Ç –Ω–∞—á–∞–ª–∞ —Ñ–∞–π–ª–∞.
        """
        if chunk_position == 0:
            return 0
        
        if last_timecode_seconds is None:
            # –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
            delta = total_duration_seconds / total_chunks
            return int(chunk_position * delta)
        
        # –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç –æ—Ç –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∏–∑–≤–µ—Å—Ç–Ω–æ–≥–æ —Ç–∞–π–º–∫–æ–¥–∞
        delta = total_duration_seconds / total_chunks
        return int(last_timecode_seconds + delta)
```

### 3.3 –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ TranscriptionStep

```python
class TranscriptionStep(BaseProcessingStep):
    def __init__(
        self,
        splitter: BaseSplitter,
        chunk_size_override: Optional[int] = None,
        enable_timecodes: bool = True,
    ):
        self.splitter = splitter
        self.chunk_size_override = chunk_size_override
        self.enable_timecodes = enable_timecodes
    
    def process(self, context: MediaContext) -> MediaContext:
        transcription = context.analysis["transcription"]
        duration_seconds = context.analysis.get("duration_seconds")
        
        # –°–æ–∑–¥–∞—ë–º parser —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
        parser = TimecodeParser(
            max_duration_seconds=duration_seconds,
            strict_ordering=False,  # Gemini –º–æ–∂–µ—Ç –æ—à–∏–±–∏—Ç—å—Å—è –≤ –ø–æ—Ä—è–¥–∫–µ
        )
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
        split_chunks = self.splitter.split(...)
        
        # –û–±–æ–≥–∞—â–∞–µ–º —Ç–∞–π–º–∫–æ–¥–∞–º–∏
        last_timecode = None
        for idx, chunk in enumerate(split_chunks):
            # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å —Ç–∞–π–º–∫–æ–¥ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            timecode_info = parser.parse(chunk.content) if self.enable_timecodes else None
            
            if timecode_info:
                chunk.metadata["start_seconds"] = timecode_info.seconds
                chunk.metadata["timecode_original"] = timecode_info.original
                last_timecode = timecode_info.seconds
            else:
                # –ù–∞—Å–ª–µ–¥—É–µ–º –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —á–∞–Ω–∫–∞
                chunk.metadata["start_seconds"] = parser.inherit_timecode(
                    last_timecode_seconds=last_timecode,
                    chunk_position=idx,
                    total_chunks=len(split_chunks),
                    total_duration_seconds=duration_seconds or 0,
                )
        
        return context.with_chunks(split_chunks)
```

---

## 4. RetryParser ‚Äî Resilient JSON Parsing

### 4.1 –ú–æ—Ç–∏–≤–∞—Ü–∏—è

**–í–æ–ø—Ä–æ—Å:** –ù—É–∂–µ–Ω –ª–∏ RetryParser, –µ—Å–ª–∏ Gemini API –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ `response_schema`?

**–û—Ç–≤–µ—Ç:** **–ù–ï–¢ –¥–ª—è –Ω–æ–≤–æ–≥–æ –∫–æ–¥–∞, –î–ê –¥–ª—è –º–∏–≥—Ä–∞—Ü–∏–∏ legacy.**

### 4.2 –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ Analyzers

**–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–¥–æ–≤–æ–π –±–∞–∑—ã:**

- `audio_analyzer.py` (—Å—Ç—Ä–æ–∫–∞ 162): `response_json = json.loads(response.text)`
- `video_analyzer.py` (—Å—Ç—Ä–æ–∫–∞ 211): `response_json = json.loads(response.text)`
- `image_analyzer.py` (—Å—Ç—Ä–æ–∫–∞ 151): `response_json = json.loads(response.text)`

‚ùå **–¢–µ–∫—É—â–∏–π –∫–æ–¥ –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ—Ç `response_schema`** ‚Äî –ø–∞—Ä—Å–∏—Ç JSON –≤—Ä—É—á–Ω—É—é!

### 4.3 –î–≤–∞ –ø—É—Ç–∏ —Ä–µ—à–µ–Ω–∏—è

#### –í–∞—Ä–∏–∞–Ω—Ç A: –ú–∏–≥—Ä–∞—Ü–∏—è –Ω–∞ `response_schema` (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

**Phase 14.1.3:** –û–±–Ω–æ–≤–∏—Ç—å –≤—Å–µ analyzers –Ω–∞ Pydantic models.

```python
# audio_analyzer.py (NEW)
from pydantic import BaseModel, Field
from google.genai import types

class AudioAnalysisResult(BaseModel):
    description: str = Field(..., description="Brief 2-3 sentence summary")
    keywords: list[str] = Field(default_factory=list)
    participants: list[str] = Field(default_factory=list)
    action_items: list[str] = Field(default_factory=list)
    duration_seconds: Optional[int] = None
    transcription: str = Field(..., description="Markdown-formatted transcript")

# –í analyze() –º–µ—Ç–æ–¥–µ:
response = self.client.models.generate_content(
    model=self.model_name,
    contents=...,
    config=types.GenerateContentConfig(
        response_mime_type="application/json",
        response_schema=AudioAnalysisResult,  # ‚Üê –ì–∞—Ä–∞–Ω—Ç–∏—è –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏
    ),
)

# –ü–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ .parsed (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ Pydantic –æ–±—ä–µ–∫—Ç)
result = response.parsed  # type: AudioAnalysisResult
return {
    "type": "audio",
    "description": result.description,
    "transcription": result.transcription,
    # ...
}
```

**–ü–ª—é—Å—ã:**

- ‚úÖ Gemini API **–≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç** –≤–∞–ª–∏–¥–Ω—ã–π JSON
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —á–µ—Ä–µ–∑ Pydantic
- ‚úÖ –ù–µ –Ω—É–∂–µ–Ω RetryParser –≤–æ–æ–±—â–µ

**–ú–∏–Ω—É—Å—ã:**

- ‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞ –≤—Å–µ—Ö 3 analyzers
- ‚ö†Ô∏è Breaking change –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø—Ä–æ–º–ø—Ç–æ–≤

#### –í–∞—Ä–∏–∞–Ω—Ç B: RetryParser –¥–ª—è legacy (–≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ)

**–ï—Å–ª–∏ –Ω–µ —Ö–æ—Ç–∏–º —Ç—Ä–æ–≥–∞—Ç—å analyzers –ø—Ä—è–º–æ —Å–µ–π—á–∞—Å:**

**–§–∞–π–ª:** `semantic_core/infrastructure/gemini/retry_parser.py`

```python
import json
import re
from typing import Any, Optional
from semantic_core.utils.logger import get_logger

try:
    from json_repair import repair_json
    HAS_JSON_REPAIR = True
except ImportError:
    HAS_JSON_REPAIR = False

logger = get_logger(__name__)


class RetryParser:
    """Resilient JSON parser –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ LLM.
    
    –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è:
    1. json.loads() ‚Äî —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
    2. repair_json() ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ (–µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞)
    3. Regex extraction ‚Äî –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ JSON –∏–∑ Markdown code blocks
    4. Fallback ‚Äî –≤–æ–∑–≤—Ä–∞—Ç –æ—à–∏–±–∫–∏
    
    NOTE: –≠—Ç–æ—Ç –∫–ª–∞—Å—Å –ù–ï –ù–£–ñ–ï–ù, –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è response_schema –≤ Gemini API.
          –û—Å—Ç–∞–≤–ª–µ–Ω –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å legacy analyzers.
    """
    
    # Regex –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è JSON –∏–∑ Markdown
    JSON_BLOCK_PATTERN = re.compile(r"```(?:json)?\s*\n(.*?)\n```", re.DOTALL)
    
    @classmethod
    def parse(
        cls,
        text: str,
        context: str = "unknown",
    ) -> dict[str, Any]:
        """–ü–∞—Ä—Å–∏—Ç JSON —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è.
        
        Args:
            text: –¢–µ–∫—Å—Ç —Å JSON (–º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±—ë—Ä–Ω—É—Ç –≤ Markdown).
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, "audio_analyzer").
        
        Returns:
            –†–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–π dict.
        
        Raises:
            ValueError: –ï—Å–ª–∏ –≤—Å–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å.
        """
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ü—Ä—è–º–æ–π –ø–∞—Ä—Å–∏–Ω–≥
        try:
            return json.loads(text)
        except json.JSONDecodeError as e:
            logger.debug(
                "Standard JSON parsing failed ‚Äî trying repair strategies",
                context=context,
                error=str(e),
            )
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: repair_json (–µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞)
        if HAS_JSON_REPAIR:
            try:
                repaired = repair_json(text)
                result = json.loads(repaired)
                logger.warning(
                    "JSON repaired successfully",
                    context=context,
                    original_length=len(text),
                    repaired_length=len(repaired),
                )
                return result
            except Exception as e:
                logger.debug(
                    "JSON repair failed",
                    context=context,
                    error=str(e),
                )
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ Markdown code block
        match = cls.JSON_BLOCK_PATTERN.search(text)
        if match:
            try:
                extracted = match.group(1)
                result = json.loads(extracted)
                logger.warning(
                    "JSON extracted from Markdown code block",
                    context=context,
                )
                return result
            except json.JSONDecodeError:
                pass
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 4: Fallback ‚Äî –æ—à–∏–±–∫–∞
        logger.error(
            "All JSON parsing strategies failed",
            context=context,
            text_preview=text[:200],
        )
        raise ValueError(f"Failed to parse JSON in {context}")
```

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ audio_analyzer.py (–≤—Ä–µ–º–µ–Ω–Ω–æ):**

```python
# audio_analyzer.py (—Å—Ç—Ä–æ–∫–∞ 162)
from semantic_core.infrastructure.gemini.retry_parser import RetryParser

# –ë–´–õ–û:
# response_json = json.loads(response.text)

# –°–¢–ê–õ–û:
response_json = RetryParser.parse(response.text, context="audio_analyzer")
```

### 4.4 –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è

**Phase 14.1.3:**

1. ‚úÖ –ú–∏–≥—Ä–∏—Ä–æ–≤–∞—Ç—å analyzers –Ω–∞ `response_schema` (–í–∞—Ä–∏–∞–Ω—Ç A)
2. ‚ùå –ù–µ –¥–æ–±–∞–≤–ª—è—Ç—å RetryParser (–æ–Ω –Ω–µ –Ω—É–∂–µ–Ω)
3. üìù –î–æ–±–∞–≤–∏—Ç—å –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é: "Gemini API guarantees JSON validity via response_schema"

**–ï—Å–ª–∏ –º–∏–≥—Ä–∞—Ü–∏—è –∑–∞—Ç—è–Ω–µ—Ç—Å—è:**

- –í—Ä–µ–º–µ–Ω–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å RetryParser –¥–ª—è legacy –∫–æ–¥–∞ (–í–∞—Ä–∏–∞–Ω—Ç B)
- –ü–æ–º–µ—Ç–∏—Ç—å –∫–∞–∫ `@deprecated` –≤ docstring
- –£–¥–∞–ª–∏—Ç—å –ø–æ—Å–ª–µ –º–∏–≥—Ä–∞—Ü–∏–∏ –Ω–∞ `response_schema`

---

## 5. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è Markdown-–æ—Ç–≤–µ—Ç–æ–≤

### 4.1 –ü—Ä–æ–±–ª–µ–º–∞ —Å —Ç–µ–∫—É—â–∏–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏

**–¢–µ–∫—É—â–∏–µ –ø—Ä–æ–º–ø—Ç—ã** —Ç—Ä–µ–±—É—é—Ç JSON, –Ω–æ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π/OCR **–ª—É—á—à–µ –ø–æ–¥—Ö–æ–¥–∏—Ç Markdown**:

```python
# audio_analyzer.py:37
SYSTEM_PROMPT_TEMPLATE = """You are an audio analyst creating descriptions for semantic search indexing.
Response language: {language}

Return a JSON with:
- description: Brief summary
- transcription: Full verbatim text
- keywords: List of key terms
- participants: List of speakers
- action_items: List of tasks mentioned
"""
```

**–ü—Ä–æ–±–ª–µ–º—ã:**

1. **–ü–æ—Ç–µ—Ä—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:** –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ª–µ–∫—Ü–∏–π –ø—Ä–µ–≤—Ä–∞—â–∞—é—Ç—Å—è –≤ ¬´–ø—Ä–æ—Å—Ç—ã–Ω—é —Ç–µ–∫—Å—Ç–∞¬ª –±–µ–∑ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
2. **Code blocks –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è:** OCR –∏–∑ –≤–∏–¥–µ–æ —Å –∫–æ–¥–æ–º –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –∫–∞–∫ plain text, —Ç–µ—Ä—è—è —Å–∏–Ω—Ç–∞–∫—Å–∏—Å
3. **–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Å–ø–∏—Å–∫–æ–≤:** Action items –±–µ–∑ –∏–µ—Ä–∞—Ä—Ö–∏–∏

### 4.2 –¶–µ–ª–µ–≤—ã–µ –ø—Ä–æ–º–ø—Ç—ã (Markdown-first)

**–ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥:** –†–∞–∑–¥–µ–ª–∏—Ç—å –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è **–º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö** (JSON) –∏ **–∫–æ–Ω—Ç–µ–Ω—Ç–∞** (Markdown).

#### AudioAnalyzer ‚Äî Hybrid JSON+Markdown

```python
SYSTEM_PROMPT_TEMPLATE = """You are an audio analyst creating descriptions for semantic search indexing.
Response language: {language}

Return a JSON with the following structure:

{{
  "description": "Brief 2-3 sentence summary of the audio content",
  "keywords": ["keyword1", "keyword2", ...],
  "participants": ["Speaker1", "Speaker2", ...],
  "action_items": ["Task 1", "Task 2", ...],
  "duration_seconds": <number>,
  "transcription": "MARKDOWN_FORMATTED_TRANSCRIPT_HERE"
}}

CRITICAL INSTRUCTIONS FOR TRANSCRIPTION FIELD:
- Use Markdown formatting (paragraphs, headers, lists)
- Split long monologues into logical paragraphs (every 3-5 sentences)
- Use `## Speaker Name` headers for speaker changes
- Use `**bold**` for emphasis or key terms
- Use `> quote` for direct quotations
- For technical content, wrap code snippets in triple backticks with language:
  ```python
  def example():
      pass
  ```

- DO NOT escape newlines as \\n ‚Äî use actual line breaks inside the JSON string

Example transcription format:

## Introduction

The speaker introduces the topic of semantic search and explains how embeddings work in modern NLP systems.

Key points:

- Embeddings capture semantic meaning
- Vector databases enable similarity search
- Context matters more than keywords

## Technical Deep Dive

Here's how we calculate cosine similarity:

```python
def cosine_similarity(a, b):
    return dot(a, b) / (norm(a) * norm(b))
```

This formula is fundamental to understanding vector search.
"""

```

#### VideoAnalyzer ‚Äî OCR with Code Detection

```python
SYSTEM_PROMPT_TEMPLATE = """You are a video analyst creating descriptions for semantic search indexing.
Response language: {language}

Return a JSON with:

{{
  "description": "Brief summary",
  "keywords": ["keyword1", ...],
  "transcription": "MARKDOWN_FORMATTED_SPEECH_TRANSCRIPT",
  "ocr_text": "MARKDOWN_FORMATTED_VISUAL_TEXT",
  "duration_seconds": <number>
}}

CRITICAL INSTRUCTIONS FOR OCR_TEXT FIELD:
- Detect and preserve code blocks from screenshots/screencasts
- Wrap code in triple backticks with language:
  ```python
  class Example:
      pass
  ```

- Use `## Slide Title` headers for new slides
- Use bullet points for slide bullet lists:
  - Point 1
  - Point 2
- For UI text (buttons, labels), use plain text
- For diagrams/charts, describe structure in Markdown tables if possible

Example OCR output:

## Introduction to SOLID Principles

### Single Responsibility Principle

A class should have only one reason to change.

**Example:**

```python
class UserService:
    def validate(self, user): ...
    def save(self, user): ...
```

**Problem:** Mixes validation and persistence.

## Better Design

Split into two classes:

```python
class UserValidator:
    def validate(self, user): ...

class UserRepository:
    def save(self, user): ...
```

"""

```

### 4.3 –ü–∞—Ä—Å–∏–Ω–≥ Markdown-–æ—Ç–≤–µ—Ç–æ–≤

**Challenge:** Gemini –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç JSON —Å Markdown **–≤–Ω—É—Ç—Ä–∏** —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –ø–æ–ª–µ–π.

**–†–µ—à–µ–Ω–∏–µ:** –ò—Å–ø–æ–ª—å–∑—É–µ–º `json.loads()` –∫–∞–∫ –æ–±—ã—á–Ω–æ, Markdown –ø–∞—Ä—Å–∏—Ç—Å—è –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ `Document`:

```python
# –í audio_analyzer.py:
response_json = json.loads(response.text)

# transcription —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç Markdown:
# "## Speaker 1\n\nHello world...\n\n```python\ndef foo(): pass\n```"

return {
    "type": "audio",
    "description": response_json["description"],
    "transcription": response_json["transcription"],  # ‚Üê Markdown string
    # ...
}

# –í TranscriptionStep.process():
temp_doc = Document(
    content=analysis["transcription"],  # ‚Üê Markdown content
    media_type=MediaType.MARKDOWN,       # ‚Üê –í–∫–ª—é—á–∞–µ—Ç MarkdownNodeParser
)
chunks = splitter.split(temp_doc)  # ‚Üê –ü–∞—Ä—Å–∏—Ç Markdown ‚Üí –∏–∑–æ–ª–∏—Ä—É–µ—Ç code blocks
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** Code blocks –∏–∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π **–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏** –¥–µ—Ç–µ–∫—Ç—è—Ç—Å—è –∏ –∏–∑–æ–ª–∏—Ä—É—é—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —á–∞–Ω–∫–∏.

### 4.4 Migration Plan –¥–ª—è –ø—Ä–æ–º–ø—Ç–æ–≤

**Phase 14.1.5: –ü—Ä–æ–º–ø—Ç—ã**

- [ ] –û–±–Ω–æ–≤–∏—Ç—å `audio_analyzer.py:SYSTEM_PROMPT_TEMPLATE` (–¥–æ–±–∞–≤–∏—Ç—å Markdown –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏)
- [ ] –û–±–Ω–æ–≤–∏—Ç—å `video_analyzer.py:SYSTEM_PROMPT_TEMPLATE` (OCR code detection)
- [ ] –î–æ–±–∞–≤–∏—Ç—å E2E —Ç–µ—Å—Ç: –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –≤–∏–¥–µ–æ —Å –∫–æ–¥–æ–º ‚Üí –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ CODE chunks
- [ ] –î–æ–±–∞–≤–∏—Ç—å E2E —Ç–µ—Å—Ç: –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –∞—É–¥–∏–æ –ª–µ–∫—Ü–∏—é ‚Üí –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –≤ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
- [ ] –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Ñ–æ—Ä–º–∞—Ç –ø—Ä–æ–º–ø—Ç–æ–≤ –≤ `doc/architecture/`

**–†–∏—Å–∫–∏:**

‚ö†Ô∏è **Gemini –º–æ–∂–µ—Ç –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å Markdown –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏** (–º–æ–¥–µ–ª–∏ –∏–Ω–æ–≥–¥–∞ —É–ø—Ä—è–º—ã)  
**Mitigation:** –î–æ–±–∞–≤–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã –≤ –ø—Ä–æ–º–ø—Ç (few-shot learning)

‚ö†Ô∏è **–ü–∞—Ä—Å–∏–Ω–≥ JSON —Å Markdown –º–æ–∂–µ—Ç —Å–ª–æ–º–∞—Ç—å—Å—è –Ω–∞ –Ω–µ—ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–∞–≤—ã—á–∫–∞—Ö**  
**Mitigation:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `response.text.strip()` + fallback –Ω–∞ regex extraction

---

## 7. E2E Testing Strategy

### 5.1 –ó–∞—á–µ–º E2E —Ç–µ—Å—Ç—ã?

**Unit-—Ç–µ—Å—Ç—ã** (—Ç–µ–∫—É—â–∏–µ) –ø—Ä–æ–≤–µ—Ä—è—é—Ç –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—É—é –ª–æ–≥–∏–∫—É:

```python
def test_summary_step_creates_chunk():
    step = SummaryStep()
    context = MediaContext(analysis={"type": "audio", "description": "Test"}, ...)
    result = step.process(context)
    assert len(result.chunks) == 1
```

‚ùå **–ù–µ –ø—Ä–æ–≤–µ—Ä—è—é—Ç:**

- –†–µ–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ SQLite
- –ü–æ–∑–∏—Ü–∏–∏ —á–∞–Ω–∫–æ–≤ –≤ –ë–î (`chunk_index`)
- –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
- –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —à–∞–≥–æ–≤ (base_index propagation)

**E2E —Ç–µ—Å—Ç—ã** –ø—Ä–æ–≤–µ—Ä—è—é—Ç **–≤–µ—Å—å flow** –æ—Ç —Ñ–∞–π–ª–∞ –¥–æ –ë–î:

```python
def test_video_with_code_creates_ocr_code_chunks(tmp_path, real_db):
    # –°–æ–∑–¥–∞—ë–º —Ç–µ—Å—Ç–æ–≤–æ–µ –≤–∏–¥–µ–æ —Å –∫–æ–¥–æ–º –Ω–∞ —ç–∫—Ä–∞–Ω–µ
    video_path = tmp_path / "python_tutorial.mp4"
    create_test_video_with_code(video_path)  # Helper
    
    # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ SemanticCore
    core = SemanticCore(db_path=real_db)
    doc_id = core.ingest_video(str(video_path), mode="sync")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ë–î –Ω–∞–ø—Ä—è–º—É—é
    chunks = ChunkModel.select().where(ChunkModel.document_id == doc_id)
    
    # Assertions:
    assert chunks.count() >= 3  # summary + transcript + ocr
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º summary chunk
    summary = chunks.where(ChunkModel.metadata["role"].as_json() == "summary").get()
    assert summary.chunk_index == 0
    assert summary.chunk_type == "video_ref"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º OCR code chunks
    code_chunks = chunks.where(
        (ChunkModel.metadata["role"].as_json() == "ocr")
        & (ChunkModel.chunk_type == "code")
    )
    assert code_chunks.count() >= 1  # –•–æ—Ç—è –±—ã –æ–¥–∏–Ω code block –¥–µ—Ç–µ–∫—Ç–Ω—É—Ç
    assert code_chunks[0].language == "python"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º chunk_index –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
    all_indexes = [c.chunk_index for c in chunks.order_by(ChunkModel.chunk_index)]
    assert all_indexes == list(range(len(all_indexes)))  # 0, 1, 2, 3, ...
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    for chunk in chunks:
        assert chunk.embedding is not None
        assert len(chunk.embedding) == 768  # Gemini embedding dimension
```

### 5.2 E2E Test Suite

**–§–∞–π–ª:** `tests/e2e/test_media_pipeline_steps.py`

```python
import pytest
from pathlib import Path
from semantic_core import SemanticCore
from semantic_core.infrastructure.storage.peewee.models import ChunkModel, DocumentModel


@pytest.fixture
def core_with_steps(tmp_path):
    """–°–æ–∑–¥–∞—ë—Ç SemanticCore —Å —Ä–µ–∞–ª—å–Ω–æ–π –ë–î –∏ step-based pipeline."""
    db_path = tmp_path / "test.db"
    core = SemanticCore(db_path=str(db_path))
    yield core
    core.close()


class TestAudioTranscriptionStep:
    """E2E —Ç–µ—Å—Ç—ã –¥–ª—è TranscriptionStep —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –∞—É–¥–∏–æ."""
    
    def test_long_audio_creates_multiple_transcript_chunks(self, core_with_steps, sample_audio_5min):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ 5-–º–∏–Ω—É—Ç–Ω–æ–µ –∞—É–¥–∏–æ —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞–Ω–∫–æ–≤."""
        doc_id = core_with_steps.ingest_audio(str(sample_audio_5min), mode="sync")
        
        chunks = list(ChunkModel.select().where(ChunkModel.document_id == doc_id))
        
        # Summary + N transcript chunks
        assert len(chunks) >= 5, "Expected at least 5 chunks for 5-min audio"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º summary
        summary = next(c for c in chunks if c.metadata.get("role") == "summary")
        assert summary.chunk_index == 0
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º transcript chunks
        transcripts = [c for c in chunks if c.metadata.get("role") == "transcript"]
        assert len(transcripts) >= 4
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–æ–≤
        for i, chunk in enumerate(transcripts):
            assert chunk.chunk_index == i + 1  # –ü–æ—Å–ª–µ summary
    
    def test_transcript_chunks_preserve_parent_path(self, core_with_steps, sample_audio_5min):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –≤—Å–µ transcript —á–∞–Ω–∫–∏ —Å—Å—ã–ª–∞—é—Ç—Å—è –Ω–∞ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π —Ñ–∞–π–ª."""
        doc_id = core_with_steps.ingest_audio(str(sample_audio_5min), mode="sync")
        
        transcripts = list(
            ChunkModel.select().where(
                (ChunkModel.document_id == doc_id)
                & (ChunkModel.metadata["role"].as_json() == "transcript")
            )
        )
        
        for chunk in transcripts:
            assert chunk.metadata["parent_media_path"] == str(sample_audio_5min)


class TestVideoOCRStep:
    """E2E —Ç–µ—Å—Ç—ã –¥–ª—è OCRStep —Å Markdown parsing."""
    
    def test_video_with_code_screenshot_detects_code_blocks(
        self, core_with_steps, sample_video_with_code
    ):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –∫–æ–¥ –Ω–∞ —ç–∫—Ä–∞–Ω–µ –¥–µ—Ç–µ–∫—Ç–∏—Ç—Å—è –∫–∞–∫ CODE chunks."""
        doc_id = core_with_steps.ingest_video(str(sample_video_with_code), mode="sync")
        
        code_chunks = list(
            ChunkModel.select().where(
                (ChunkModel.document_id == doc_id)
                & (ChunkModel.chunk_type == "code")
                & (ChunkModel.metadata["role"].as_json() == "ocr")
            )
        )
        
        assert len(code_chunks) >= 1, "Expected at least one CODE chunk from OCR"
        assert code_chunks[0].language in ("python", "javascript", "java")
    
    def test_video_ocr_markdown_headers_preserved(
        self, core_with_steps, sample_video_slides
    ):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å–ª–∞–π–¥–æ–≤ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ metadata."""
        doc_id = core_with_steps.ingest_video(str(sample_video_slides), mode="sync")
        
        ocr_chunks = list(
            ChunkModel.select().where(
                (ChunkModel.document_id == doc_id)
                & (ChunkModel.metadata["role"].as_json() == "ocr")
            )
        )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —á–∞–Ω–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç header –≤ metadata
        headers_found = any(
            chunk.metadata.get("headers") for chunk in ocr_chunks
        )
        assert headers_found, "Expected headers in OCR chunk metadata"


class TestStepIndexPropagation:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å chunk_index –ø—Ä–∏ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ."""
    
    def test_chunk_indexes_are_sequential(self, core_with_steps, sample_video_full):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –∏–Ω–¥–µ–∫—Å—ã –∏–¥—É—Ç 0, 1, 2, 3... –±–µ–∑ –ø—Ä–æ–ø—É—Å–∫–æ–≤."""
        doc_id = core_with_steps.ingest_video(str(sample_video_full), mode="sync")
        
        chunks = list(
            ChunkModel.select()
            .where(ChunkModel.document_id == doc_id)
            .order_by(ChunkModel.chunk_index)
        )
        
        expected_indexes = list(range(len(chunks)))
        actual_indexes = [c.chunk_index for c in chunks]
        
        assert actual_indexes == expected_indexes, "Chunk indexes must be sequential"
    
    def test_summary_always_first_transcript_second_ocr_last(
        self, core_with_steps, sample_video_full
    ):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ—Ä—è–¥–æ–∫ —à–∞–≥–æ–≤: summary ‚Üí transcript ‚Üí ocr."""
        doc_id = core_with_steps.ingest_video(str(sample_video_full), mode="sync")
        
        chunks = list(
            ChunkModel.select()
            .where(ChunkModel.document_id == doc_id)
            .order_by(ChunkModel.chunk_index)
        )
        
        roles = [c.metadata.get("role") for c in chunks]
        
        # Summary –≤—Å–µ–≥–¥–∞ –ø–µ—Ä–≤—ã–π
        assert roles[0] == "summary"
        
        # Transcript chunks –∏–¥—É—Ç –ø–æ–¥—Ä—è–¥
        first_transcript_idx = roles.index("transcript")
        last_transcript_idx = len(roles) - 1 - roles[::-1].index("transcript")
        
        # OCR chunks –∏–¥—É—Ç –ø–æ—Å–ª–µ –≤—Å–µ—Ö transcript
        if "ocr" in roles:
            first_ocr_idx = roles.index("ocr")
            assert first_ocr_idx > last_transcript_idx


class TestEmbeddings:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤."""
    
    def test_all_chunks_have_embeddings(self, core_with_steps, sample_audio_5min):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –≤—Å–µ —á–∞–Ω–∫–∏ –ø–æ–ª—É—á–∏–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏."""
        doc_id = core_with_steps.ingest_audio(str(sample_audio_5min), mode="sync")
        
        chunks = ChunkModel.select().where(ChunkModel.document_id == doc_id)
        
        for chunk in chunks:
            assert chunk.embedding is not None, f"Chunk {chunk.id} missing embedding"
            assert len(chunk.embedding) == 768, "Gemini embeddings are 768-dim"
    
    def test_code_chunks_embeddings_differ_from_text(
        self, core_with_steps, sample_video_with_code
    ):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ CODE –∏ TEXT —á–∞–Ω–∫–∏ –∏–º–µ—é—Ç —Ä–∞–∑–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏."""
        import numpy as np
        
        doc_id = core_with_steps.ingest_video(str(sample_video_with_code), mode="sync")
        
        code_chunk = ChunkModel.get(
            (ChunkModel.document_id == doc_id) & (ChunkModel.chunk_type == "code")
        )
        text_chunk = ChunkModel.get(
            (ChunkModel.document_id == doc_id) & (ChunkModel.chunk_type == "text")
        )
        
        # –í—ã—á–∏—Å–ª—è–µ–º cosine similarity
        code_vec = np.array(code_chunk.embedding)
        text_vec = np.array(text_chunk.embedding)
        similarity = np.dot(code_vec, text_vec) / (
            np.linalg.norm(code_vec) * np.linalg.norm(text_vec)
        )
        
        # –î–æ–ª–∂–Ω—ã –±—ã—Ç—å –ù–ï –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–º–∏ (similarity < 0.99)
        assert similarity < 0.99, "Code and text embeddings should differ"
```

### 5.3 Test Fixtures

**–§–∞–π–ª:** `tests/fixtures/media_samples.py`

```python
import pytest
from pathlib import Path
import subprocess


@pytest.fixture(scope="session")
def sample_audio_5min(tmp_path_factory):
    """–°–æ–∑–¥–∞—ë—Ç 5-–º–∏–Ω—É—Ç–Ω–æ–µ —Ç–µ—Å—Ç–æ–≤–æ–µ –∞—É–¥–∏–æ —Å —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ä–µ—á—å—é."""
    audio_path = tmp_path_factory.mktemp("media") / "sample_5min.mp3"
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ TTS (–∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º pre-recorded —Ñ–∞–π–ª)
    # –î–ª—è CI: —Å–∫–∞—á–∏–≤–∞–µ–º —Å test assets
    download_test_asset("sample_5min.mp3", audio_path)
    
    return audio_path


@pytest.fixture(scope="session")
def sample_video_with_code(tmp_path_factory):
    """–°–æ–∑–¥–∞—ë—Ç –≤–∏–¥–µ–æ —Å –∫–æ–¥–æ–º –Ω–∞ —ç–∫—Ä–∞–Ω–µ (Python tutorial screencast)."""
    video_path = tmp_path_factory.mktemp("media") / "python_tutorial.mp4"
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º ffmpeg –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–¥–µ–æ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –∫–æ–¥–æ–º
    create_video_from_code_image(video_path)
    
    return video_path


def create_video_from_code_image(output_path: Path):
    """–°–æ–∑–¥–∞—ë—Ç 10-—Å–µ–∫—É–Ω–¥–Ω–æ–µ –≤–∏–¥–µ–æ —Å –∫–æ–¥–æ–º –Ω–∞ —ç–∫—Ä–∞–Ω–µ."""
    # 1. –°–æ–∑–¥–∞—ë–º PNG —Å –∫–æ–¥–æ–º
    code_image = output_path.parent / "code.png"
    generate_code_screenshot(
        code="""
def calculate_similarity(a, b):
    return dot(a, b) / (norm(a) * norm(b))
""",
        output=code_image,
    )
    
    # 2. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –≤–∏–¥–µ–æ
    subprocess.run([
        "ffmpeg", "-loop", "1", "-i", str(code_image),
        "-t", "10", "-pix_fmt", "yuv420p", str(output_path)
    ], check=True)
```

---

## 8. –†–∏—Å–∫–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è

### 6.1 –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ä–∏—Å–∫–∏

| –†–∏—Å–∫ | –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å | –í–ª–∏—è–Ω–∏–µ | Mitigation |
|------|-------------|---------|-----------|
| **Context mutation bugs** | –°—Ä–µ–¥–Ω—è—è | –í—ã—Å–æ–∫–æ–µ | Immutable dataclass + copy() |
| **Step dependency hell** | –ù–∏–∑–∫–∞—è | –°—Ä–µ–¥–Ω–µ–µ | Service locator pattern |
| **False code detection in OCR** | –°—Ä–µ–¥–Ω—è—è | –ù–∏–∑–∫–æ–µ | –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ code_ratio + config toggle |
| **Gemini –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç Markdown** | –°—Ä–µ–¥–Ω—è—è | –°—Ä–µ–¥–Ω–µ–µ | Few-shot examples –≤ –ø—Ä–æ–º–ø—Ç–µ |
| **Rerun step –Ω–µ –æ–±–Ω–æ–≤–ª—è–µ—Ç embeddings** | –í—ã—Å–æ–∫–∞—è | –ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ | TODO: Implement partial embedding update |

### 6.2 –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è Phase 14.1

**–ß—Ç–æ –ù–ï –≤—Ö–æ–¥–∏—Ç –≤ —ç—Ç—É —Ñ–∞–∑—É:**

‚ùå **–ö–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä—É–µ–º—ã–µ –ø—Ä–æ–º–ø—Ç—ã —á–µ—Ä–µ–∑ TOML** ‚Äî —ç—Ç–æ Phase 14.3  
‚ùå **–ü–ª–∞–≥–∏–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω—ã—Ö —à–∞–≥–æ–≤** ‚Äî —á–∞—Å—Ç–∏—á–Ω–æ (—Ç–æ–ª—å–∫–æ `register_step()`)  
‚ùå **Timeline extraction** (timestamps –¥–ª—è –≤–∏–¥–µ–æ) ‚Äî —ç—Ç–æ Phase 14.2  
‚ùå **Re-run —Å –Ω–æ–≤—ã–º –∞–Ω–∞–ª–∏–∑–æ–º** ‚Äî —Ç–µ–∫—É—â–∏–π rerun –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç–∞—Ä—ã–π analysis –∏–∑ –ë–î  
‚ùå **Partial embedding update** ‚Äî rerun –ø–µ—Ä–µ—Å–æ–∑–¥–∞—ë—Ç —á–∞–Ω–∫–∏, –Ω–æ embeddings –Ω—É–∂–Ω–æ –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ

### 6.3 –ó–∞—â–∏—Ç–∞ –æ—Ç –∏–Ω—ä–µ–∫—Ü–∏–π

**–†–µ—à–µ–Ω–∏–µ:** –ù–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ª–∏—á–Ω–æ–≥–æ —Å–æ—Ñ—Ç–∞.

**–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ:**

- SemanticCore –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω–æ, –Ω–µ—Ç –≤–Ω–µ—à–Ω–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
- Flask App (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è) —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ localhost
- –ù–µ—Ç –ø—É–±–ª–∏—á–Ω–æ–≥–æ API endpoint

**–ï—Å–ª–∏ –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –≤ –±—É–¥—É—â–µ–º (Phase 15+):**

- –í–∞–ª–∏–¥–∞—Ü–∏—è `step_name` —á–µ—Ä–µ–∑ whitelist
- Pydantic –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è `MediaContext`
- Rate limiting –¥–ª—è `rerun_step()`

---

## 7. Success Metrics

### 7.1 –ö—Ä–∏—Ç–µ—Ä–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ñ–∞–∑—ã

**Code Metrics:**

- ‚úÖ 100% –ø–æ–∫—Ä—ã—Ç–∏–µ unit-—Ç–µ—Å—Ç–∞–º–∏ –¥–ª—è `BaseProcessingStep`, `MediaContext`
- ‚úÖ E2E —Ç–µ—Å—Ç—ã –ø—Ä–æ—Ö–æ–¥—è—Ç –¥–ª—è audio (5 min), video (—Å –∫–æ–¥–æ–º), video (—Å–ª–∞–π–¥—ã)
- ‚úÖ Chunk indexes –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã (0, 1, 2...) –≤ 100% —Å–ª—É—á–∞–µ–≤
- ‚úÖ Code detection –≤ OCR —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é >80% (–Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–∏–¥–µ–æ)

**Performance:**

- ‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è 5-–º–∏–Ω—É—Ç–Ω–æ–≥–æ –∞—É–¥–∏–æ < 30 —Å–µ–∫—É–Ω–¥ (sync mode)
- ‚úÖ Memory overhead –æ—Ç step executor < 10% (vs legacy)

**Documentation:**

- ‚úÖ Architecture article –Ω–∞–ø–∏—Å–∞–Ω–∞ (`doc/architecture/74_processing_steps.md`)
- ‚úÖ –ü—Ä–∏–º–µ—Ä—ã –∫–∞—Å—Ç–æ–º–Ω—ã—Ö —à–∞–≥–æ–≤ –≤ `examples/custom_steps/`
- ‚úÖ Migration guide –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å Phase 14.0

### 7.2 Rollback Plan

**–ï—Å–ª–∏ Phase 14.1 –ø—Ä–æ–≤–∞–ª–∏—Ç—Å—è:**

1. –û—Ç–∫–∞—Ç—ã–≤–∞–µ–º –∫ Phase 14.0 (legacy `_build_media_chunks()` –æ—Å—Ç–∞—ë—Ç—Å—è —Ä–∞–±–æ—á–∏–º)
2. Step-based –º–µ—Ç–æ–¥—ã (`_build_media_chunks_v2()`) —É–¥–∞–ª—è–µ–º
3. Markdown –ø—Ä–æ–º–ø—Ç—ã –æ—Ç–∫–∞—Ç—ã–≤–∞–µ–º –∫ JSON-only

**–ö—Ä–∏—Ç–µ—Ä–∏–π –ø—Ä–æ–≤–∞–ª–∞:** E2E —Ç–µ—Å—Ç—ã –Ω–µ –ø—Ä–æ—Ö–æ–¥—è—Ç –ø–æ—Å–ª–µ 2 –Ω–µ–¥–µ–ª—å –æ—Ç–ª–∞–¥–∫–∏.

---

## 8. Next Steps

–ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è Phase 14.1:

**Phase 14.2: Aggregation & Service Layer**

- `MediaService.get_media_details(doc_id)` ‚Äî —Å–±–æ—Ä–∫–∞ —á–∞–Ω–∫–æ–≤ –≤ DTO
- Flask UI –¥–ª—è `/media/<id>` —Å timeline
- Search filters –ø–æ `role` (—Ç–æ–ª—å–∫–æ transcript, —Ç–æ–ª—å–∫–æ OCR)

**Phase 14.3: User Flexibility**

- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä—É–µ–º—ã–µ –ø—Ä–æ–º–ø—Ç—ã —á–µ—Ä–µ–∑ `semantic.toml`
- `ocr_parser_mode` config field
- Per-role chunk sizing (`transcript_chunk_size`, `ocr_chunk_size`)
- Full rerun —Å –Ω–æ–≤—ã–º –∞–Ω–∞–ª–∏–∑–æ–º (–Ω–µ –∏–∑ –∫—ç—à–∞ –ë–î)

---

**End of Phase 14.1 Plan**  
**Status:** Ready for implementation  
**Estimated Duration:** 3-4 weeks  
**Team:** 1 senior engineer + 1 QA for E2E tests
