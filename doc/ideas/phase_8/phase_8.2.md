````markdown
# ğŸ“‹ Phase 8.2: RAG Chat â€” Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ

**Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ:** ğŸ”² ĞŸĞ»Ğ°Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ  
**Ğ—Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸:** Phase 8.0 (Core CLI) âœ…

---

## ğŸ¯ Ğ¦ĞµĞ»ÑŒ

Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ‡Ğ°Ñ‚-Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ñ RAG (Retrieval-Augmented Generation):
- **REPL Ñ€ĞµĞ¶Ğ¸Ğ¼** â€” Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚
- **ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº** â€” Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ retrieval Ğ¸Ğ· Ğ±Ğ°Ğ·Ñ‹
- **LLM Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ** â€” Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Gemini Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼

---

## ğŸ§  RAG Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User      â”‚â”€â”€â”€â”€â–¶â”‚   Search    â”‚â”€â”€â”€â”€â–¶â”‚  Build      â”‚â”€â”€â”€â”€â–¶â”‚   Gemini    â”‚
â”‚   Query     â”‚     â”‚   (top-k)   â”‚     â”‚  Prompt     â”‚     â”‚   Generate  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚                                       â”‚
                           â–¼                                       â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Retrieved  â”‚                         â”‚   Answer    â”‚
                    â”‚  Chunks     â”‚                         â”‚  + Sources  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ğ¨Ğ°Ğ³Ğ¸

1. **Retrieval:** `core.search(query, limit=5)` â†’ ÑĞ¿Ğ¸ÑĞ¾Ğº Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²
2. **Prompt Building:** Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ· Ñ‡Ğ°Ğ½ĞºĞ¾Ğ² + Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ
3. **Generation:** ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ² Gemini, Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚
4. **Presentation:** Ğ ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚ ĞºĞ°Ğº Markdown + Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸

---

## ğŸ“¦ ĞĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸

```text
semantic_core/cli/commands/
â””â”€â”€ chat.py               # semantic chat

semantic_core/core/
â””â”€â”€ rag.py                # RAGPipeline class (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)
```

---

## ğŸ“ ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° `chat` â€” Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼

**Ğ¤Ğ°Ğ¹Ğ»:** `commands/chat.py`

### Ğ¡Ğ¸Ğ³Ğ½Ğ°Ñ‚ÑƒÑ€Ğ°

```bash
semantic chat [OPTIONS]
```

### ĞĞ¿Ñ†Ğ¸Ğ¸

| ĞĞ¿Ñ†Ğ¸Ñ | Ğ¢Ğ¸Ğ¿ | ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ |
|-------|-----|----------|
| `--model` | TEXT | ĞœĞ¾Ğ´ĞµĞ»ÑŒ Gemini (default: gemini-2.5-flash) |
| `--context-chunks` | INT | ĞšĞ¾Ğ»-Ğ²Ğ¾ Ñ‡Ğ°Ğ½ĞºĞ¾Ğ² Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° (default: 5) |
| `--system-prompt` | TEXT | ĞšĞ°ÑÑ‚Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ |
| `--no-sources` | FLAG | ĞĞµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ |

### REPL Loop

```python
from rich.prompt import Prompt
from rich.markdown import Markdown

console.print("ğŸ’¬ Semantic Chat (type 'exit' to quit, '/help' for commands)")
console.print()

while True:
    try:
        query = Prompt.ask("[bold blue]You[/]")
        
        if query.lower() in ("exit", "quit", "/q"):
            break
        
        if query.startswith("/"):
            handle_slash_command(query)
            continue
        
        # RAG Pipeline
        with console.status("ğŸ” Searching..."):
            chunks = core.search(query, limit=context_chunks)
        
        with console.status("ğŸ§  Thinking..."):
            answer = generate_answer(query, chunks, model)
        
        # Render answer
        console.print()
        console.print(Markdown(answer))
        console.print()
        
        # Show sources
        if not no_sources and chunks:
            render_sources(chunks, console)
        
    except KeyboardInterrupt:
        break

console.print("\nğŸ‘‹ Goodbye!")
```

### Slash Commands

| ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° | Ğ”ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ |
|---------|----------|
| `/help` | ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ ÑĞ¿Ñ€Ğ°Ğ²ĞºÑƒ |
| `/sources` | ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° |
| `/source N` | ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° N |
| `/clear` | ĞÑ‡Ğ¸ÑÑ‚Ğ¸Ñ‚ÑŒ ÑĞºÑ€Ğ°Ğ½ |
| `/model <name>` | Ğ¡Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ |
| `/context N` | Ğ˜Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»-Ğ²Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… Ñ‡Ğ°Ğ½ĞºĞ¾Ğ² |

### UX

```
$ semantic chat

ğŸ’¬ Semantic Chat (type 'exit' to quit, '/help' for commands)

You: ĞšĞ°Ğº Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº?

ğŸ” Searching... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100%
ğŸ§  Thinking... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100%

Ğ”Ğ»Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´ `search()` Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ¼ 
`search_type="hybrid"`:

```python
results = core.search(
    query="Ğ²Ğ°Ñˆ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ",
    search_type="hybrid",
    limit=10,
)
```

Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ **Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº** (ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾) 
Ğ¸ **FTS5** (Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ) Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ **RRF** (Reciprocal Rank Fusion).

ğŸ“š Sources:
  [1] docs/architecture/05_hybrid_search_rrf.md (score: 0.94)
  [2] docs/architecture/04_search_types.md (score: 0.87)

You: /source 1

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Source [1]: docs/architecture/05_hybrid_search_rrf.md

## Hybrid Search Ñ RRF

Reciprocal Rank Fusion (RRF) â€” ÑÑ‚Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¿Ğ¸ÑĞºĞ¾Ğ²...
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

You: exit

ğŸ‘‹ Goodbye!
```

---

## ğŸ“ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°

### System Prompt Template

```python
SYSTEM_PROMPT = """You are a helpful assistant for the Semantic Core library.
Answer questions based ONLY on the provided context.
If the context doesn't contain the answer, say "I don't have information about that."

Format your response in Markdown. Use code blocks for code examples.
Be concise but complete."""
```

### Context Building

```python
def build_context(chunks: list[SearchResult]) -> str:
    """Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ñ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ· Ñ‡Ğ°Ğ½ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°."""
    context_parts = []
    
    for i, chunk in enumerate(chunks, 1):
        source = chunk.metadata.get("source_file", "unknown")
        context_parts.append(f"[Source {i}: {source}]\n{chunk.content}\n")
    
    return "\n---\n".join(context_parts)
```

### Prompt Assembly

```python
def build_prompt(query: str, context: str, system_prompt: str) -> str:
    return f"""{system_prompt}

CONTEXT:
{context}

USER QUESTION:
{query}

ANSWER:"""
```

### Gemini Call

```python
from google import genai

def generate_answer(query: str, chunks: list, model: str) -> str:
    context = build_context(chunks)
    prompt = build_prompt(query, context, SYSTEM_PROMPT)
    
    client = genai.Client(api_key=settings.api_key)
    response = client.models.generate_content(
        model=model,
        contents=prompt,
    )
    
    return response.text
```

---

## ğŸ”¤ CLI Ğ­Ğ¼Ğ¾Ğ´Ğ·Ğ¸ Ğ´Ğ»Ñ Ğ»Ğ¾Ğ³Ğ³ĞµÑ€Ğ°

**ĞĞ¾Ğ²Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹:**

| ĞŸĞ°Ñ‚Ñ‚ĞµÑ€Ğ½ | Ğ­Ğ¼Ğ¾Ğ´Ğ·Ğ¸ | ĞœĞ¾Ğ´ÑƒĞ»ÑŒ |
|---------|--------|--------|
| `chat` | ğŸ’¬ | chat.py |
| `rag` | ğŸ¤– | rag.py |
| `generate`, `llm` | ğŸ§  | Ğ£Ğ¶Ğµ ĞµÑÑ‚ÑŒ |

**Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ² EMOJI_MAP:**
- `chat` â†’ ğŸ’¬
- `rag` â†’ ğŸ¤–

---

## âœ… Acceptance Criteria

### Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ

1. [ ] `semantic chat` Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ REPL
2. [ ] ĞŸĞ¾Ğ¸ÑĞº Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ñ‡Ğ°Ğ½ĞºĞ¸
3. [ ] Gemini Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°
4. [ ] ĞÑ‚Ğ²ĞµÑ‚ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ñ‚ÑÑ ĞºĞ°Ğº Markdown
5. [ ] Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°
6. [ ] `/source N` Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°
7. [ ] `/help` Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚
8. [ ] `exit` Ğ¸ Ctrl+C ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ°ÑÑ‚ Ñ‡Ğ°Ñ‚

### ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾

9. [ ] Rate limiting Ğ´Ğ»Ñ Gemini (Ğ½Ğµ ÑĞ¿Ğ°Ğ¼Ğ¸Ñ‚ÑŒ API)
10. [ ] ĞÑˆĞ¸Ğ±ĞºĞ¸ API Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ ĞºÑ€Ğ°ÑĞ¸Ğ²Ğ¾
11. [ ] Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´ (readline Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ)

### Ğ¢ĞµÑÑ‚Ñ‹

12. [ ] Unit-Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ build_context()
13. [ ] Unit-Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ build_prompt()
14. [ ] Integration-Ñ‚ĞµÑÑ‚ RAG pipeline (mock Gemini)

---

## ğŸ“š Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ (Ğ¿Ğ¾ÑĞ»Ğµ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸)

### ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ ÑĞµÑ€Ğ¸Ğ°Ğ»

1. **Episode 42:** `42_rag_pipeline.md` â€” RAG Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°
   - Retrieval + Augmentation + Generation
   - Prompt engineering
   - Context window management

2. **Episode 43:** `43_chat_interface.md` â€” REPL Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹
   - Slash commands
   - Session state
   - Rich console integration

### ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ

- Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ ÑĞµĞºÑ†Ğ¸Ñ "Chat Mode" Ğ² README
- ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ½Ñ‹Ñ… system prompts
- Ğ“Ğ°Ğ¹Ğ´ Ğ¿Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ context_chunks

### EMOJI_MAP

Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ² `formatters.py`:
```python
"chat": "ğŸ’¬",
"rag": "ğŸ¤–",
```

---

## ğŸ”® Ğ˜Ğ´ĞµĞ¸ Ğ½Ğ° Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞµ (Ğ½Ğµ Ğ² ÑĞºĞ¾ÑƒĞ¿Ğµ)

1. **Streaming responses:** ĞŸĞ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° (Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ°ĞµÑ‚ Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸)
2. **Conversation history:** Ğ¥Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° Ğ´Ğ»Ñ follow-up Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²
3. **Multi-turn RAG:** Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ ĞºĞ°Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚
4. **Export chat:** Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ğ² Markdown Ñ„Ğ°Ğ¹Ğ»
5. **Voice input:** Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ whisper Ğ´Ğ»Ñ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ²Ğ¾Ğ´Ğ°

---

## ğŸ”— Ğ¡Ğ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹

- **ĞŸÑ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ°Ñ:** [Phase 8.1 â€” Operations CLI](phase_8.1.md)
- **Gemini API:** [Phase 2 â€” Gemini Integration](../phase_2/report_phase_2.md)
- **Search:** [04_search_types.md](../../architecture/04_search_types.md)

````
