# üó∫Ô∏è Phase 6.2: Audio & Video Analysis (Extended)

**–¶–µ–ª—å:** –†–∞—Å—à–∏—Ä–∏—Ç—å –º–µ–¥–∏–∞-–∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏–∑ Phase 6.0 –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ.

**–ü—Ä–µ–¥—É—Å–ª–æ–≤–∏–µ:** Phase 6.0 (Images + Queue) –∏ Phase 6.1 (Tests) –∑–∞–≤–µ—Ä—à–µ–Ω—ã.

**–ü—Ä–∏–Ω—Ü–∏–ø:** –ü–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º –º–∞–∫—Å–∏–º—É–º –∏–∑ 6.0 ‚Äî —Ç–µ –∂–µ DTO, –æ—á–µ—Ä–µ–¥—å, rate limiter, resilience.

---

## üìê –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ 6.0)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ingest_media() ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  MediaTaskModel ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ QueueProcessor  ‚îÇ
‚îÇ  auto-detect    ‚îÇ     ‚îÇ   (SQLite)      ‚îÇ     ‚îÇ + RateLimiter   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                         ‚îÇ
                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                        ‚ñº                                ‚ñº                                ‚ñº
               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
               ‚îÇ ImageAnalyzer   ‚îÇ              ‚îÇ AudioAnalyzer   ‚îÇ              ‚îÇ VideoAnalyzer   ‚îÇ
               ‚îÇ gemini-2.5-flash‚îÇ              ‚îÇ gemini-2.5-flash‚îÇ              ‚îÇ gemini-2.5-pro  ‚îÇ
               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**–ù–æ–≤–æ–µ –≤ 6.2:**

- `GeminiAudioAnalyzer` ‚Äî —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è + –∞–Ω–∞–ª–∏–∑ –∞—É–¥–∏–æ
- `GeminiVideoAnalyzer` ‚Äî –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–∫–∞–¥—Ä—ã + –∞—É–¥–∏–æ)
- `AudioExtractor` ‚Äî –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—É–¥–∏–æ-–¥–æ—Ä–æ–∂–∫–∏ –∏–∑ –≤–∏–¥–µ–æ
- `FrameExtractor` ‚Äî –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤ –∏–∑ –≤–∏–¥–µ–æ
- `MediaRouter` ‚Äî –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –ø–æ —Ç–∏–ø—É –º–µ–¥–∏–∞

---

## üì¶ 1. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ DTO (`domain/media.py`)

**–î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—è –¥–ª—è audio/video:**

```python
@dataclass
class MediaAnalysisResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ (—Ä–∞—Å—à–∏—Ä–µ–Ω –¥–ª—è audio/video)."""
    
    # –û–±—â–∏–µ
    description: str
    alt_text: Optional[str] = None
    keywords: List[str] = field(default_factory=list)
    
    # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    ocr_text: Optional[str] = None
    
    # –ê—É–¥–∏–æ/–í–∏–¥–µ–æ (NEW)
    transcription: Optional[str] = None
    participants: List[str] = field(default_factory=list)
    action_items: List[str] = field(default_factory=list)
    duration_seconds: Optional[float] = None
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    tokens_used: Optional[int] = None


@dataclass
class VideoAnalysisConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –≤–∏–¥–µ–æ."""
    
    # –†–µ–∂–∏–º –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–∞–¥—Ä–æ–≤
    frame_mode: Literal["fps", "total", "interval"] = "total"
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–µ–∂–∏–º–æ–≤
    fps: float = 1.0              # –î–ª—è mode="fps"
    frame_count: int = 10         # –î–ª—è mode="total"
    interval_seconds: float = 5.0 # –î–ª—è mode="interval"
    
    # –ö–∞—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤
    frame_quality: Literal["hd", "fhd", "balanced"] = "hd"
    
    # –õ–∏–º–∏—Ç—ã
    max_frames: int = 50
    include_audio: bool = True
```

---

## üì¶ 2. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ Config (`domain/config.py`)

```python
@dataclass
class MediaConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –º–µ–¥–∏–∞."""
    
    # –ú–æ–¥–µ–ª–∏ Gemini
    image_model: str = "gemini-2.5-flash"
    audio_model: str = "gemini-2.5-flash"     # –û–±–Ω–æ–≤–ª–µ–Ω–æ
    video_model: str = "gemini-2.5-pro"       # –î–ª—è —Å–ª–æ–∂–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    
    # Rate Limiting (—Ä–∞–∑–Ω—ã–µ –ª–∏–º–∏—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤)
    image_rpm: int = 15
    audio_rpm: int = 10   # –ê—É–¥–∏–æ —Ç—è–∂–µ–ª–µ–µ
    video_rpm: int = 5    # –í–∏–¥–µ–æ —Å–∞–º–æ–µ —Ç—è–∂—ë–ª–æ–µ
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
    max_image_dimension: int = 1920
    max_audio_duration_sec: int = 600   # 10 –º–∏–Ω—É—Ç
    max_video_duration_sec: int = 300   # 5 –º–∏–Ω—É—Ç
    
    # –ê—É–¥–∏–æ
    audio_format: str = "ogg"
    audio_sample_rate: int = 16000
    audio_mono: bool = True
    
    # –í–∏–¥–µ–æ
    video_frame_mode: str = "total"
    video_frame_count: int = 10
```

---

## üõ†Ô∏è 3. –£—Ç–∏–ª–∏—Ç—ã –ê—É–¥–∏–æ (`infrastructure/media/utils/audio.py`)

**–î–æ–Ω–æ—Ä:** `doc/code_assets/audio_extractor.py`

```python
"""–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∞—É–¥–∏–æ."""

from pydub import AudioSegment
from pathlib import Path
from typing import Optional

SUPPORTED_AUDIO_TYPES = [
    "audio/mpeg", "audio/mp3", "audio/wav", 
    "audio/ogg", "audio/flac", "audio/aac"
]

def extract_audio_from_video(
    video_path: str,
    output_path: Optional[str] = None,
    format: str = "ogg",
    sample_rate: int = 16000,
    mono: bool = True,
) -> str:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞—É–¥–∏–æ-–¥–æ—Ä–æ–∂–∫—É –∏–∑ –≤–∏–¥–µ–æ.
    
    Args:
        video_path: –ü—É—Ç—å –∫ –≤–∏–¥–µ–æ
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (auto –µ—Å–ª–∏ None)
        format: –§–æ—Ä–º–∞—Ç –≤—ã—Ö–æ–¥–∞ (ogg, mp3, wav)
        sample_rate: –ß–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏
        mono: –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ –º–æ–Ω–æ
        
    Returns:
        –ü—É—Ç—å –∫ –∞—É–¥–∏–æ-—Ñ–∞–π–ª—É
    """
    video = AudioSegment.from_file(video_path)
    
    if mono:
        video = video.set_channels(1)
    
    video = video.set_frame_rate(sample_rate)
    
    if output_path is None:
        output_path = str(Path(video_path).with_suffix(f".{format}"))
    
    video.export(output_path, format=format)
    
    return output_path


def get_audio_duration(path: str) -> float:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞—É–¥–∏–æ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö."""
    audio = AudioSegment.from_file(path)
    return len(audio) / 1000.0


def is_audio_valid(path: str, max_duration: int = 600) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –∞—É–¥–∏–æ."""
    try:
        duration = get_audio_duration(path)
        return duration <= max_duration
    except Exception:
        return False
```

---

## üõ†Ô∏è 4. –£—Ç–∏–ª–∏—Ç—ã –í–∏–¥–µ–æ (`infrastructure/media/utils/video.py`)

**–î–æ–Ω–æ—Ä:** `doc/code_assets/media_frame_extractor.py`

```python
"""–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–∏–¥–µ–æ."""

import imageio.v3 as iio
from PIL import Image
from pathlib import Path
from typing import List, Literal

SUPPORTED_VIDEO_TYPES = [
    "video/mp4", "video/webm", "video/quicktime",
    "video/x-msvideo", "video/x-matroska"
]

QUALITY_PRESETS = {
    "fhd": 1920,   # 1080p
    "hd": 1280,    # 720p
    "balanced": 960,
}


def extract_frames(
    video_path: str,
    mode: Literal["fps", "total", "interval"] = "total",
    fps: float = 1.0,
    frame_count: int = 10,
    interval_seconds: float = 5.0,
    quality: str = "hd",
    max_frames: int = 50,
) -> List[Image.Image]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–∞–¥—Ä—ã –∏–∑ –≤–∏–¥–µ–æ.
    
    Modes:
        fps: –ò–∑–≤–ª–µ–∫–∞—Ç—å N –∫–∞–¥—Ä–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É
        total: –ò–∑–≤–ª–µ—á—å —Ä–æ–≤–Ω–æ N —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã—Ö –∫–∞–¥—Ä–æ–≤
        interval: –ò–∑–≤–ª–µ–∫–∞—Ç—å –∫–∞–¥—Ä –∫–∞–∂–¥—ã–µ N —Å–µ–∫—É–Ω–¥
    
    Returns:
        –°–ø–∏—Å–æ–∫ PIL.Image
    """
    # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    meta = iio.immeta(video_path)
    duration = meta.get("duration", 0)
    video_fps = meta.get("fps", 30)
    total_frames = int(duration * video_fps)
    
    # –í—ã—á–∏—Å–ª—è–µ–º –∏–Ω–¥–µ–∫—Å—ã –∫–∞–¥—Ä–æ–≤
    if mode == "fps":
        step = int(video_fps / fps)
        indices = list(range(0, total_frames, step))
    elif mode == "total":
        indices = [int(i * total_frames / frame_count) for i in range(frame_count)]
    elif mode == "interval":
        step = int(interval_seconds * video_fps)
        indices = list(range(0, total_frames, step))
    else:
        raise ValueError(f"Unknown mode: {mode}")
    
    # –õ–∏–º–∏—Ç–∏—Ä—É–µ–º
    indices = indices[:max_frames]
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º
    frames = []
    max_dim = QUALITY_PRESETS.get(quality, 1280)
    
    for idx in indices:
        frame = iio.imread(video_path, index=idx, plugin="pyav")
        img = Image.fromarray(frame)
        
        # –†–µ—Å–∞–π–∑
        img = _resize_frame(img, max_dim)
        frames.append(img)
    
    return frames


def get_video_duration(path: str) -> float:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤–∏–¥–µ–æ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö."""
    meta = iio.immeta(path)
    return meta.get("duration", 0)


def _resize_frame(img: Image.Image, max_dim: int) -> Image.Image:
    """–†–µ—Å–∞–π–∑ –∫–∞–¥—Ä–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø—Ä–æ–ø–æ—Ä—Ü–∏–π."""
    width, height = img.size
    if max(width, height) <= max_dim:
        return img
    
    ratio = max_dim / max(width, height)
    new_size = (int(width * ratio), int(height * ratio))
    return img.resize(new_size, Image.Resampling.LANCZOS)
```

---

## ‚ö° 5. Audio Analyzer (`infrastructure/gemini/audio_analyzer.py`)

```python
"""–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∞—É–¥–∏–æ —á–µ—Ä–µ–∑ Gemini."""

import base64
from pathlib import Path
from google import genai
from google.genai import types

from semantic_core.domain.media import MediaRequest, MediaAnalysisResult
from semantic_core.infrastructure.gemini.resilience import retry_with_backoff

AUDIO_SYSTEM_PROMPT = """You are an audio analyst for semantic search indexing.
Analyze the audio and provide:
1. Full transcription
2. Summary/description
3. Participants (if identifiable)
4. Keywords for search
5. Action items (if applicable)

Output JSON: {transcription, description, participants, keywords, action_items}"""


class GeminiAudioAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∞—É–¥–∏–æ —á–µ—Ä–µ–∑ Gemini."""
    
    def __init__(self, api_key: str, model: str = "gemini-2.5-flash"):
        self.api_key = api_key
        self.model = model
        self._client = None
    
    @property
    def client(self):
        if self._client is None:
            self._client = genai.Client(api_key=self.api_key)
        return self._client
    
    @retry_with_backoff(max_retries=5)
    def analyze(self, request: MediaRequest) -> MediaAnalysisResult:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ-—Ñ–∞–π–ª."""
        
        # 1. –ß–∏—Ç–∞–µ–º –∞—É–¥–∏–æ
        audio_path = request.resource.path
        audio_bytes = Path(audio_path).read_bytes()
        
        # 2. –°–æ–±–∏—Ä–∞–µ–º –ø—Ä–æ–º–ø—Ç
        prompt_parts = []
        if request.context_text:
            prompt_parts.append(f"Context: {request.context_text}")
        if request.user_prompt:
            prompt_parts.append(request.user_prompt)
        else:
            prompt_parts.append("Transcribe and analyze this audio.")
        
        prompt = "\n".join(prompt_parts)
        
        # 3. Inline audio (–¥–æ 20MB)
        audio_part = types.Part.from_bytes(
            data=audio_bytes,
            mime_type=request.resource.mime_type,
        )
        
        # 4. –í—ã–∑—ã–≤–∞–µ–º API
        response = self.client.models.generate_content(
            model=self.model,
            contents=[prompt, audio_part],
            config=types.GenerateContentConfig(
                system_instruction=AUDIO_SYSTEM_PROMPT,
                temperature=0.3,
                response_mime_type="application/json",
            ),
        )
        
        # 5. –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        import json
        data = json.loads(response.text)
        
        return MediaAnalysisResult(
            description=data.get("description", ""),
            transcription=data.get("transcription"),
            participants=data.get("participants", []),
            keywords=data.get("keywords", []),
            action_items=data.get("action_items", []),
        )
```

---

## ‚ö° 6. Video Analyzer (`infrastructure/gemini/video_analyzer.py`)

**–î–æ–Ω–æ—Ä:** `doc/code_assets/video_analyzer.py`

```python
"""–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ Gemini."""

import json
from pathlib import Path
from typing import List, Optional
from PIL import Image
from google import genai
from google.genai import types

from semantic_core.domain.media import (
    MediaRequest, MediaAnalysisResult, VideoAnalysisConfig
)
from semantic_core.infrastructure.gemini.resilience import retry_with_backoff
from semantic_core.infrastructure.media.utils.video import extract_frames
from semantic_core.infrastructure.media.utils.audio import extract_audio_from_video

VIDEO_SYSTEM_PROMPT = """You are a video analyst for semantic search indexing.
You receive video frames and optionally audio transcription.
Analyze the content and provide:
1. Description of what happens in the video
2. Key visual elements and text (OCR)
3. Audio summary (if provided)
4. Keywords for search
5. Participants (if identifiable)

Output JSON: {description, ocr_text, transcription, keywords, participants}"""


class GeminiVideoAnalyzer:
    """–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –≤–∏–¥–µ–æ."""
    
    # –õ–∏–º–∏—Ç inline –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    MAX_INLINE_SIZE = 20 * 1024 * 1024  # 20MB
    
    def __init__(
        self,
        api_key: str,
        model: str = "gemini-2.5-pro",
        audio_analyzer: Optional["GeminiAudioAnalyzer"] = None,
    ):
        self.api_key = api_key
        self.model = model
        self.audio_analyzer = audio_analyzer
        self._client = None
    
    @property
    def client(self):
        if self._client is None:
            self._client = genai.Client(api_key=self.api_key)
        return self._client
    
    @retry_with_backoff(max_retries=5)
    def analyze(
        self,
        request: MediaRequest,
        config: Optional[VideoAnalysisConfig] = None,
    ) -> MediaAnalysisResult:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–∏–¥–µ–æ (–∫–∞–¥—Ä—ã + –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –∞—É–¥–∏–æ)."""
        
        config = config or VideoAnalysisConfig()
        video_path = str(request.resource.path)
        
        # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞–¥—Ä—ã
        frames = extract_frames(
            video_path,
            mode=config.frame_mode,
            fps=config.fps,
            frame_count=config.frame_count,
            interval_seconds=config.interval_seconds,
            quality=config.frame_quality,
            max_frames=config.max_frames,
        )
        
        # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º –∞—É–¥–∏–æ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        audio_transcription = None
        if config.include_audio and self.audio_analyzer:
            try:
                audio_path = extract_audio_from_video(video_path)
                # –°–æ–∑–¥–∞—ë–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –∞—É–¥–∏–æ
                from semantic_core.domain.media import MediaResource, MediaType
                audio_resource = MediaResource(
                    path=Path(audio_path),
                    media_type=MediaType.AUDIO,
                    mime_type="audio/ogg",
                )
                audio_request = MediaRequest(resource=audio_resource)
                audio_result = self.audio_analyzer.analyze(audio_request)
                audio_transcription = audio_result.transcription
            except Exception as e:
                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –∞—É–¥–∏–æ
                pass
        
        # 3. –°–æ–±–∏—Ä–∞–µ–º –ø—Ä–æ–º–ø—Ç
        prompt_parts = []
        if request.context_text:
            prompt_parts.append(f"Context: {request.context_text}")
        if audio_transcription:
            prompt_parts.append(f"Audio transcription:\n{audio_transcription}")
        if request.user_prompt:
            prompt_parts.append(request.user_prompt)
        else:
            prompt_parts.append(f"Analyze these {len(frames)} frames from a video.")
        
        prompt = "\n".join(prompt_parts)
        
        # 4. –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç (prompt + frames)
        contents = [prompt] + frames
        
        # 5. –í—ã–∑—ã–≤–∞–µ–º API
        response = self.client.models.generate_content(
            model=self.model,
            contents=contents,
            config=types.GenerateContentConfig(
                system_instruction=VIDEO_SYSTEM_PROMPT,
                temperature=0.4,
                response_mime_type="application/json",
            ),
        )
        
        # 6. –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        data = json.loads(response.text)
        
        return MediaAnalysisResult(
            description=data.get("description", ""),
            ocr_text=data.get("ocr_text"),
            transcription=audio_transcription or data.get("transcription"),
            keywords=data.get("keywords", []),
            participants=data.get("participants", []),
        )
```

---

## üîÄ 7. Media Router (`infrastructure/gemini/media_router.py`)

```python
"""–ú–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –º–µ–¥–∏–∞."""

from typing import Protocol, Optional
from semantic_core.domain.media import (
    MediaType, MediaRequest, MediaAnalysisResult, VideoAnalysisConfig
)


class MediaAnalyzerProtocol(Protocol):
    """–ü—Ä–æ—Ç–æ–∫–æ–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤."""
    def analyze(self, request: MediaRequest) -> MediaAnalysisResult: ...


class MediaRouter:
    """–ú–∞—Ä—à—Ä—É—Ç–∏–∑–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å—ã –∫ –Ω—É–∂–Ω–æ–º—É –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä—É."""
    
    def __init__(
        self,
        image_analyzer: Optional[MediaAnalyzerProtocol] = None,
        audio_analyzer: Optional[MediaAnalyzerProtocol] = None,
        video_analyzer: Optional[MediaAnalyzerProtocol] = None,
    ):
        self._analyzers = {
            MediaType.IMAGE: image_analyzer,
            MediaType.AUDIO: audio_analyzer,
            MediaType.VIDEO: video_analyzer,
        }
    
    def analyze(
        self,
        request: MediaRequest,
        video_config: Optional[VideoAnalysisConfig] = None,
    ) -> MediaAnalysisResult:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–µ–¥–∏–∞ —á–µ—Ä–µ–∑ –ø–æ–¥—Ö–æ–¥—è—â–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä."""
        
        media_type = request.resource.media_type
        analyzer = self._analyzers.get(media_type)
        
        if analyzer is None:
            raise ValueError(f"No analyzer for {media_type}")
        
        # –í–∏–¥–µ–æ –º–æ–∂–µ—Ç –ø—Ä–∏–Ω–∏–º–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥
        if media_type == MediaType.VIDEO and video_config:
            return analyzer.analyze(request, config=video_config)
        
        return analyzer.analyze(request)
    
    def supports(self, media_type: MediaType) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –ª–∏ —Ç–∏–ø."""
        return self._analyzers.get(media_type) is not None
```

---

## üîÑ 8. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ Queue Processor

**–î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É audio/video:**

```python
class MediaQueueProcessor:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—á–µ—Ä–µ–¥–∏ –º–µ–¥–∏–∞-–∑–∞–¥–∞—á."""
    
    def __init__(
        self,
        db,
        router: MediaRouter,           # –í–º–µ—Å—Ç–æ –æ–¥–Ω–æ–≥–æ analyzer
        rate_limiters: dict,            # {MediaType: RateLimiter}
        pipeline,
    ):
        self.db = db
        self.router = router
        self.rate_limiters = rate_limiters
        self.pipeline = pipeline
    
    def process_one(self) -> bool:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É –∑–∞–¥–∞—á—É."""
        task = self._get_pending_task()
        if not task:
            return False
        
        media_type = MediaType(task.media_type)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º rate limiter –¥–ª—è —ç—Ç–æ–≥–æ —Ç–∏–ø–∞
        limiter = self.rate_limiters.get(media_type)
        if limiter:
            limiter.wait()
        
        self._update_status(task.id, "processing")
        
        try:
            request = self._to_request(task)
            result = self.router.analyze(request)
            
            chunk_id = self._create_chunk(task, result)
            self._save_result(task.id, result, chunk_id)
            return True
            
        except Exception as e:
            self._update_status(task.id, "failed", error=str(e))
            return True
```

---

## üîÑ 9. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ Pipeline

```python
class IngestionPipeline:
    
    def ingest_media(
        self,
        path: str,
        user_prompt: Optional[str] = None,
        context_text: Optional[str] = None,
        mode: Literal["sync", "async"] = "sync",
    ) -> Optional[str]:
        """
        –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ª—é–±–æ–≥–æ –º–µ–¥–∏–∞.
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø.
        
        Returns:
            sync: chunk_id
            async: task_id
        """
        media_type = get_media_type(path)
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ —Ç–∏–ø—É
        if media_type == MediaType.IMAGE:
            if not is_image_valid(path):
                raise ValueError(f"Invalid image: {path}")
        elif media_type == MediaType.AUDIO:
            if not is_audio_valid(path, self.media_config.max_audio_duration_sec):
                raise ValueError(f"Invalid audio: {path}")
        elif media_type == MediaType.VIDEO:
            duration = get_video_duration(path)
            if duration > self.media_config.max_video_duration_sec:
                raise ValueError(f"Video too long: {duration}s")
        
        # –°–æ–∑–¥–∞—ë–º –∑–∞–¥–∞—á—É
        task_id = self._create_media_task(path, user_prompt, context_text)
        
        if mode == "sync":
            self._ensure_queue_processor()
            success = self._media_queue.process_task(task_id)
            if not success:
                raise RuntimeError(f"Failed to process {path}")
            
            task = MediaTaskModel.get_by_id(task_id)
            return task.result_chunk_id
        
        return task_id
    
    # –ê–ª–∏–∞—Å—ã –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
    def ingest_image(self, path: str, **kwargs):
        return self.ingest_media(path, **kwargs)
    
    def ingest_audio(self, path: str, **kwargs):
        return self.ingest_media(path, **kwargs)
    
    def ingest_video(self, path: str, **kwargs):
        return self.ingest_media(path, **kwargs)
```

---

## üìÇ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª–æ–≤ Phase 6.2

```text
semantic_core/
‚îú‚îÄ‚îÄ domain/
‚îÇ   ‚îú‚îÄ‚îÄ config.py                      # UPDATE: audio/video settings
‚îÇ   ‚îî‚îÄ‚îÄ media.py                       # UPDATE: VideoAnalysisConfig, extended result
‚îú‚îÄ‚îÄ infrastructure/
‚îÇ   ‚îú‚îÄ‚îÄ gemini/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ image_analyzer.py          # (from 6.0)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ audio_analyzer.py          # NEW
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ video_analyzer.py          # NEW
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ media_router.py            # NEW
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ resilience.py              # (from 6.0)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rate_limiter.py            # (from 6.0)
‚îÇ   ‚îî‚îÄ‚îÄ media/
‚îÇ       ‚îî‚îÄ‚îÄ utils/
‚îÇ           ‚îú‚îÄ‚îÄ files.py               # UPDATE: audio/video MIME
‚îÇ           ‚îú‚îÄ‚îÄ tokens.py              # (from 6.0)
‚îÇ           ‚îú‚îÄ‚îÄ images.py              # (from 6.0)
‚îÇ           ‚îú‚îÄ‚îÄ audio.py               # NEW
‚îÇ           ‚îî‚îÄ‚îÄ video.py               # NEW
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îî‚îÄ‚îÄ media_queue.py                 # UPDATE: MediaRouter support
‚îî‚îÄ‚îÄ pipeline.py                        # UPDATE: ingest_media()
```

---

## üîó –ù–æ–≤—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏

```toml
[project.optional-dependencies]
media = [
    "Pillow>=10.0.0",       # Images
    "pydub>=0.25.0",        # Audio extraction
    "imageio[pyav]>=2.31",  # Video frames
]
```

---

## üß™ –¢–µ—Å—Ç—ã –¥–ª—è Phase 6.2

–î–æ–±–∞–≤–∏—Ç—å –≤ `tests/`:

```text
tests/
‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îî‚îÄ‚îÄ infrastructure/
‚îÇ       ‚îú‚îÄ‚îÄ media/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ test_audio_utils.py    # extract, duration
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ test_video_utils.py    # frames, duration
‚îÇ       ‚îî‚îÄ‚îÄ gemini/
‚îÇ           ‚îú‚îÄ‚îÄ test_audio_analyzer.py
‚îÇ           ‚îú‚îÄ‚îÄ test_video_analyzer.py
‚îÇ           ‚îî‚îÄ‚îÄ test_media_router.py
‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îî‚îÄ‚îÄ media/
‚îÇ       ‚îî‚îÄ‚îÄ test_pipeline_media.py     # all types
‚îî‚îÄ‚îÄ e2e/
    ‚îî‚îÄ‚îÄ gemini/
        ‚îú‚îÄ‚îÄ test_real_audio.py
        ‚îî‚îÄ‚îÄ test_real_video.py
```

---

## ‚úÖ Definition of Done (Phase 6.2)

1. **–ê—É–¥–∏–æ —Ä–∞–±–æ—Ç–∞–µ—Ç:**

   ```python
   chunk_id = pipeline.ingest_audio("podcast.mp3", mode="sync")
   results = store.search("machine learning")  # –ù–∞—Ö–æ–¥–∏—Ç –ø–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏!
   ```

2. **–í–∏–¥–µ–æ —Ä–∞–±–æ—Ç–∞–µ—Ç:**

   ```python
   chunk_id = pipeline.ingest_video("lecture.mp4", mode="sync")
   results = store.search("neural networks")  # –ù–∞—Ö–æ–¥–∏—Ç!
   ```

3. **–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥:**

   ```python
   # –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞
   pipeline.ingest_media("photo.jpg")   # ‚Üí image
   pipeline.ingest_media("song.mp3")    # ‚Üí audio
   pipeline.ingest_media("clip.mp4")    # ‚Üí video
   ```

4. **MediaRouter —Ä–∞–±–æ—Ç–∞–µ—Ç:** –ü—Ä–∞–≤–∏–ª—å–Ω–æ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∏—Ä—É–µ—Ç –ø–æ —Ç–∏–ø—É.

5. **Rate limiting —Ä–∞–∑–¥–µ–ª—å–Ω—ã–π:** Image 15 RPM, Audio 10 RPM, Video 5 RPM.

---

## üöÄ –ü–æ—Ä—è–¥–æ–∫ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

1. **–£—Ç–∏–ª–∏—Ç—ã:** `audio.py`, `video.py`
2. **–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä—ã:** `GeminiAudioAnalyzer`, `GeminiVideoAnalyzer`
3. **–†–æ—É—Ç–µ—Ä:** `MediaRouter`
4. **–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ Queue:** –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤
5. **–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ Pipeline:** `ingest_media()`
6. **–¢–µ—Å—Ç—ã:** unit ‚Üí integration ‚Üí E2E

---

## üìù –ü—Ä–∏–º–µ—á–∞–Ω–∏—è

- **Gemini 2.5 Pro –¥–ª—è –≤–∏–¥–µ–æ** ‚Äî –Ω—É–∂–µ–Ω –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
- **–ê—É–¥–∏–æ —á–µ—Ä–µ–∑ pydub** ‚Äî –∑–∞–≤–∏—Å–∏—Ç –æ—Ç ffmpeg (—É–∫–∞–∑–∞—Ç—å –≤ README)
- **–í–∏–¥–µ–æ —á–µ—Ä–µ–∑ imageio[pyav]** ‚Äî —á–∏—Å—Ç—ã–π Python, –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
- **–ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–¥—Ä–æ–≤** ‚Äî –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –≤ –±—É–¥—É—â–µ–º –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
