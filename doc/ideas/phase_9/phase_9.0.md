# Phase 9.0: Core RAG Engine

**–°—Ç–∞—Ç—É—Å:** üî≤ –ü–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è  
**–û—Ü–µ–Ω–∫–∞:** ~1.5 –¥–Ω—è

---

## üéØ –¶–µ–ª—å

–ë–∞–∑–æ–≤—ã–π RAG —á–∞—Ç –±–µ–∑ –∏—Å—Ç–æ—Ä–∏–∏ ‚Äî –æ–¥–∏–Ω –≤–æ–ø—Ä–æ—Å ‚Üí –ø–æ–∏—Å–∫ ‚Üí –æ—Ç–≤–µ—Ç.

---

## üì¶ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª–æ–≤

```
semantic_core/
‚îú‚îÄ‚îÄ interfaces/
‚îÇ   ‚îî‚îÄ‚îÄ llm.py                    # BaseLLMProvider, GenerationResult
‚îú‚îÄ‚îÄ infrastructure/
‚îÇ   ‚îî‚îÄ‚îÄ llm/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ gemini.py             # GeminiLLMProvider
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îî‚îÄ‚îÄ rag.py                    # RAGEngine
‚îî‚îÄ‚îÄ cli/commands/
    ‚îî‚îÄ‚îÄ chat.py                   # semantic chat (–±–∞–∑–æ–≤—ã–π REPL)
```

---

## üìê –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å BaseLLMProvider

```python
# semantic_core/interfaces/llm.py

from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Optional

@dataclass
class GenerationResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç LLM."""
    text: str
    model: str
    input_tokens: Optional[int] = None
    output_tokens: Optional[int] = None
    finish_reason: Optional[str] = None

class BaseLLMProvider(ABC):
    """–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ LLM."""
    
    @abstractmethod
    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
    ) -> GenerationResult:
        pass
    
    @property
    @abstractmethod
    def model_name(self) -> str:
        pass
```

---

## üìê GeminiLLMProvider

```python
# semantic_core/infrastructure/llm/gemini.py

class GeminiLLMProvider(BaseLLMProvider):
    def __init__(self, api_key: str, model: str = "gemini-2.0-flash"):
        self._client = genai.Client(api_key=api_key)
        self._model = model
    
    def generate(self, prompt, system_prompt=None, temperature=0.7, max_tokens=None):
        response = self._client.models.generate_content(
            model=self._model,
            contents=prompt,
            config=types.GenerateContentConfig(
                temperature=temperature,
                max_output_tokens=max_tokens,
                system_instruction=system_prompt,
            ),
        )
        return GenerationResult(
            text=response.text,
            model=self._model,
            input_tokens=response.usage_metadata.prompt_token_count,
            output_tokens=response.usage_metadata.candidates_token_count,
        )
```

---

## üìê RAGEngine

```python
# semantic_core/core/rag.py

@dataclass
class RAGResult:
    answer: str
    sources: list[SearchResult]
    generation: GenerationResult

class RAGEngine:
    DEFAULT_SYSTEM_PROMPT = """Answer based ONLY on the provided context.
If context doesn't have the answer, say so. Format in Markdown."""
    
    def __init__(self, core: SemanticCore, llm: BaseLLMProvider, context_chunks: int = 5):
        self.core = core
        self.llm = llm
        self.context_chunks = context_chunks
    
    def ask(self, query: str, search_mode: str = "hybrid") -> RAGResult:
        # 1. Retrieval
        sources = self.core.search(query, limit=self.context_chunks, mode=search_mode)
        # 2. Build context
        context = self._build_context(sources)
        # 3. Generate
        generation = self.llm.generate(
            prompt=f"CONTEXT:\n{context}\n\nQUESTION:\n{query}",
            system_prompt=self.DEFAULT_SYSTEM_PROMPT,
        )
        return RAGResult(answer=generation.text, sources=sources, generation=generation)
```

---

## üìê CLI: semantic chat

```python
# semantic_core/cli/commands/chat.py

@chat_cmd.callback(invoke_without_command=True)
def chat(
    model: str = Option("gemini-2.0-flash", "--model", "-m"),
    context_chunks: int = Option(5, "--context", "-c"),
    search_mode: str = Option("hybrid", "--search", "-s", help="vector/fts/hybrid"),
):
    """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π RAG —á–∞—Ç."""
    # ... –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ...
    
    while True:
        query = Prompt.ask("[bold blue]You[/]")
        if query in ("exit", "quit"): break
        
        result = rag.ask(query, search_mode=search_mode)
        console.print(Markdown(result.answer))
        _show_sources(result.sources)
```

---

## ‚úÖ Acceptance Criteria

- [ ] `BaseLLMProvider` –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
- [ ] `GeminiLLMProvider` —Å —Ç–æ–∫–µ–Ω–∞–º–∏ –≤ –æ—Ç–≤–µ—Ç–µ
- [ ] `RAGEngine.ask()` —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] `semantic chat` –∑–∞–ø—É—Å–∫–∞–µ—Ç REPL
- [ ] –ü–æ–¥–¥–µ—Ä–∂–∫–∞ `--search vector|fts|hybrid`
- [ ] –¢–µ—Å—Ç—ã: mock LLM, build_context

---

## ‚è±Ô∏è –û—Ü–µ–Ω–∫–∞

| –ó–∞–¥–∞—á–∞ | –ß–∞—Å—ã |
|--------|------|
| interfaces/llm.py | 0.5 |
| infrastructure/llm/gemini.py | 1.5 |
| core/rag.py | 2 |
| cli/commands/chat.py | 3 |
| –¢–µ—Å—Ç—ã | 2 |
| EMOJI_MAP | 0.5 |
| **–ò—Ç–æ–≥–æ** | **~10 —á–∞—Å–æ–≤** |
