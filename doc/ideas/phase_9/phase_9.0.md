# Phase 9.0: Core RAG Engine

**–°—Ç–∞—Ç—É—Å:** üî≤ –ü–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è  
**–û—Ü–µ–Ω–∫–∞:** ~1.5 –¥–Ω—è

---

## üéØ –¶–µ–ª—å

–ë–∞–∑–æ–≤—ã–π RAG —á–∞—Ç –±–µ–∑ –∏—Å—Ç–æ—Ä–∏–∏ ‚Äî –æ–¥–∏–Ω –≤–æ–ø—Ä–æ—Å ‚Üí –ø–æ–∏—Å–∫ ‚Üí –æ—Ç–≤–µ—Ç.

---

## üì¶ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª–æ–≤

```
semantic_core/
‚îú‚îÄ‚îÄ interfaces/
‚îÇ   ‚îî‚îÄ‚îÄ llm.py                    # BaseLLMProvider, GenerationResult
‚îú‚îÄ‚îÄ infrastructure/
‚îÇ   ‚îî‚îÄ‚îÄ llm/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ gemini.py             # GeminiLLMProvider
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îî‚îÄ‚îÄ rag.py                    # RAGEngine
‚îî‚îÄ‚îÄ cli/commands/
    ‚îî‚îÄ‚îÄ chat.py                   # semantic chat (–±–∞–∑–æ–≤—ã–π REPL)
```

---

## üìê –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å BaseLLMProvider

```python
# semantic_core/interfaces/llm.py

from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Optional

@dataclass
class GenerationResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç LLM."""
    text: str
    model: str
    input_tokens: Optional[int] = None
    output_tokens: Optional[int] = None
    finish_reason: Optional[str] = None

class BaseLLMProvider(ABC):
    """–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ LLM."""
    
    @abstractmethod
    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
    ) -> GenerationResult:
        pass
    
    @property
    @abstractmethod
    def model_name(self) -> str:
        pass
```

---

## üìê GeminiLLMProvider

```python
# semantic_core/infrastructure/llm/gemini.py

class GeminiLLMProvider(BaseLLMProvider):
    def __init__(self, api_key: str, model: str = "gemini-2.5-flash-lite"):
        self._client = genai.Client(api_key=api_key)
        self._model = model
    
    def generate(self, prompt, system_prompt=None, temperature=0.7, max_tokens=None):
        response = self._client.models.generate_content(
            model=self._model,
            contents=prompt,
            config=types.GenerateContentConfig(
                temperature=temperature,
                max_output_tokens=max_tokens,
                system_instruction=system_prompt,
            ),
        )
        return GenerationResult(
            text=response.text,
            model=self._model,
            input_tokens=response.usage_metadata.prompt_token_count,
            output_tokens=response.usage_metadata.candidates_token_count,
        )
```

---

## üìê RAGEngine

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

RAGEngine –∏—Å–ø–æ–ª—å–∑—É–µ—Ç **–≥—Ä–∞–Ω—É–ª—è—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ —á–∞–Ω–∫–∞–º** (`search_chunks()`) –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.
–≠—Ç–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –ø–æ —Ç–æ–∫–µ–Ω–∞–º –∏ –∫–∞—á–µ—Å—Ç–≤—É. –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –º–æ–∂–Ω–æ –∑–∞–ø—Ä–æ—Å–∏—Ç—å –ø–æ–ª–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  –†–µ–∂–∏–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (full_docs=False)                           ‚îÇ
‚îÇ  –ü–æ–∏—Å–∫ ‚Üí ChunkResult ‚Üí –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏      ‚îÇ
‚îÇ  5 —á–∞–Ω–∫–æ–≤ √ó ~500 —Å–∏–º–≤–æ–ª–æ–≤ = ~2.5k —Ç–æ–∫–µ–Ω–æ–≤                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  –†–µ–∂–∏–º full_docs=True (--full-docs –≤ CLI)                       ‚îÇ
‚îÇ  –ü–æ–∏—Å–∫ ‚Üí ChunkResult ‚Üí –ø–æ–¥–≥—Ä—É–∂–∞–µ–º parent –¥–æ–∫—É–º–µ–Ω—Ç—ã —Ü–µ–ª–∏–∫–æ–º      ‚îÇ
‚îÇ  –î–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ –∫–æ–≥–¥–∞ –Ω—É–∂–µ–Ω –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### –ö–æ–¥

```python
# semantic_core/core/rag.py

@dataclass
class RAGResult:
    answer: str
    sources: list[ChunkResult]  # –ì—Ä–∞–Ω—É–ª—è—Ä–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    generation: GenerationResult
    query: str = ""

class RAGEngine:
    DEFAULT_SYSTEM_PROMPT = """Answer based ONLY on the provided context.
If context doesn't have the answer, say so. Format in Markdown."""
    
    def __init__(self, core: SemanticCore, llm: BaseLLMProvider, context_chunks: int = 5):
        self.core = core
        self.llm = llm
        self.context_chunks = context_chunks
    
    def ask(
        self, 
        query: str, 
        search_mode: str = "hybrid",
        full_docs: bool = False,  # –ü–æ–¥–≥—Ä—É–∂–∞—Ç—å –ø–æ–ª–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã?
    ) -> RAGResult:
        # 1. Retrieval ‚Äî –≤—Å–µ–≥–¥–∞ –≥—Ä–∞–Ω—É–ª—è—Ä–Ω—ã–π –ø–æ–∏—Å–∫
        chunks = self.core.search_chunks(query, limit=self.context_chunks, mode=search_mode)
        
        # 2. Build context
        if full_docs:
            context = self._build_full_docs_context(chunks)
        else:
            context = self._build_chunks_context(chunks)  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        # 3. Generate
        generation = self.llm.generate(
            prompt=query,
            system_prompt=self._format_system_prompt(context),
        )
        return RAGResult(answer=generation.text, sources=chunks, generation=generation)
    
    def _build_chunks_context(self, chunks: list[ChunkResult]) -> str:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —á–∞–Ω–∫–æ–≤ (—ç–∫–æ–Ω–æ–º–Ω—ã–π —Ä–µ–∂–∏–º)."""
        parts = []
        for i, chunk in enumerate(chunks, 1):
            source = chunk.parent_doc_title or f"Source {i}"
            parts.append(f"[{i}] {source} (score: {chunk.score:.3f})\n{chunk.content}")
        return "\n\n---\n\n".join(parts)
    
    def _build_full_docs_context(self, chunks: list[ChunkResult]) -> str:
        """–ü–æ–¥–≥—Ä—É–∂–∞–µ—Ç –ø–æ–ª–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ parent_id."""
        seen_doc_ids = set()
        parts = []
        for chunk in chunks:
            if chunk.parent_doc_id in seen_doc_ids:
                continue
            seen_doc_ids.add(chunk.parent_doc_id)
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç —á–µ—Ä–µ–∑ store
            doc = self.core.store.get_document(chunk.parent_doc_id)
            if doc:
                source = doc.metadata.get("source", f"Document {chunk.parent_doc_id}")
                parts.append(f"[{source}]\n{doc.content}")
        return "\n\n---\n\n".join(parts)
```

---

## üìê CLI: semantic chat

```python
# semantic_core/cli/commands/chat.py

@chat_cmd.callback(invoke_without_command=True)
def chat(
    model: str = Option("gemini-2.5-flash-lite", "--model", "-m"),
    context_chunks: int = Option(5, "--context", "-c"),
    search_mode: str = Option("hybrid", "--search", "-s", help="vector/fts/hybrid"),
    full_docs: bool = Option(False, "--full-docs", "-f", help="–ü–æ–¥–≥—Ä—É–∂–∞—Ç—å –ø–æ–ª–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"),
):
    """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π RAG —á–∞—Ç."""
    # ... –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ...
    
    while True:
        query = Prompt.ask("[bold blue]You[/]")
        if query in ("exit", "quit"): break
        
        result = rag.ask(query, search_mode=search_mode, full_docs=full_docs)
        console.print(Markdown(result.answer))
        _show_sources(result.sources)
```

---

## ‚úÖ Acceptance Criteria

- [ ] `BaseLLMProvider` –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
- [ ] `GeminiLLMProvider` —Å —Ç–æ–∫–µ–Ω–∞–º–∏ –≤ –æ—Ç–≤–µ—Ç–µ
- [ ] `RAGEngine.ask()` —Å –≥—Ä–∞–Ω—É–ª—è—Ä–Ω—ã–º –ø–æ–∏—Å–∫–æ–º –ø–æ —á–∞–Ω–∫–∞–º
- [ ] `--full-docs` —Ñ–ª–∞–≥ –¥–ª—è –ø–æ–¥–≥—Ä—É–∑–∫–∏ –ø–æ–ª–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
- [ ] `semantic chat` –∑–∞–ø—É—Å–∫–∞–µ—Ç REPL
- [ ] –ü–æ–¥–¥–µ—Ä–∂–∫–∞ `--search vector|fts|hybrid`
- [ ] –¢–µ—Å—Ç—ã: mock LLM, build_chunks_context, build_full_docs_context

---

## ‚è±Ô∏è –û—Ü–µ–Ω–∫–∞

| –ó–∞–¥–∞—á–∞ | –ß–∞—Å—ã |
|--------|------|
| interfaces/llm.py | 0.5 |
| infrastructure/llm/gemini.py | 1.5 |
| core/rag.py | 2 |
| cli/commands/chat.py | 3 |
| –¢–µ—Å—Ç—ã | 2 |
| EMOJI_MAP | 0.5 |
| **–ò—Ç–æ–≥–æ** | **~10 —á–∞—Å–æ–≤** |
