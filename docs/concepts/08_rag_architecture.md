---
title: "RAG Architecture"
description: "Retrieval-Augmented Generation — поиск контекста и генерация ответов"
tags: ["rag", "llm", "search", "generation"]
difficulty: "advanced"
related: ["02_vector_search", "03_hybrid_rrf", "01_embeddings"]
---

# RAG Architecture 🤖

> **RAG = Retrieval + Augmentation + Generation**
> Находим релевантные документы, формируем контекст, генерируем ответ.

---

## Что такое RAG? 🎯

**Retrieval-Augmented Generation** — паттерн, при котором LLM отвечает на вопросы,
опираясь на найденные документы, а не на свои "знания".

```
┌──────────────────────────────────────────────────────────┐
│                      RAG Pipeline                        │
│                                                          │
│  Query ──▶ [Search] ──▶ Context ──▶ [LLM] ──▶ Answer    │
│              │                        │                  │
│              ▼                        ▼                  │
│         Sources []              GenerationResult         │
└──────────────────────────────────────────────────────────┘
```

---

## Три стадии RAG 📐

| Стадия | Действие | Компонент |
|--------|----------|-----------|
| **Retrieval** | Поиск релевантных чанков | `SemanticCore.search_chunks()` |
| **Augmentation** | Формирование контекста | System prompt + context |
| **Generation** | Генерация ответа | `BaseLLMProvider.generate()` |

---

## Режимы поиска 🔍

RAGEngine поддерживает три режима через параметр `search_mode`:

| Режим | Как ищет | Когда использовать |
|-------|----------|-------------------|
| `vector` | Семантически похожие | Размытые вопросы, понятийный поиск |
| `fts` | Точное совпадение (FTS5) | Код, термины, имена |
| `hybrid` | RRF-объединение | **По умолчанию** — баланс |

---

## Chunks vs Full Docs ⚖️

RAGEngine поддерживает два режима контекста:

```
┌────────────────────────────────────────┐
│         full_docs=False (default)      │
│                                        │
│  ┌──────┐ ┌──────┐ ┌──────┐           │
│  │Chunk1│ │Chunk2│ │Chunk3│  ◀─ Гранулярно
│  └──────┘ └──────┘ └──────┘           │
│                                        │
├────────────────────────────────────────┤
│            full_docs=True              │
│                                        │
│  ┌──────────────────────────┐         │
│  │      Весь документ       │  ◀─ Целиком
│  └──────────────────────────┘         │
└────────────────────────────────────────┘
```

| Режим | Плюсы | Минусы |
|-------|-------|--------|
| Chunks | Точный контекст, меньше токенов | Может потерять контекст |
| Full Docs | Полная картина | Больше токенов, шум |

---

## Интеграция с историей чата 💬

RAGEngine поддерживает `history: list[ChatMessage]`:

```
┌─────────────────────────────────────────────────┐
│                  LLM Request                     │
│                                                  │
│  ┌─────────────┐                                │
│  │System Prompt│ ◀─ DEFAULT_SYSTEM_PROMPT       │
│  │+ {context}  │    + найденные чанки           │
│  └─────────────┘                                │
│        ▼                                         │
│  ┌─────────────┐                                │
│  │  History[]  │ ◀─ Предыдущие сообщения        │
│  │ user/model  │                                │
│  └─────────────┘                                │
│        ▼                                         │
│  ┌─────────────┐                                │
│  │   Query     │ ◀─ Текущий вопрос              │
│  └─────────────┘                                │
└─────────────────────────────────────────────────┘
```

---

## Системный промпт 📝

RAGEngine использует DEFAULT_SYSTEM_PROMPT с правилами:

1. **Отвечать ТОЛЬКО на основе контекста**
2. Если информации нет → честно сказать "I don't have enough information"
3. Использовать Markdown форматирование
4. Цитировать источники при необходимости

Кастомный промпт передаётся через `system_prompt` в конструкторе.

---

## Результат: RAGResult 📦

```
┌─────────────────────────────────────────────┐
│                 RAGResult                    │
├─────────────────────────────────────────────┤
│ answer: str         # Сгенерированный ответ │
│ sources: []         # ChunkResult[] или     │
│                     # SearchResult[]         │
│ generation:         # GenerationResult       │
│   - text            # (дублирует answer)     │
│   - model           # gemini-2.5-flash       │
│   - input_tokens    # Токены промпта         │
│   - output_tokens   # Токены ответа          │
│ query: str          # Исходный вопрос        │
│ full_docs: bool     # Режим документов       │
├─────────────────────────────────────────────┤
│ @property                                    │
│ has_sources: bool   # Были ли источники     │
│ total_tokens: int   # Сумма токенов          │
└─────────────────────────────────────────────┘
```

---

## Формирование контекста 🔧

RAGEngine форматирует найденные источники в текст:

```
[1] doc_title [code] (python) (score: 0.892)
def hello():
    print("world")

---

[2] another_doc [text] (score: 0.845)
Описание функции hello...
```

Каждый блок содержит:
- Номер источника
- Заголовок документа (parent_doc_title)
- Тип чанка и язык
- Оценка релевантности
- Содержимое (до 2000 символов)

---

## Связанные концепты 🔗

| Концепт | Как связан |
|---------|------------|
| [Vector Search](02_vector_search.md) | Семантический поиск для Retrieval |
| [Hybrid RRF](03_hybrid_rrf.md) | Режим `hybrid` в RAG |
| [Embeddings](01_embeddings.md) | Векторизация запроса |
| [Chunking](04_chunking.md) | Гранулярность контекста |

---

## Итог 📌

| Аспект | Реализация |
|--------|------------|
| Класс | `RAGEngine` |
| Поиск | `SemanticCore.search_chunks()` |
| Генерация | `BaseLLMProvider.generate()` |
| Результат | `RAGResult` с sources и метриками |
| Режимы | vector / fts / hybrid |
| Контекст | Chunks (default) или Full Docs |
