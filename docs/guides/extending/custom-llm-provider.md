---
title: "Custom LLM Provider"
description: "–ö–∞–∫ –¥–æ–±–∞–≤–∏—Ç—å OpenAI, Anthropic, Ollama –∏ –¥—Ä—É–≥–∏–µ LLM"
tags: ["extending", "llm", "openai", "anthropic", "ollama"]
difficulty: "intermediate"
prerequisites: ["../../concepts/10_plugin_system"]
---

# Custom LLM Provider ü§ñ

> –î–æ–±–∞–≤—å—Ç–µ —Å–≤–æ–π LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä, —Ä–µ–∞–ª–∏–∑–æ–≤–∞–≤ –æ–¥–∏–Ω –º–µ—Ç–æ–¥.

---

## –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å BaseLLMProvider üìã

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           BaseLLMProvider (ABC)             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ @abstractmethod                             ‚îÇ
‚îÇ generate(                                   ‚îÇ
‚îÇ   prompt: str,                              ‚îÇ
‚îÇ   system_prompt: str | None,                ‚îÇ
‚îÇ   temperature: float = 0.7,                 ‚îÇ
‚îÇ   max_tokens: int | None,                   ‚îÇ
‚îÇ   history: list[dict] | None                ‚îÇ
‚îÇ ) -> GenerationResult                       ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ @property @abstractmethod                   ‚îÇ
‚îÇ model_name: str                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## GenerationResult DTO üì¶

| –ü–æ–ª–µ | –¢–∏–ø | –û–ø–∏—Å–∞–Ω–∏–µ |
|------|-----|----------|
| `text` | str | –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç |
| `model` | str | –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ |
| `input_tokens` | int? | –¢–æ–∫–µ–Ω—ã –ø—Ä–æ–º–ø—Ç–∞ |
| `output_tokens` | int? | –¢–æ–∫–µ–Ω—ã –æ—Ç–≤–µ—Ç–∞ |
| `finish_reason` | str? | –ü—Ä–∏—á–∏–Ω–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ |

---

## –ü—Ä–∏–º–µ—Ä: OpenAI üü¢

```python
from openai import OpenAI
from semantic_core.interfaces import BaseLLMProvider, GenerationResult

class OpenAIProvider(BaseLLMProvider):
    def __init__(self, api_key: str, model: str = "gpt-4o"):
        self.client = OpenAI(api_key=api_key)
        self._model = model
    
    @property
    def model_name(self) -> str:
        return self._model
    
    def generate(
        self,
        prompt: str,
        system_prompt: str | None = None,
        temperature: float = 0.7,
        max_tokens: int | None = None,
        history: list[dict] | None = None,
    ) -> GenerationResult:
        messages = []
        
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        if history:
            messages.extend(history)
        messages.append({"role": "user", "content": prompt})
        
        response = self.client.chat.completions.create(
            model=self._model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
        )
        
        choice = response.choices[0]
        return GenerationResult(
            text=choice.message.content,
            model=response.model,
            input_tokens=response.usage.prompt_tokens,
            output_tokens=response.usage.completion_tokens,
            finish_reason=choice.finish_reason,
        )
```

---

## –ü—Ä–∏–º–µ—Ä: Anthropic üü£

```python
from anthropic import Anthropic
from semantic_core.interfaces import BaseLLMProvider, GenerationResult

class AnthropicProvider(BaseLLMProvider):
    def __init__(self, api_key: str, model: str = "claude-3-5-sonnet-20241022"):
        self.client = Anthropic(api_key=api_key)
        self._model = model
    
    @property
    def model_name(self) -> str:
        return self._model
    
    def generate(self, prompt: str, **kwargs) -> GenerationResult:
        messages = []
        if kwargs.get("history"):
            messages.extend(kwargs["history"])
        messages.append({"role": "user", "content": prompt})
        
        response = self.client.messages.create(
            model=self._model,
            system=kwargs.get("system_prompt", ""),
            messages=messages,
            temperature=kwargs.get("temperature", 0.7),
            max_tokens=kwargs.get("max_tokens", 4096),
        )
        
        return GenerationResult(
            text=response.content[0].text,
            model=self._model,
            input_tokens=response.usage.input_tokens,
            output_tokens=response.usage.output_tokens,
        )
```

---

## –ü—Ä–∏–º–µ—Ä: Ollama (–ª–æ–∫–∞–ª—å–Ω—ã–π) üè†

```python
import ollama
from semantic_core.interfaces import BaseLLMProvider, GenerationResult

class OllamaProvider(BaseLLMProvider):
    def __init__(self, model: str = "llama3.3"):
        self._model = model
    
    @property
    def model_name(self) -> str:
        return self._model
    
    def generate(self, prompt: str, **kwargs) -> GenerationResult:
        messages = []
        if kwargs.get("system_prompt"):
            messages.append({"role": "system", "content": kwargs["system_prompt"]})
        if kwargs.get("history"):
            messages.extend(kwargs["history"])
        messages.append({"role": "user", "content": prompt})
        
        response = ollama.chat(
            model=self._model,
            messages=messages,
        )
        
        return GenerationResult(
            text=response["message"]["content"],
            model=self._model,
        )
```

---

## –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –≤ RAGEngine ‚öôÔ∏è

```python
from semantic_core import SemanticCore
from semantic_core.core.rag import RAGEngine

core = SemanticCore.from_config()
llm = OpenAIProvider(api_key="sk-...")

rag = RAGEngine(core=core, llm=llm)
result = rag.ask("–í–æ–ø—Ä–æ—Å")
```

---

## Mapping –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ üìä

| Semantic Core | OpenAI | Anthropic | Ollama |
|---------------|--------|-----------|--------|
| `prompt` | messages[-1] | messages[-1] | messages[-1] |
| `system_prompt` | messages[0] role=system | system | messages[0] |
| `temperature` | temperature | temperature | ‚Äî (–≤ options) |
| `max_tokens` | max_tokens | max_tokens | ‚Äî |
| `history` | messages[1:-1] | messages[:-1] | messages[1:-1] |

---

## –ß–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏ ‚ö†Ô∏è

| –û—à–∏–±–∫–∞ | –ü—Ä–∏—á–∏–Ω–∞ | –†–µ—à–µ–Ω–∏–µ |
|--------|---------|---------|
| –ü—É—Å—Ç–æ–π history | –ù–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω None | `if history: ...` |
| Token overflow | –ù–µ—Ç max_tokens | –ü–µ—Ä–µ–¥–∞–≤–∞–π—Ç–µ –ª–∏–º–∏—Ç |
| Timeout | –ú–µ–¥–ª–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å | –£–≤–µ–ª–∏—á—å—Ç–µ timeout |

---

## –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏ üîó

| –ì–∞–π–¥ | –ß—Ç–æ —É–∑–Ω–∞–µ—Ç–µ |
|------|-------------|
| [Custom Embedder](custom-embedder.md) | –°–≤–æ–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ |
| [Plugin System](../../concepts/10_plugin_system.md) | –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ |
