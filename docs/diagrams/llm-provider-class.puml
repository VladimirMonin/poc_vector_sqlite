@startuml
title LLM Provider Abstraction

interface BaseLLMProvider {
    +generate(prompt, system_prompt, temperature, max_tokens, history): GenerationResult
    +model_name: str
}

class GenerationResult {
    +text: str
    +model: str
    +input_tokens: int
    +output_tokens: int
    +finish_reason: str
    --
    +total_tokens: int
}

class GeminiLLMProvider {
    -client: genai.Client
    -model: str
    --
    +generate(...)
    +model_name: str
}

class "OpenAIProvider\n(potential)" as openai {
    -client: OpenAI
    -model: str
    --
    +generate(...)
    +model_name: str
}

class "OllamaProvider\n(potential)" as ollama {
    -base_url: str
    -model: str
    --
    +generate(...)
    +model_name: str
}

class "AnthropicProvider\n(potential)" as anthropic {
    -client: Anthropic
    -model: str
    --
    +generate(...)
    +model_name: str
}

BaseLLMProvider <|.. GeminiLLMProvider
BaseLLMProvider <|.. openai
BaseLLMProvider <|.. ollama
BaseLLMProvider <|.. anthropic

BaseLLMProvider --> GenerationResult : returns

note right of GeminiLLMProvider
    Built-in implementation
    Uses google-genai SDK
end note

note bottom of openai
    User can implement
    custom providers
end note

legend right
    |= Provider |= Models |
    | Gemini | 2.5-flash, 2.5-pro |
    | OpenAI | gpt-4o, o1 |
    | Ollama | llama3, mistral |
    | Anthropic | claude-3.5 |
endlegend
@enduml
