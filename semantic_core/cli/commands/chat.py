"""–ö–æ–º–∞–Ω–¥–∞ chat –¥–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ RAG-—á–∞—Ç–∞.

–ó–∞–ø—É—Å–∫–∞–µ—Ç REPL-—Ä–µ–∂–∏–º —Å Retrieval-Augmented Generation.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–Ω—ã–µ —Ä–µ–∂–∏–º—ã –ø–æ–∏—Å–∫–∞, –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ LLM –∏ slash-–∫–æ–º–∞–Ω–¥—ã.

Usage:
    semantic chat                     # –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫, gemini-2.5-flash-lite
    semantic chat --model gemini-1.5-pro  # –î—Ä—É–≥–∞—è –º–æ–¥–µ–ª—å
    semantic chat --search vector     # –¢–æ–ª—å–∫–æ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
    semantic chat --context 10        # –ë–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    semantic chat --history-limit 20  # –•—Ä–∞–Ω–∏—Ç—å 20 —Å–æ–æ–±—â–µ–Ω–∏–π
    semantic chat --token-budget 10000  # –õ–∏–º–∏—Ç –ø–æ —Ç–æ–∫–µ–Ω–∞–º

Slash-–∫–æ–º–∞–Ω–¥—ã –≤ —á–∞—Ç–µ:
    /help           –°–ø—Ä–∞–≤–∫–∞ –ø–æ –∫–æ–º–∞–Ω–¥–∞–º
    /search <query> –ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
    /sources        –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ—Ç–≤–µ—Ç–∞
    /model          –ü–æ–∫–∞–∑–∞—Ç—å/—Å–º–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å
    /tokens         –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤
    /quit           –í—ã—Ö–æ–¥
"""

from typing import Optional

import typer
from rich.console import Console
from rich.markdown import Markdown
from rich.panel import Panel
from rich.table import Table
from rich.prompt import Prompt
from rich.text import Text

from semantic_core.cli.console import console as default_console

chat_cmd = typer.Typer(
    name="chat",
    help="–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π RAG-—á–∞—Ç —Å –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π",
    no_args_is_help=False,
)


@chat_cmd.callback(invoke_without_command=True)
def chat(
    ctx: typer.Context,
    model: str = typer.Option(
        "gemini-2.5-flash-lite",
        "--model",
        "-m",
        help="–ú–æ–¥–µ–ª—å LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤",
    ),
    context_chunks: int = typer.Option(
        5,
        "--context",
        "-c",
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞",
        min=1,
        max=20,
    ),
    search_mode: str = typer.Option(
        "hybrid",
        "--search",
        "-s",
        help="–†–µ–∂–∏–º –ø–æ–∏—Å–∫–∞: vector, fts, hybrid",
    ),
    temperature: float = typer.Option(
        0.7,
        "--temperature",
        "-t",
        help="–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (0.0-2.0)",
        min=0.0,
        max=2.0,
    ),
    show_sources: bool = typer.Option(
        True,
        "--sources/--no-sources",
        help="–ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –æ—Ç–≤–µ—Ç–∞",
    ),
    max_tokens: Optional[int] = typer.Option(
        None,
        "--max-tokens",
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ",
    ),
    full_docs: bool = typer.Option(
        False,
        "--full-docs",
        help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–ª–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤–º–µ—Å—Ç–æ —á–∞–Ω–∫–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞",
    ),
    history_limit: int = typer.Option(
        10,
        "--history-limit",
        "-H",
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –∏—Å—Ç–æ—Ä–∏–∏",
        min=1,
        max=100,
    ),
    token_budget: Optional[int] = typer.Option(
        None,
        "--token-budget",
        help="–õ–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏ (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç --history-limit)",
    ),
    compress_at: Optional[int] = typer.Option(
        None,
        "--compress-at",
        help="–ü–æ—Ä–æ–≥ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–∂–∞—Ç–∏—è –∏—Å—Ç–æ—Ä–∏–∏ —á–µ—Ä–µ–∑ LLM",
    ),
    compress_target: int = typer.Option(
        10000,
        "--compress-target",
        help="–¶–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ—Å–ª–µ —Å–∂–∞—Ç–∏—è (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å --compress-at)",
    ),
    no_history: bool = typer.Option(
        False,
        "--no-history",
        help="–û—Ç–∫–ª—é—á–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é (–±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π)",
    ),
) -> None:
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π RAG-—á–∞—Ç.

    –ü—Ä–∏–º–µ—Ä—ã:
        semantic chat
        semantic chat --model gemini-1.5-pro --context 10
        semantic chat --search vector --no-sources
        semantic chat --full-docs  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–ª–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        semantic chat --history-limit 20  # –•—Ä–∞–Ω–∏—Ç—å 20 —Å–æ–æ–±—â–µ–Ω–∏–π
        semantic chat --token-budget 10000  # –õ–∏–º–∏—Ç –ø–æ —Ç–æ–∫–µ–Ω–∞–º
        semantic chat --compress-at 30000  # –°–∂–∏–º–∞—Ç—å –ø—Ä–∏ 30k —Ç–æ–∫–µ–Ω–æ–≤
        semantic chat --no-history  # –ë–µ–∑ –∏—Å—Ç–æ—Ä–∏–∏
    """
    from semantic_core.cli.app import get_cli_context

    cli_ctx = get_cli_context()
    console = default_console

    # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∂–∏–º–∞ –ø–æ–∏—Å–∫–∞
    valid_modes = ("vector", "fts", "hybrid")
    if search_mode not in valid_modes:
        raise typer.BadParameter(
            f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ä–µ–∂–∏–º –ø–æ–∏—Å–∫–∞: {search_mode}. "
            f"–î–æ–ø—É—Å—Ç–∏–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {', '.join(valid_modes)}"
        )

    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    try:
        core = cli_ctx.get_core()
        config = cli_ctx.get_config()
    except Exception as e:
        console.print(
            Panel(
                f"[red]–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}[/red]",
                title="‚ùå –û—à–∏–±–∫–∞",
            )
        )
        raise typer.Exit(1)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä
    try:
        from semantic_core.infrastructure.llm import GeminiLLMProvider

        api_key = config.require_api_key()
        llm = GeminiLLMProvider(api_key=api_key, model=model)
    except Exception as e:
        console.print(
            Panel(
                f"[red]–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ LLM: {e}[/red]",
                title="‚ùå –û—à–∏–±–∫–∞",
            )
        )
        raise typer.Exit(1)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º RAG Engine
    from semantic_core.core.rag import RAGEngine

    rag = RAGEngine(
        core=core,
        llm=llm,
        context_chunks=context_chunks,
    )

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä –∏—Å—Ç–æ—Ä–∏–∏
    from semantic_core.core.context import (
        ChatHistoryManager,
        LastNMessages,
        TokenBudget,
        Unlimited,
        AdaptiveWithCompression,
        ContextCompressor,
    )

    if no_history:
        # –ë–µ–∑ –∏—Å—Ç–æ—Ä–∏–∏ ‚Äî Unlimited, –Ω–æ –Ω–µ –±—É–¥–µ–º –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –≤ RAG
        history_manager = None
        history_label = "–æ—Ç–∫–ª—é—á–µ–Ω–∞"
    elif compress_at:
        # –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ —á–µ—Ä–µ–∑ LLM
        compressor = ContextCompressor(llm)
        strategy = AdaptiveWithCompression(
            compressor=compressor,
            threshold_tokens=compress_at,
            target_tokens=compress_target,
        )
        history_manager = ChatHistoryManager(strategy)
        history_label = f"—Å–∂–∞—Ç–∏–µ –ø—Ä–∏ {compress_at} —Ç–æ–∫–µ–Ω–æ–≤"
    elif token_budget:
        # –ü–æ —Ç–æ–∫–µ–Ω–∞–º
        history_manager = ChatHistoryManager(TokenBudget(max_tokens=token_budget))
        history_label = f"–¥–æ {token_budget} —Ç–æ–∫–µ–Ω–æ–≤"
    else:
        # –ü–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–æ–æ–±—â–µ–Ω–∏–π
        history_manager = ChatHistoryManager(LastNMessages(n=history_limit))
        history_label = f"–¥–æ {history_limit} —Å–æ–æ–±—â–µ–Ω–∏–π"

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º—É slash-–∫–æ–º–∞–Ω–¥
    from semantic_core.cli.chat.slash import (
        SlashCommandHandler,
        ChatContext,
        SlashAction,
        # Basic commands
        HelpCommand,
        ClearCommand,
        QuitCommand,
        TokensCommand,
        HistoryCommand,
        CompressCommand,
        # Search commands
        SearchCommand,
        SearchModeCommand,
        SourcesCommand,
        SourceCommand,
        # Settings commands
        ModelCommand,
        ContextCommand,
    )

    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–∞—Ç–∞
    chat_context = ChatContext(
        console=console,
        core=core,
        rag=rag,
        llm=llm,
        history_manager=history_manager,
        last_result=None,
        search_mode=search_mode,
        context_chunks=context_chunks,
        temperature=temperature,
    )
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤ extra_context
    chat_context.extra_context["_show_sources"] = str(show_sources)
    chat_context.extra_context["_full_docs"] = str(full_docs)
    chat_context.extra_context["_max_tokens"] = str(max_tokens) if max_tokens else ""
    chat_context.extra_context["_model"] = model

    # –°–æ–∑–¥–∞–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ slash-–∫–æ–º–∞–Ω–¥
    slash_handler = SlashCommandHandler()

    # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –∫–æ–º–∞–Ω–¥—ã (HelpCommand —Ç—Ä–µ–±—É–µ—Ç handler)
    slash_handler.register(HelpCommand(slash_handler))
    slash_handler.register(ClearCommand())
    slash_handler.register(QuitCommand())
    slash_handler.register(TokensCommand())
    slash_handler.register(HistoryCommand())
    slash_handler.register(CompressCommand())
    slash_handler.register(SearchCommand())
    slash_handler.register(SearchModeCommand())
    slash_handler.register(SourcesCommand())
    slash_handler.register(SourceCommand())
    slash_handler.register(ModelCommand())
    slash_handler.register(ContextCommand())

    # –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ
    _show_welcome(console, model, search_mode, context_chunks, full_docs, history_label)

    # REPL —Ü–∏–∫–ª
    while True:
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤–≤–æ–¥
            query = Prompt.ask("\n[bold blue]You[/bold blue]")

            # –ü—É—Å—Ç–æ–π –≤–≤–æ–¥
            if not query.strip():
                continue

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ slash-–∫–æ–º–∞–Ω–¥
            if query.startswith("/"):
                result = slash_handler.handle(query, chat_context)

                # –í—ã–≤–æ–¥–∏–º —Å–æ–æ–±—â–µ–Ω–∏–µ –µ—Å–ª–∏ –µ—Å—Ç—å
                if result.message:
                    console.print(result.message)

                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
                if result.action == SlashAction.EXIT:
                    console.print("[dim]–î–æ —Å–≤–∏–¥–∞–Ω–∏—è! üëã[/dim]")
                    break
                elif result.action == SlashAction.CLEAR:
                    console.clear()
                    current_model = chat_context.extra_context.get("_model", model)
                    current_full_docs = (
                        chat_context.extra_context.get("_full_docs", "False") == "True"
                    )
                    _show_welcome(
                        console,
                        current_model,
                        chat_context.search_mode,
                        chat_context.context_chunks,
                        current_full_docs,
                        history_label,
                    )
                    console.print("[green]‚úì –≠–∫—Ä–∞–Ω –æ—á–∏—â–µ–Ω[/green]")
                continue

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –∫–æ–º–∞–Ω–¥—ã –≤—ã—Ö–æ–¥–∞ (–±–µ–∑ —Å–ª–µ—à–∞)
            if query.lower() in ("exit", "quit", "q"):
                console.print("[dim]–î–æ —Å–≤–∏–¥–∞–Ω–∏—è! üëã[/dim]")
                break

            # –í—ã–ø–æ–ª–Ω—è–µ–º RAG –∑–∞–ø—Ä–æ—Å
            with console.status("[bold green]–î—É–º–∞—é...[/bold green]", spinner="dots"):
                try:
                    # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è RAG (–µ—Å–ª–∏ –µ—Å—Ç—å)
                    history = history_manager.get_history() if history_manager else None

                    # –ß–∏—Ç–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                    current_max_tokens_str = chat_context.extra_context.get(
                        "_max_tokens", ""
                    )
                    current_max_tokens = (
                        int(current_max_tokens_str)
                        if current_max_tokens_str
                        else max_tokens
                    )
                    current_full_docs = (
                        chat_context.extra_context.get("_full_docs", "False") == "True"
                    )

                    result = rag.ask(
                        query=query,
                        search_mode=chat_context.search_mode,
                        temperature=chat_context.temperature,
                        max_tokens=current_max_tokens,
                        full_docs=current_full_docs,
                        history=history,
                    )

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç
                    chat_context.last_result = result

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
                    if history_manager:
                        input_tokens = result.generation.input_tokens or 0
                        output_tokens = result.generation.output_tokens or 0
                        # –ü—Ä–∏–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤
                        history_manager.add_user(query, tokens=input_tokens // 2)
                        history_manager.add_assistant(
                            result.answer, tokens=output_tokens
                        )

                except Exception as e:
                    console.print(
                        Panel(
                            f"[red]–û—à–∏–±–∫–∞: {e}[/red]",
                            title="‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏",
                        )
                    )
                    continue

            # –í—ã–≤–æ–¥–∏–º –æ—Ç–≤–µ—Ç
            console.print()
            console.print("[bold green]Assistant[/bold green]")
            console.print(Markdown(result.answer))

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            current_show_sources = (
                chat_context.extra_context.get("_show_sources", "True") == "True"
            )
            current_full_docs = (
                chat_context.extra_context.get("_full_docs", "False") == "True"
            )
            if current_show_sources and result.has_sources:
                _show_sources(console, result.sources, current_full_docs)

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã
            if result.total_tokens:
                history_info = ""
                if history_manager:
                    msg_count = len(history_manager)
                    total_history_tokens = history_manager.total_tokens()
                    history_info = f" | –∏—Å—Ç–æ—Ä–∏—è: {msg_count} —Å–æ–æ–±—â., {total_history_tokens} —Ç–æ–∫–µ–Ω–æ–≤"

                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–∂–∞—Ç–∏–∏
                    if history_manager.has_summary:
                        history_info += " (—Å–∂–∞—Ç–æ)"

                console.print(
                    f"\n[dim]–¢–æ–∫–µ–Ω—ã: {result.total_tokens} "
                    f"(input: {result.generation.input_tokens}, "
                    f"output: {result.generation.output_tokens}){history_info}[/dim]"
                )

        except KeyboardInterrupt:
            console.print("\n[dim]–ü—Ä–µ—Ä–≤–∞–Ω–æ. –í–≤–µ–¥–∏—Ç–µ '/quit' –¥–ª—è –≤—ã—Ö–æ–¥–∞.[/dim]")
            continue

        except EOFError:
            console.print("\n[dim]–î–æ —Å–≤–∏–¥–∞–Ω–∏—è! üëã[/dim]")
            break


def _show_welcome(
    console: Console,
    model: str,
    search_mode: str,
    context_chunks: int,
    full_docs: bool = False,
    history_label: str = "–¥–æ 10 —Å–æ–æ–±—â–µ–Ω–∏–π",
) -> None:
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ."""
    mode_icons = {
        "vector": "üéØ –í–µ–∫—Ç–æ—Ä–Ω—ã–π",
        "fts": "üìù –ü–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤—ã–π",
        "hybrid": "üîÄ –ì–∏–±—Ä–∏–¥–Ω—ã–π",
    }
    mode_label = mode_icons.get(search_mode, search_mode)
    context_mode = "–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤" if full_docs else "—á–∞–Ω–∫–æ–≤"

    welcome_text = (
        f"[bold]ü§ñ Semantic Chat[/bold]\n\n"
        f"–ú–æ–¥–µ–ª—å: [cyan]{model}[/cyan]\n"
        f"–ü–æ–∏—Å–∫: [cyan]{mode_label}[/cyan]\n"
        f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: [cyan]{context_chunks} {context_mode}[/cyan]\n"
        f"–ò—Å—Ç–æ—Ä–∏—è: [cyan]{history_label}[/cyan]\n"
    )

    if full_docs:
        welcome_text += f"–†–µ–∂–∏–º: [yellow]–ø–æ–ª–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã[/yellow]\n"

    welcome_text += f"\n[dim]–í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å –∏–ª–∏ /help –¥–ª—è —Å–ø–∏—Å–∫–∞ –∫–æ–º–∞–Ω–¥.[/dim]"

    console.print(
        Panel(
            welcome_text,
            title="üí¨ RAG Chat",
            border_style="blue",
        )
    )


def _show_sources(console: Console, sources: list, full_docs: bool = False) -> None:
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –æ—Ç–≤–µ—Ç–∞."""
    console.print("\n[bold dim]üìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏:[/bold dim]")

    table = Table(show_header=False, box=None, padding=(0, 1))
    table.add_column("", width=3)
    table.add_column("", style="dim")
    table.add_column("", justify="right", style="dim")

    for i, source in enumerate(sources[:5], 1):
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—É—Ç—å –∏ score –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        if full_docs:
            # SearchResult ‚Äî –ø–æ–ª–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            source_path = source.document.metadata.get("source", "‚Äî")
        else:
            # ChunkResult ‚Äî —á–∞–Ω–∫–∏
            source_path = source.parent_doc_title or f"Doc#{source.parent_doc_id}"

        if len(source_path) > 50:
            source_path = "..." + source_path[-47:]

        score_text = f"{source.score:.2f}"
        table.add_row(f"[{i}]", source_path, score_text)

    console.print(table)


__all__ = ["chat_cmd"]
