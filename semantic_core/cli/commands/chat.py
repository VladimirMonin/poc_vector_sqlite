"""ÐšÐ¾Ð¼Ð°Ð½Ð´Ð° chat Ð´Ð»Ñ Ð¸Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾Ð³Ð¾ RAG-Ñ‡Ð°Ñ‚Ð°.

Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ REPL-Ñ€ÐµÐ¶Ð¸Ð¼ Ñ Retrieval-Augmented Generation.
ÐŸÐ¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚ Ñ€Ð°Ð·Ð½Ñ‹Ðµ Ñ€ÐµÐ¶Ð¸Ð¼Ñ‹ Ð¿Ð¾Ð¸ÑÐºÐ° Ð¸ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ LLM.

Usage:
    semantic chat                     # Ð“Ð¸Ð±Ñ€Ð¸Ð´Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº, gemini-2.0-flash
    semantic chat --model gemini-1.5-pro  # Ð”Ñ€ÑƒÐ³Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
    semantic chat --search vector     # Ð¢Ð¾Ð»ÑŒÐºÐ¾ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº
    semantic chat --context 10        # Ð‘Ð¾Ð»ÑŒÑˆÐµ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°
"""

from typing import Optional

import typer
from rich.console import Console
from rich.markdown import Markdown
from rich.panel import Panel
from rich.table import Table
from rich.prompt import Prompt
from rich.text import Text

from semantic_core.cli.console import console as default_console

chat_cmd = typer.Typer(
    name="chat",
    help="Ð˜Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹ RAG-Ñ‡Ð°Ñ‚ Ñ Ð±Ð°Ð·Ð¾Ð¹ Ð·Ð½Ð°Ð½Ð¸Ð¹",
    no_args_is_help=False,
)


@chat_cmd.callback(invoke_without_command=True)
def chat(
    ctx: typer.Context,
    model: str = typer.Option(
        "gemini-2.0-flash",
        "--model",
        "-m",
        help="ÐœÐ¾Ð´ÐµÐ»ÑŒ LLM Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð²",
    ),
    context_chunks: int = typer.Option(
        5,
        "--context",
        "-c",
        help="ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ‡Ð°Ð½ÐºÐ¾Ð² ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°",
        min=1,
        max=20,
    ),
    search_mode: str = typer.Option(
        "hybrid",
        "--search",
        "-s",
        help="Ð ÐµÐ¶Ð¸Ð¼ Ð¿Ð¾Ð¸ÑÐºÐ°: vector, fts, hybrid",
    ),
    temperature: float = typer.Option(
        0.7,
        "--temperature",
        "-t",
        help="Ð¢ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð° Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ (0.0-2.0)",
        min=0.0,
        max=2.0,
    ),
    show_sources: bool = typer.Option(
        True,
        "--sources/--no-sources",
        help="ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°Ñ‚ÑŒ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð°",
    ),
    max_tokens: Optional[int] = typer.Option(
        None,
        "--max-tokens",
        help="ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð² Ð¾Ñ‚Ð²ÐµÑ‚Ðµ",
    ),
    full_docs: bool = typer.Option(
        False,
        "--full-docs",
        help="Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ð¾Ð»Ð½Ñ‹Ðµ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ Ð²Ð¼ÐµÑÑ‚Ð¾ Ñ‡Ð°Ð½ÐºÐ¾Ð² Ð´Ð»Ñ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°",
    ),
) -> None:
    """Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ Ð¸Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹ RAG-Ñ‡Ð°Ñ‚.

    ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹:
        semantic chat
        semantic chat --model gemini-1.5-pro --context 10
        semantic chat --search vector --no-sources
        semantic chat --full-docs  # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ð¾Ð»Ð½Ñ‹Ðµ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹
    """
    from semantic_core.cli.app import get_cli_context

    cli_ctx = get_cli_context()
    console = default_console

    # Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ñ€ÐµÐ¶Ð¸Ð¼Ð° Ð¿Ð¾Ð¸ÑÐºÐ°
    valid_modes = ("vector", "fts", "hybrid")
    if search_mode not in valid_modes:
        raise typer.BadParameter(
            f"ÐÐµÐ²ÐµÑ€Ð½Ñ‹Ð¹ Ñ€ÐµÐ¶Ð¸Ð¼ Ð¿Ð¾Ð¸ÑÐºÐ°: {search_mode}. "
            f"Ð”Ð¾Ð¿ÑƒÑÑ‚Ð¸Ð¼Ñ‹Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ: {', '.join(valid_modes)}"
        )

    # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ñ‹
    try:
        core = cli_ctx.get_core()
        config = cli_ctx.get_config()
    except Exception as e:
        console.print(
            Panel(
                f"[red]ÐžÑˆÐ¸Ð±ÐºÐ° Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸: {e}[/red]",
                title="âŒ ÐžÑˆÐ¸Ð±ÐºÐ°",
            )
        )
        raise typer.Exit(1)

    # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ LLM Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€
    try:
        from semantic_core.infrastructure.llm import GeminiLLMProvider

        api_key = config.require_api_key()
        llm = GeminiLLMProvider(api_key=api_key, model=model)
    except Exception as e:
        console.print(
            Panel(
                f"[red]ÐžÑˆÐ¸Ð±ÐºÐ° Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ LLM: {e}[/red]",
                title="âŒ ÐžÑˆÐ¸Ð±ÐºÐ°",
            )
        )
        raise typer.Exit(1)

    # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ RAG Engine
    from semantic_core.core.rag import RAGEngine

    rag = RAGEngine(
        core=core,
        llm=llm,
        context_chunks=context_chunks,
    )

    # ÐŸÑ€Ð¸Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ðµ
    _show_welcome(console, model, search_mode, context_chunks, full_docs)

    # REPL Ñ†Ð¸ÐºÐ»
    while True:
        try:
            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð²Ð²Ð¾Ð´
            query = Prompt.ask("\n[bold blue]You[/bold blue]")

            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð° Ð²Ñ‹Ñ…Ð¾Ð´
            if query.lower() in ("exit", "quit", "/exit", "/quit", "q"):
                console.print("[dim]Ð”Ð¾ ÑÐ²Ð¸Ð´Ð°Ð½Ð¸Ñ! ðŸ‘‹[/dim]")
                break

            # ÐŸÑƒÑÑ‚Ð¾Ð¹ Ð²Ð²Ð¾Ð´
            if not query.strip():
                continue

            # Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ RAG Ð·Ð°Ð¿Ñ€Ð¾Ñ
            with console.status("[bold green]Ð”ÑƒÐ¼Ð°ÑŽ...[/bold green]", spinner="dots"):
                try:
                    result = rag.ask(
                        query=query,
                        search_mode=search_mode,  # type: ignore
                        temperature=temperature,
                        max_tokens=max_tokens,
                        full_docs=full_docs,
                    )
                except Exception as e:
                    console.print(
                        Panel(
                            f"[red]ÐžÑˆÐ¸Ð±ÐºÐ°: {e}[/red]",
                            title="âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸",
                        )
                    )
                    continue

            # Ð’Ñ‹Ð²Ð¾Ð´Ð¸Ð¼ Ð¾Ñ‚Ð²ÐµÑ‚
            console.print()
            console.print("[bold green]Assistant[/bold green]")
            console.print(Markdown(result.answer))

            # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸
            if show_sources and result.has_sources:
                _show_sources(console, result.sources, result.full_docs)

            # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ñ‚Ð¾ÐºÐµÐ½Ñ‹
            if result.total_tokens:
                console.print(
                    f"\n[dim]Ð¢Ð¾ÐºÐµÐ½Ñ‹: {result.total_tokens} "
                    f"(input: {result.generation.input_tokens}, "
                    f"output: {result.generation.output_tokens})[/dim]"
                )

        except KeyboardInterrupt:
            console.print("\n[dim]ÐŸÑ€ÐµÑ€Ð²Ð°Ð½Ð¾. Ð’Ð²ÐµÐ´Ð¸Ñ‚Ðµ 'exit' Ð´Ð»Ñ Ð²Ñ‹Ñ…Ð¾Ð´Ð°.[/dim]")
            continue

        except EOFError:
            console.print("\n[dim]Ð”Ð¾ ÑÐ²Ð¸Ð´Ð°Ð½Ð¸Ñ! ðŸ‘‹[/dim]")
            break


def _show_welcome(
    console: Console,
    model: str,
    search_mode: str,
    context_chunks: int,
    full_docs: bool = False,
) -> None:
    """ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ Ð¿Ñ€Ð¸Ð²ÐµÑ‚ÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ."""
    mode_icons = {
        "vector": "ðŸŽ¯ Ð’ÐµÐºÑ‚Ð¾Ñ€Ð½Ñ‹Ð¹",
        "fts": "ðŸ“ ÐŸÐ¾Ð»Ð½Ð¾Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ð¹",
        "hybrid": "ðŸ”€ Ð“Ð¸Ð±Ñ€Ð¸Ð´Ð½Ñ‹Ð¹",
    }
    mode_label = mode_icons.get(search_mode, search_mode)
    context_mode = "Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²" if full_docs else "Ñ‡Ð°Ð½ÐºÐ¾Ð²"

    welcome_text = (
        f"[bold]ðŸ¤– Semantic Chat[/bold]\n\n"
        f"ÐœÐ¾Ð´ÐµÐ»ÑŒ: [cyan]{model}[/cyan]\n"
        f"ÐŸÐ¾Ð¸ÑÐº: [cyan]{mode_label}[/cyan]\n"
        f"ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚: [cyan]{context_chunks} {context_mode}[/cyan]\n"
    )

    if full_docs:
        welcome_text += f"Ð ÐµÐ¶Ð¸Ð¼: [yellow]Ð¿Ð¾Ð»Ð½Ñ‹Ðµ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹[/yellow]\n"

    welcome_text += f"\n[dim]Ð’Ð²ÐµÐ´Ð¸Ñ‚Ðµ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¸Ð»Ð¸ 'exit' Ð´Ð»Ñ Ð²Ñ‹Ñ…Ð¾Ð´Ð°.[/dim]"

    console.print(
        Panel(
            welcome_text,
            title="ðŸ’¬ RAG Chat",
            border_style="blue",
        )
    )


def _show_sources(console: Console, sources: list, full_docs: bool = False) -> None:
    """ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð°."""
    console.print("\n[bold dim]ðŸ“š Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸:[/bold dim]")

    table = Table(show_header=False, box=None, padding=(0, 1))
    table.add_column("", width=3)
    table.add_column("", style="dim")
    table.add_column("", justify="right", style="dim")

    for i, source in enumerate(sources[:5], 1):
        # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð¿ÑƒÑ‚ÑŒ Ð¸ score Ð² Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸ Ð¾Ñ‚ Ñ‚Ð¸Ð¿Ð° Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ°
        if full_docs:
            # SearchResult â€” Ð¿Ð¾Ð»Ð½Ñ‹Ðµ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹
            source_path = source.document.metadata.get("source", "â€”")
        else:
            # ChunkResult â€” Ñ‡Ð°Ð½ÐºÐ¸
            source_path = source.parent_doc_title or f"Doc#{source.parent_doc_id}"

        if len(source_path) > 50:
            source_path = "..." + source_path[-47:]

        score_text = f"{source.score:.2f}"
        table.add_row(f"[{i}]", source_path, score_text)

    console.print(table)


__all__ = ["chat_cmd"]
