"""–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è pytest –¥–ª—è e2e audit —Ç–µ—Å—Ç–æ–≤.

–§–∏–∫—Å—Ç—É—Ä—ã:
    - audit_session: –°–æ–∑–¥–∞—ë—Ç –ø–∞–ø–∫—É –æ—Ç—á—ë—Ç–æ–≤ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –∑–∞–ø—É—Å–∫–∞
    - pipeline_inspector: –û–±—ë—Ä—Ç–∫–∞ –Ω–∞–¥ SemanticCore –¥–ª—è –ø–µ—Ä–µ—Ö–≤–∞—Ç–∞ –¥–∞–Ω–Ω—ã—Ö
    - test_assets_path: –ü—É—Ç—å –∫ —Ç–µ—Å—Ç–æ–≤—ã–º –∞—Å—Å–µ—Ç–∞–º (tests/asests/)
"""

import json
import os
import pytest
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass, field, asdict
from typing import Optional, Any
from unittest.mock import MagicMock
import numpy as np

from semantic_core import (
    SemanticCore,
    PeeweeVectorStore,
    init_peewee_database,
)
from semantic_core.domain import Document, Chunk, ChunkType
from semantic_core.domain.media import MediaAnalysisResult
from semantic_core.processing.parsers import MarkdownNodeParser
from semantic_core.processing.splitters import SmartSplitter
from semantic_core.processing.context import HierarchicalContextStrategy


# ============================================================================
# –ü—É—Ç–∏ ‚Äî –ò–°–ü–†–ê–í–õ–ï–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º tests/asests/
# ============================================================================


@pytest.fixture(scope="session")
def test_assets_path() -> Path:
    """–ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –∞—Å—Å–µ—Ç–∞–º–∏ (tests/asests/)."""
    return Path(__file__).parent.parent.parent / "asests"


@pytest.fixture(scope="session")
def audit_reports_root() -> Path:
    """–ö–æ—Ä–Ω–µ–≤–∞—è –ø–∞–ø–∫–∞ –¥–ª—è –æ—Ç—á—ë—Ç–æ–≤ (tests/audit_reports/)."""
    return Path(__file__).parent.parent.parent / "audit_reports"


@pytest.fixture(scope="session")
def audit_session(audit_reports_root: Path) -> Path:
    """–°–æ–∑–¥–∞—ë—Ç –ø–∞–ø–∫—É –æ—Ç—á—ë—Ç–æ–≤ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –∑–∞–ø—É—Å–∫–∞.
    
    –§–æ—Ä–º–∞—Ç: YYYY-MM-DD_HH-MM
    """
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M")
    session_path = audit_reports_root / timestamp
    session_path.mkdir(parents=True, exist_ok=True)
    return session_path


# ============================================================================
# Inspector: –ü–µ—Ä–µ—Ö–≤–∞—Ç –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
# ============================================================================


@dataclass
class ChunkInspection:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —á–∞–Ω–∫–µ –¥–ª—è –æ—Ç—á—ë—Ç–∞."""
    
    chunk_id: int
    chunk_type: ChunkType
    content: str
    headers: list[str]
    language: Optional[str]
    size: int
    context_text: str  # –¢–µ–∫—Å—Ç –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ (–ø–æ–ª–Ω—ã–π!)
    embedding_preview: Optional[list[float]] = None  # –ü–µ—Ä–≤—ã–µ 10 –∑–Ω–∞—á–µ–Ω–∏–π
    embedding_full: Optional[list[float]] = None  # –ü–æ–ª–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –¥–ª—è –æ—Ç—á—ë—Ç–∞


@dataclass
class MediaInspection:
    """–ü–æ–ª–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ–±—Ä–∞–±–æ—Ç–∫–µ –º–µ–¥–∏–∞."""
    
    asset_path: str
    asset_absolute_path: str
    media_type: str
    file_size_bytes: int
    
    # –ö–æ–Ω—Ç–µ–∫—Å—Ç
    surrounding_text_before: str
    surrounding_text_after: str
    
    # –ó–∞–ø—Ä–æ—Å –≤ –º–æ–¥–µ–ª—å
    system_prompt: str
    user_prompt: str
    model_name: str
    
    # –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ (–ø–æ–ª–Ω—ã–π!)
    response_raw: Optional[dict]  # –ü–æ–ª–Ω—ã–π JSON –æ—Ç–≤–µ—Ç
    response_parsed: Optional[MediaAnalysisResult]
    
    # –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    final_chunk_content: str
    processing_time_ms: float


@dataclass
class SearchInspection:
    """–ü–æ–ª–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–∏—Å–∫–æ–≤–æ–º –∑–∞–ø—Ä–æ—Å–µ."""
    
    query: str
    search_mode: str
    limit: int
    
    # –í–µ–∫—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–∞
    query_vector_full: list[float]  # –ü–æ–ª–Ω—ã–π –≤–µ–∫—Ç–æ—Ä!
    query_vector_preview: list[float]  # –ü–µ—Ä–≤—ã–µ 10
    
    # SQL –∑–∞–ø—Ä–æ—Å—ã (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
    sql_query: Optional[str]
    sql_params: Optional[list]
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results: list[dict]  # –ü–æ–ª–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–∞–∂–¥–æ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ
    results_count: int
    search_time_ms: float


@dataclass
class InspectionReport:
    """–ü–æ–ª–Ω—ã–π –æ—Ç—á—ë—Ç –∏–Ω—Å–ø–µ–∫—Ü–∏–∏."""
    
    file_path: str
    file_content_preview: str  # –ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    chunks: list[ChunkInspection] = field(default_factory=list)
    media: list[MediaInspection] = field(default_factory=list)
    searches: list[SearchInspection] = field(default_factory=list)


class PipelineInspector:
    """–û–±—ë—Ä—Ç–∫–∞ –Ω–∞–¥ SemanticCore –¥–ª—è –ø–µ—Ä–µ—Ö–≤–∞—Ç–∞ –í–°–ï–• –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
    
    –ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç:
    - –í—Å–µ —á–∞–Ω–∫–∏ —Å –ø–æ–ª–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
    - –í—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (–ø–æ–ª–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã)
    - –í—Å–µ –∑–∞–ø—Ä–æ—Å—ã –∫ LLM
    - –í—Å–µ –æ—Ç–≤–µ—Ç—ã –æ—Ç LLM
    - –í—Å–µ SQL –∑–∞–ø—Ä–æ—Å—ã
    """
    
    def __init__(
        self,
        core: SemanticCore,
        session_path: Path,
    ):
        self.core = core
        self.session_path = session_path
        self.reports: list[InspectionReport] = []
        self._current_report: Optional[InspectionReport] = None
    
    def ingest_with_inspection(
        self,
        document: Document,
        mode: str = "sync",
        enrich_media: bool = False,
    ) -> Document:
        """–ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç —Å –∑–∞–ø–∏—Å—å—é –í–°–ï–• –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
        source = document.metadata.get("source", "unknown")
        content_preview = document.content[:500] if document.content else ""
        
        self._current_report = InspectionReport(
            file_path=source,
            file_content_preview=content_preview,
        )
        
        # 1. –°–ø–ª–∏—Ç—Ç–∏–Ω–≥ (–ø–µ—Ä–µ—Ö–≤–∞—Ç—ã–≤–∞–µ–º —á–∞–Ω–∫–∏)
        chunks = self.core.splitter.split(document)
        
        # 2. –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –∏–Ω—Å–ø–µ–∫—Ü–∏—é
        vector_texts = []
        for i, chunk in enumerate(chunks):
            context_text = self.core.context_strategy.form_vector_text(chunk, document)
            vector_texts.append(context_text)
            
            # headers —Ö—Ä–∞–Ω—è—Ç—Å—è –≤ metadata
            headers = chunk.metadata.get("headers", [])
            
            inspection = ChunkInspection(
                chunk_id=i + 1,
                chunk_type=chunk.chunk_type,
                content=chunk.content,
                headers=headers,
                language=chunk.language,
                size=len(chunk.content),
                context_text=context_text,  # –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç!
            )
            self._current_report.chunks.append(inspection)
        
        # 3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        if mode == "sync":
            embeddings = self.core.embedder.embed_documents(vector_texts)
            for chunk, embedding, inspection in zip(
                chunks, embeddings, self._current_report.chunks
            ):
                chunk.embedding = embedding
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ü–û–õ–ù–´–ô –≤–µ–∫—Ç–æ—Ä –∏ preview
                if hasattr(embedding, 'tolist'):
                    full_vec = embedding.tolist()
                else:
                    full_vec = list(embedding)
                
                inspection.embedding_full = full_vec
                inspection.embedding_preview = full_vec[:10]
        
        # 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
        saved_document = self.core.store.save(document, chunks)
        
        self.reports.append(self._current_report)
        self._current_report = None
        
        return saved_document
    
    def search_with_inspection(
        self,
        query: str,
        limit: int = 10,
        mode: str = "hybrid",
    ) -> list:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ —Å –∑–∞–ø–∏—Å—å—é –í–°–ï–• –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
        import time
        start_time = time.perf_counter()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–µ–∫—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–∞
        query_vector = self.core.embedder.embed_query(query)
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        results = self.core.search(query=query, limit=limit, mode=mode)
        
        search_time_ms = (time.perf_counter() - start_time) * 1000
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
        if hasattr(query_vector, 'tolist'):
            full_vec = query_vector.tolist()
        else:
            full_vec = list(query_vector)
        
        inspection = SearchInspection(
            query=query,
            search_mode=mode,
            limit=limit,
            query_vector_full=full_vec,
            query_vector_preview=full_vec[:10],
            sql_query=None,  # TODO: –ø–µ—Ä–µ—Ö–≤–∞—Ç—ã–≤–∞—Ç—å SQL
            sql_params=None,
            results=[
                {
                    "rank": i + 1,
                    "score": r.score,
                    "match_type": r.match_type.value if r.match_type else "unknown",
                    "document_id": r.document.id if r.document else None,
                    "content_full": r.document.content if r.document else "",
                    "content_preview": r.document.content[:200] if r.document and r.document.content else "",
                    "metadata": r.document.metadata if r.document else {},
                    "chunk_id": r.chunk_id,
                }
                for i, r in enumerate(results)
            ],
            results_count=len(results),
            search_time_ms=search_time_ms,
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É –æ—Ç—á—ë—Ç—É –∏–ª–∏ —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π
        if self.reports:
            self.reports[-1].searches.append(inspection)
        else:
            report = InspectionReport(
                file_path="search_only",
                file_content_preview="",
            )
            report.searches.append(inspection)
            self.reports.append(report)
        
        return results
    
    def add_media_inspection(self, inspection: MediaInspection):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –∏–Ω—Å–ø–µ–∫—Ü–∏—é –º–µ–¥–∏–∞-–æ–±—Ä–∞–±–æ—Ç–∫–∏."""
        if self._current_report:
            self._current_report.media.append(inspection)
        elif self.reports:
            self.reports[-1].media.append(inspection)
    
    def generate_chunking_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ü–û–õ–ù–´–ô Markdown-–æ—Ç—á—ë—Ç –æ —á–∞–Ω–∫–∏–Ω–≥–µ."""
        lines = [
            "# üìã Chunking Audit Report",
            "",
            f"*Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*",
            "",
            "## Summary",
            "",
            f"- **Total Files:** {len(self.reports)}",
            f"- **Total Chunks:** {sum(len(r.chunks) for r in self.reports)}",
            "",
            "---",
            "",
        ]
        
        for report in self.reports:
            lines.append(f"# üìÑ File: `{report.file_path}`")
            lines.append("")
            lines.append("## Source Content Preview")
            lines.append("```")
            lines.append(report.file_content_preview)
            lines.append("```")
            lines.append("")
            lines.append(f"**Total Chunks:** {len(report.chunks)}")
            lines.append("")
            
            for chunk in report.chunks:
                type_emoji = {
                    ChunkType.TEXT: "üìù",
                    ChunkType.CODE: "üíª",
                    ChunkType.TABLE: "üìä",
                    ChunkType.IMAGE_REF: "üñºÔ∏è",
                    ChunkType.AUDIO_REF: "üéµ",
                    ChunkType.VIDEO_REF: "üé¨",
                }.get(chunk.chunk_type, "üìÑ")
                
                lines.append("---")
                lines.append(
                    f"## Chunk #{chunk.chunk_id} [{chunk.chunk_type.value.upper()}] {type_emoji}"
                )
                lines.append("")
                
                if chunk.headers:
                    breadcrumbs = " > ".join(chunk.headers)
                    lines.append(f"**Headers Breadcrumbs:** `{breadcrumbs}`")
                
                if chunk.language:
                    lines.append(f"**Language:** `{chunk.language}`")
                
                lines.append(f"**Size:** {chunk.size} chars")
                lines.append("")
                
                # –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
                lines.append("### Content (Full)")
                if chunk.chunk_type == ChunkType.CODE:
                    lang = chunk.language or ""
                    lines.append(f"```{lang}")
                    lines.append(chunk.content)
                    lines.append("```")
                else:
                    lines.append("```")
                    lines.append(chunk.content)
                    lines.append("```")
                lines.append("")
                
                # –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
                lines.append("### Vector Context (Full Text Sent to Embedder)")
                lines.append("```")
                lines.append(chunk.context_text)
                lines.append("```")
                lines.append("")
                
                # –≠–º–±–µ–¥–¥–∏–Ω–≥
                if chunk.embedding_preview:
                    preview = ", ".join(f"{v:.6f}" for v in chunk.embedding_preview)
                    lines.append(f"**Embedding Preview (first 10):** `[{preview}]`")
                    lines.append(f"**Embedding Dimension:** {len(chunk.embedding_full) if chunk.embedding_full else 'N/A'}")
                
                lines.append("")
        
        return "\n".join(lines)
    
    def generate_search_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ü–û–õ–ù–´–ô Markdown-–æ—Ç—á—ë—Ç –æ –ø–æ–∏—Å–∫–µ."""
        lines = [
            "# üîç Search Audit Report",
            "",
            f"*Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*",
            "",
            "---",
            "",
        ]
        
        all_searches = []
        for report in self.reports:
            all_searches.extend(report.searches)
        
        lines.append(f"**Total Searches:** {len(all_searches)}")
        lines.append("")
        
        for idx, search in enumerate(all_searches, 1):
            lines.append(f"# Search #{idx}")
            lines.append("")
            lines.append(f"**Query:** `{search.query}`")
            lines.append(f"**Mode:** `{search.search_mode}`")
            lines.append(f"**Limit:** {search.limit}")
            lines.append(f"**Time:** {search.search_time_ms:.2f} ms")
            lines.append(f"**Results Found:** {search.results_count}")
            lines.append("")
            
            # –í–µ–∫—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–∞ (–ø–æ–ª–Ω—ã–π!)
            lines.append("## Query Vector")
            lines.append("")
            preview = ", ".join(f"{v:.6f}" for v in search.query_vector_preview)
            lines.append(f"**Preview (first 10):** `[{preview}]`")
            lines.append(f"**Dimension:** {len(search.query_vector_full)}")
            lines.append("")
            
            # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
            lines.append("## Results")
            lines.append("")
            
            if search.results:
                for r in search.results:
                    lines.append(f"### Result #{r['rank']}")
                    lines.append("")
                    lines.append(f"- **Score:** {r['score']:.6f}")
                    lines.append(f"- **Match Type:** {r['match_type']}")
                    lines.append(f"- **Document ID:** {r['document_id']}")
                    lines.append(f"- **Chunk ID:** {r['chunk_id']}")
                    lines.append(f"- **Metadata:** `{json.dumps(r['metadata'], ensure_ascii=False)}`")
                    lines.append("")
                    lines.append("**Content Preview:**")
                    lines.append("```")
                    lines.append(r['content_preview'])
                    lines.append("```")
                    lines.append("")
            else:
                lines.append("*No results found*")
            
            lines.append("---")
            lines.append("")
        
        return "\n".join(lines)
    
    def generate_media_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ü–û–õ–ù–´–ô Markdown-–æ—Ç—á—ë—Ç –æ –º–µ–¥–∏–∞-–æ–±—Ä–∞–±–æ—Ç–∫–µ."""
        lines = [
            "# üé¨ Media Processing Audit Report",
            "",
            f"*Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*",
            "",
            "---",
            "",
        ]
        
        all_media = []
        for report in self.reports:
            all_media.extend(report.media)
        
        if not all_media:
            lines.append("*No media files processed*")
            return "\n".join(lines)
        
        lines.append(f"**Total Media Files:** {len(all_media)}")
        lines.append("")
        
        for idx, media in enumerate(all_media, 1):
            emoji = {"image": "üñºÔ∏è", "audio": "üéµ", "video": "üé¨"}.get(media.media_type, "üìÅ")
            
            lines.append(f"# {emoji} Media #{idx}: `{media.asset_path}`")
            lines.append("")
            lines.append(f"**Type:** {media.media_type.upper()}")
            lines.append(f"**Absolute Path:** `{media.asset_absolute_path}`")
            lines.append(f"**File Size:** {media.file_size_bytes:,} bytes")
            lines.append(f"**Processing Time:** {media.processing_time_ms:.2f} ms")
            lines.append("")
            
            # –ö–æ–Ω—Ç–µ–∫—Å—Ç
            lines.append("## 1. Surrounding Context")
            lines.append("")
            lines.append("**Text Before:**")
            lines.append("```")
            lines.append(media.surrounding_text_before or "(none)")
            lines.append("```")
            lines.append("")
            lines.append("**Text After:**")
            lines.append("```")
            lines.append(media.surrounding_text_after or "(none)")
            lines.append("```")
            lines.append("")
            
            # –ó–∞–ø—Ä–æ—Å –≤ –º–æ–¥–µ–ª—å
            lines.append("## 2. LLM Request")
            lines.append("")
            lines.append(f"**Model:** `{media.model_name}`")
            lines.append("")
            lines.append("**System Prompt:**")
            lines.append("```")
            lines.append(media.system_prompt or "(none)")
            lines.append("```")
            lines.append("")
            lines.append("**User Prompt:**")
            lines.append("```")
            lines.append(media.user_prompt or "(none)")
            lines.append("```")
            lines.append("")
            
            # –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
            lines.append("## 3. LLM Response (Raw)")
            lines.append("")
            if media.response_raw:
                lines.append("```json")
                lines.append(json.dumps(media.response_raw, ensure_ascii=False, indent=2))
                lines.append("```")
            else:
                lines.append("*(No raw response available)*")
            lines.append("")
            
            # –†–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
            lines.append("## 4. Parsed Response")
            lines.append("")
            if media.response_parsed:
                r = media.response_parsed
                lines.append(f"- **Description:** {r.description}")
                if r.alt_text:
                    lines.append(f"- **Alt Text:** {r.alt_text}")
                if r.keywords:
                    lines.append(f"- **Keywords:** {r.keywords}")
                if r.ocr_text:
                    lines.append(f"- **OCR Text:** {r.ocr_text}")
                if r.transcription:
                    lines.append(f"- **Transcription:** {r.transcription}")
                if r.participants:
                    lines.append(f"- **Participants:** {r.participants}")
                if r.duration_seconds:
                    lines.append(f"- **Duration:** {r.duration_seconds} sec")
            else:
                lines.append("*(No parsed response)*")
            lines.append("")
            
            # –§–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
            lines.append("## 5. Final Chunk Content")
            lines.append("")
            lines.append("```")
            lines.append(media.final_chunk_content)
            lines.append("```")
            lines.append("")
            lines.append("---")
            lines.append("")
        
        return "\n".join(lines)
    
    def save_reports(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –í–°–ï –æ—Ç—á—ë—Ç—ã –≤ —Ñ–∞–π–ª—ã."""
        # Chunking report
        chunking_path = self.session_path / "01_chunking_audit.md"
        chunking_path.write_text(self.generate_chunking_report(), encoding="utf-8")
        
        # Media report
        media_path = self.session_path / "02_media_audit.md"
        media_path.write_text(self.generate_media_report(), encoding="utf-8")
        
        # Search report
        search_path = self.session_path / "03_search_audit.md"
        search_path.write_text(self.generate_search_report(), encoding="utf-8")


# ============================================================================
# –§–∏–∫—Å—Ç—É—Ä—ã –¥–ª—è —Ç–µ—Å—Ç–æ–≤
# ============================================================================


@pytest.fixture
def mock_embedder():
    """Mock embedder –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤."""
    import hashlib
    
    class MockEmbedder:
        def __init__(self, dim: int = 768):
            self.dim = dim
        
        def embed_query(self, text: str) -> np.ndarray:
            # –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ö–µ—à–∞
            hash_val = int(hashlib.md5(text.encode()).hexdigest(), 16)
            vector = np.array([
                ((hash_val + i) % 1000) / 1000.0 - 0.5
                for i in range(self.dim)
            ], dtype=np.float32)
            return vector
        
        def embed_documents(self, texts: list[str]) -> list[np.ndarray]:
            return [self.embed_query(text) for text in texts]
    
    return MockEmbedder()


@pytest.fixture
def audit_db(tmp_path):
    """–ò–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ë–î –¥–ª—è –∞—É–¥–∏—Ç–∞."""
    db_path = tmp_path / "audit.db"
    db = init_peewee_database(str(db_path))
    yield db
    db.close()


@pytest.fixture
def pipeline_inspector(
    audit_db,
    mock_embedder,
    audit_session: Path,
) -> PipelineInspector:
    """–°–æ–∑–¥–∞—ë—Ç PipelineInspector —Å mock –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏."""
    parser = MarkdownNodeParser()
    splitter = SmartSplitter(parser=parser, chunk_size=500, code_chunk_size=1000)
    context = HierarchicalContextStrategy(include_doc_title=True)
    store = PeeweeVectorStore(audit_db)
    
    core = SemanticCore(
        embedder=mock_embedder,
        store=store,
        splitter=splitter,
        context_strategy=context,
    )
    
    inspector = PipelineInspector(core=core, session_path=audit_session)
    
    yield inspector
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á—ë—Ç—ã –ø–æ—Å–ª–µ —Ç–µ—Å—Ç–∞
    inspector.save_reports()
